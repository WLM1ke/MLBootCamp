{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "from keras import backend as K\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "from sklearn import compose\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "COORDINATES = [\"Xmin\", \"Ymin\", \"Xmax\", \"Ymax\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WLMike/Documents/PycharmProjects/ML_venv/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/WLMike/Documents/PycharmProjects/ML_venv/lib/python3.6/site-packages/ipykernel_launcher.py:4: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "answers = pd.read_csv(\"../raw/train_answers.csv\").set_index(\"itemId\")\n",
    "answers.columns = COORDINATES\n",
    "scaler = preprocessing.StandardScaler().fit(np.vstack([answers.values, answers.values[:, [2, 3, 0, 1]]]))\n",
    "answers[COORDINATES] = scaler.transform(answers[COORDINATES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WLMike/Documents/PycharmProjects/ML_venv/lib/python3.6/site-packages/ipykernel_launcher.py:2: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "votes = pd.read_csv(\"../raw/train_data.csv\").set_index(\"itemId\")\n",
    "votes[COORDINATES] = scaler.transform(votes[COORDINATES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WLMike/Documents/PycharmProjects/ML_venv/lib/python3.6/site-packages/ipykernel_launcher.py:2: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "votes_test = pd.read_csv(\"../raw/test_data.csv\").set_index(\"itemId\")\n",
    "votes_test[COORDINATES] = scaler.transform(votes_test[COORDINATES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def yield_batch(data):\n",
    "    votes, answers = data\n",
    "    item_ids = list(set(votes.index))\n",
    "    while True:\n",
    "        item_id = np.random.choice(item_ids, 1)\n",
    "        forecasts = votes.loc[item_id]\n",
    "        x = np.zeros((1, *forecasts.shape))\n",
    "        y = np.zeros((1, 4))\n",
    "        x[0] = forecasts.sample(len(forecasts))\n",
    "        y[0] = answers.loc[item_id]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def yield_batch_val(data):\n",
    "    votes, answers = data\n",
    "    item_ids = set(votes.index)\n",
    "    while True:\n",
    "        for item_id in item_ids:\n",
    "            forecasts = votes.loc[item_id]\n",
    "            x = np.zeros((1, *forecasts.shape))\n",
    "            y = np.zeros((1, 4))\n",
    "\n",
    "            x[0] = forecasts\n",
    "            y[0] = answers.loc[item_id]\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def yield_batch_test(data):\n",
    "    item_ids = data.index.unique()\n",
    "    for item_id in item_ids:\n",
    "        forecasts = data.loc[item_id]\n",
    "        x = np.zeros((1, *forecasts.shape))\n",
    "        y = np.zeros((1, 4))\n",
    "        x[0] = forecasts\n",
    "        yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def intersection_over_union(boxes_pred, boxes_true):\n",
    "\n",
    "    x_min = K.stack([boxes_pred[:, 0], boxes_true[:, 0]], axis=-1)\n",
    "    y_min = K.stack([boxes_pred[:, 1], boxes_true[:, 1]], axis=-1)\n",
    "    x_max = K.stack([boxes_pred[:, 2], boxes_true[:, 2]], axis=-1)\n",
    "    y_max = K.stack([boxes_pred[:, 3], boxes_true[:, 3]], axis=-1)\n",
    "\n",
    "    x_min = K.max(x_min, axis=-1)\n",
    "    y_min = K.max(y_min, axis=-1)\n",
    "    x_max = K.min(x_max, axis=-1)\n",
    "    y_max = K.min(y_max, axis=-1)\n",
    "\n",
    "    zeros = K.zeros_like(x_max)\n",
    "\n",
    "    x_inter = K.stack([zeros, x_max - x_min], axis=-1)\n",
    "    y_inter = K.stack([zeros, y_max - y_min], axis=-1)\n",
    "\n",
    "    x_inter = K.max(x_inter, axis=-1)\n",
    "    y_inter = K.max(y_inter, axis=-1)\n",
    "    inter_area = x_inter * y_inter\n",
    "    \n",
    "    area_pred = (K.max(K.stack([zeros, boxes_pred[:, 2] - boxes_pred[:, 0]], axis=-1), axis=-1) * \n",
    "                 K.max(K.stack([zeros, boxes_pred[:, 3] - boxes_pred[:, 1]], axis=-1), axis=-1))\n",
    "    area_true = (K.max(K.stack([zeros, boxes_true[:, 2] - boxes_true[:, 0]], axis=-1), axis=-1) * \n",
    "                 K.max(K.stack([zeros, boxes_true[:, 3] - boxes_true[:, 1]], axis=-1), axis=-1))\n",
    "\n",
    "    iou = inter_area / (area_pred + area_true - inter_area + K.epsilon())\n",
    "    \n",
    "    return -K.mean(iou, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def make_model(filters, input_shape, conv_l=6):\n",
    "    K.clear_session()\n",
    "    \n",
    "    y = x = layers.Input(shape=(None, input_shape))\n",
    "    \n",
    "    y_hot = layers.Lambda(lambda z: z[:, :, :-4])(y)\n",
    "    y_coor = layers.Lambda(lambda z: z[:, :, -4:])(y)\n",
    "    \n",
    "    y_hot = layers.Conv1D(\n",
    "            filters=1,\n",
    "            kernel_size=1,\n",
    "            strides=1,\n",
    "            padding=\"same\",\n",
    "            activation=None\n",
    "        )(y_hot)\n",
    "    y = layers.concatenate([y_hot, y_coor])\n",
    "    \n",
    "    for i in range(conv_l):\n",
    "        y = layers.Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=3,\n",
    "            strides=1,\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\"\n",
    "        )(y)\n",
    "        \n",
    "    y = layers.Conv1D(\n",
    "        filters=4,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding=\"same\",\n",
    "        activation=None\n",
    "    )(y)\n",
    "    \n",
    "    y = layers.GlobalAveragePooling1D()(y)\n",
    "    \n",
    "    model = models.Model(inputs=x, outputs=y)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class MaxLRTest(callbacks.Callback):\n",
    "\n",
    "    def __init__(self, base_lr=1.0e-9, max_lr=10, steps=10000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.steps = steps\n",
    "        \n",
    "        self.iterations = None\n",
    "        self.history = None\n",
    "        \n",
    "    def clr(self):\n",
    "        return self.base_lr * (self.max_lr / self.base_lr) ** (self.iterations / self.steps) \n",
    "  \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.iterations = 0\n",
    "        self.history = {}\n",
    "        K.set_value(self.model.optimizer.lr, self.base_lr)     \n",
    "            \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if self.iterations > self.steps:\n",
    "            self.model.stop_training = True\n",
    "        \n",
    "        self.history.setdefault(\"iterations\", []).append(self.iterations)\n",
    "        self.history.setdefault(\"lr\", []).append(K.get_value(self.model.optimizer.lr))\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "            \n",
    "        self.iterations += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "        \n",
    "    def plot(self, smooth=None, t_score=2):\n",
    "        smooth = int(smooth or self.steps // 10)\n",
    "        df = pd.DataFrame(self.history).set_index(\"lr\").loss.rolling(smooth).agg([\"mean\", \"std\"])\n",
    "        df[\"std\"] = df[\"mean\"] + t_score * df[\"std\"] / smooth ** 0.5\n",
    "        print(f\"Max speed learning rate  - {df['std'].idxmin():.1e}\")\n",
    "        df.plot(logx=True, figsize=(16, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class DecayingLR(callbacks.Callback):\n",
    "\n",
    "    def __init__(self, max_lr=0.1, warm_up=1, decay_per_epoch=1, epsilon=0.0001, wait=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_lr = max_lr\n",
    "        self.warm_up = warm_up\n",
    "        self.decay_per_epoch = decay_per_epoch\n",
    "        self.epsilon=epsilon\n",
    "        self.wait=wait\n",
    "        \n",
    "        self.iterations = None\n",
    "        self.history = None\n",
    "        \n",
    "        self.d_loss_iterations = None\n",
    "        self.d_loss = None\n",
    "        self.d_iterations = None\n",
    "        self.d_iterations2 = None\n",
    "        self.d_1 = None\n",
    "                \n",
    "        self.steps = None\n",
    "        self.decay = None\n",
    "        \n",
    "        self.wait_count=None\n",
    "        self.prev_loss=None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.iterations = 1\n",
    "        self.history = {}\n",
    "        \n",
    "        self.d_loss_iterations = 0.0\n",
    "        self.d_loss = 0.0\n",
    "        self.d_iterations = 0.0\n",
    "        self.d_iterations2 = 0.0\n",
    "        self.d_1 = 0.0\n",
    "        \n",
    "        self.steps = self.params[\"steps\"]\n",
    "        self.decay = (1 - 1 / self.steps) ** (1 / self.decay_per_epoch)\n",
    "        \n",
    "        self.wait_count=0\n",
    "        self.prev_loss=None\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.lr())\n",
    "    \n",
    "    def make_decay(self, old, new):\n",
    "        decay = self.decay\n",
    "        return old * decay + new * (1 - decay)\n",
    "    \n",
    "    def lr(self):\n",
    "        warm_up = self.iterations / self.steps * self.warm_up\n",
    "        if warm_up <= 1:\n",
    "            return self.max_lr * warm_up \n",
    "        \n",
    "        lr = self.history[\"lr\"][-1]\n",
    "        speed = self.history[\"speed\"][-1]\n",
    "        \n",
    "        if speed > 0:\n",
    "            return lr * self.decay\n",
    "        \n",
    "        return lr\n",
    "            \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.history.setdefault(\"iterations\", []).append(self.iterations)\n",
    "        self.history.setdefault(\"lr\", []).append(K.get_value(self.model.optimizer.lr))\n",
    "        \n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "            \n",
    "        self.d_loss_iterations = self.make_decay(self.d_loss_iterations, logs[\"loss\"] * self.iterations)\n",
    "        self.d_loss = self.make_decay(self.d_loss, logs[\"loss\"])\n",
    "        self.d_iterations = self.make_decay(self.d_iterations, self.iterations)\n",
    "        self.d_iterations2 = self.make_decay(self.d_iterations2, self.iterations ** 2)\n",
    "        self.d_1 = self.make_decay(self.d_1, 1)\n",
    "        \n",
    "        cov = self.d_loss_iterations - self.d_loss * self.d_iterations / self.d_1\n",
    "        var = self.d_iterations2 - self.d_iterations ** 2 / self.d_1\n",
    "        \n",
    "        speed = self.steps * cov / var\n",
    "        self.history.setdefault(\"speed\", []).append(speed)\n",
    "        \n",
    "        self.iterations += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.lr())     \n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = self.history[\"lr\"][-1]\n",
    "        speed = self.history[\"speed\"][-1]\n",
    "        print(f\"Learning rate: {lr:.1e}\")\n",
    "        print(f\"Speed per epoch: {speed:.4f}\")\n",
    "        \n",
    "        if self.prev_loss is None:\n",
    "            self.prev_loss = logs[\"val_loss\"]\n",
    "        elif abs(self.prev_loss - logs[\"val_loss\"]) < self.epsilon:\n",
    "            self.wait_count += 1\n",
    "        else:\n",
    "            self.wait_count = 0\n",
    "        self.prev_loss = logs[\"val_loss\"]\n",
    "        print(f\"Wait to stop: {self.wait - self.wait_count}\\n\")\n",
    "        if self.wait == self.wait_count:\n",
    "            self.model.stop_training = True\n",
    "        \n",
    "    def plot(self, smooth=None):\n",
    "        smooth = int(smooth or self.steps)\n",
    "        df = pd.DataFrame(self.history).set_index(\"iterations\")[[\"loss\", \"lr\"]].rolling(smooth).mean()\n",
    "        df.plot(figsize=(16, 8), secondary_y=\"lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def train_model(data_train, data_val, units, test=False):\n",
    "    \n",
    "    max_lr = 7.9e-04\n",
    "    \n",
    "    steps_per_epoch = 1000\n",
    "    epochs = 100\n",
    "    \n",
    "    model = make_model(units, data_train[0].shape[1])  \n",
    "    \n",
    "    model.compile(optimizer=optimizers.Nadam(lr=max_lr / 100, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004),\n",
    "                      loss=\"mae\",\n",
    "                      metrics=[intersection_over_union]\n",
    "        )\n",
    "    rez = model.fit_generator(\n",
    "            yield_batch(data_train),\n",
    "            steps_per_epoch=1000,\n",
    "            epochs=1,\n",
    "            callbacks=None,\n",
    "            validation_data=yield_batch_val(data_val),\n",
    "            validation_steps=len(data_val[1].index),\n",
    "        )\n",
    "    \n",
    "    \n",
    "    model.compile(optimizer=optimizers.Nadam(lr=max_lr, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004),\n",
    "                  loss=intersection_over_union,\n",
    "                  metrics=None\n",
    "    )\n",
    "    cycle = MaxLRTest() if test else DecayingLR(max_lr=max_lr)\n",
    "    cb = [\n",
    "        callbacks.ModelCheckpoint(\"../processed/model3.h5\", monitor=\"val_loss\", verbose=1, save_best_only=True),\n",
    "        cycle\n",
    "    ]\n",
    "    rez = model.fit_generator(\n",
    "        yield_batch(data_train),\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=epochs,\n",
    "        callbacks=cb,\n",
    "        validation_data=yield_batch_val(data_val),\n",
    "        validation_steps=len(data_val[1].index),\n",
    "        )\n",
    "    \n",
    "    model = models.load_model(\"../processed/model3.h5\", custom_objects={\"intersection_over_union\": intersection_over_union})\n",
    "    \n",
    "    return rez, cycle, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 31)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 27)     0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 1)      28          lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None, 4)      0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 5)      0           conv1d_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 64)     1024        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, None, 64)     12352       conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, None, 64)     12352       conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, None, 64)     12352       conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, None, 64)     12352       conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, None, 64)     12352       conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, None, 4)      260         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 4)            0           conv1d_8[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 63,072\n",
      "Trainable params: 63,072\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.7713 - intersection_over_union: -0.0061 - val_loss: 0.6777 - val_intersection_over_union: -0.0346\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: -0.0329 - val_loss: -0.0346\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to -0.03462, saving model to ../processed/model3.h5\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.0366 - val_loss: -0.0354\n",
      "\n",
      "Epoch 00002: val_loss improved from -0.03462 to -0.03540, saving model to ../processed/model3.h5\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: -0.0396 - val_loss: -0.0444\n",
      "\n",
      "Epoch 00003: val_loss improved from -0.03540 to -0.04442, saving model to ../processed/model3.h5\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.0891 - val_loss: -0.1675\n",
      "\n",
      "Epoch 00004: val_loss improved from -0.04442 to -0.16755, saving model to ../processed/model3.h5\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.2733 - val_loss: -0.3991\n",
      "\n",
      "Epoch 00005: val_loss improved from -0.16755 to -0.39914, saving model to ../processed/model3.h5\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4000 - val_loss: -0.3962\n",
      "\n",
      "Epoch 00006: val_loss did not improve from -0.39914\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.1510 - val_loss: 0.0000e+00\n",
      "\n",
      "Epoch 00007: val_loss did not improve from -0.39914\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "\n",
      "Epoch 00008: val_loss did not improve from -0.39914\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "\n",
      "Epoch 00009: val_loss did not improve from -0.39914\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "\n",
      "Epoch 00010: val_loss did not improve from -0.39914\n",
      "Epoch 11/100\n",
      "   1/1000 [..............................] - ETA: 3s - loss: 0.0000e+00\n",
      "Epoch 00011: val_loss did not improve from -0.39914\n",
      "Max speed learning rate  - 7.9e-04\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7QAAAHnCAYAAACIQhHeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xd8VfXh//HX52bvkJ2QhL33EFBQWYp71FEHFOuq37a/fttva61WrbW1w7bWtnbYWveoqyoqiqAgQ2TvlTBDyE7Invfm8/vjRAICMhJyMt7PxyOPc+455+a+78VS3vdzzucYay0iIiIiIiIiHY3H7QAiIiIiIiIip0OFVkRERERERDokFVoRERERERHpkFRoRUREREREpENSoRUREREREZEOSYVWREREREREOiQVWhEREREREemQVGhFRERERESkQ1KhFRERERERkQ7J3+0ApyMuLs727NnT7RgiIiIiIiJyBqxZs6bIWht/ouM6ZKHt2bMnq1evdjuGiIiIiIiInAHGmH0nc5xOORYREREREZEOSYVWREREREREOiQVWhEREREREemQOuQ1tMfS0NBAdnY2tbW1bkdpc8HBwaSmphIQEOB2FBERERERkTbTaQptdnY2ERER9OzZE2OM23HajLWW4uJisrOz6dWrl9txRERERERE2kynOeW4traW2NjYLlVmAYwxxMbGdsmRaRERERER6do6TaEFulyZ/UJXfd8iIiIiItK1dapCKyIiIiIiIl2HCq2IiIiIiIh0SCq0IiIiIiIi0iGp0LaivXv3MnDgQG655Rb69+/PzTffzIIFC5g4cSL9+vVj5cqVVFVVceuttzJu3DhGjRrFO++8c+i55557LqNHj2b06NF89tlnACxatIjJkydz7bXXMnDgQG6++WastW6+TRERERERkXah09y253A/f3cLW3PKW/V3Dk6J5GeXDznhcTt37uT111/n6aef5qyzzuLll19m6dKlzJkzh1/96lcMHjyYqVOn8vTTT1NaWsq4ceOYPn06CQkJzJ8/n+DgYDIzM7nxxhtZvXo1AOvWrWPLli2kpKQwceJEli1bxqRJk1r1/YmIiIiIiHQ0nbLQuqlXr14MGzYMgCFDhjBt2jSMMQwbNoy9e/eSnZ3NnDlz+P3vfw84txvKysoiJSWF7373u6xfvx4/Pz8yMjIO/c5x48aRmpoKwMiRI9m7d68KrYiIiIiIdHmdstCezEjqmRIUFHRo3ePxHHrs8Xjwer34+fnx5ptvMmDAgCOe99BDD5GYmMiGDRtobGwkODj4mL/Tz88Pr9d7ht+FiIiIiIhI+6draNvYjBkz+Mtf/nLoOth169YBUFZWRnJyMh6PhxdeeAGfz+dmTBERERERkXavVQqtMeYiY8wOY8xOY8xPjrE/yBjzatP+FcaYnoftu7dp+w5jzIzWyNOePfDAAzQ0NDB8+HCGDBnCAw88AMC3v/1tnnvuOUaMGMH27dsJCwtzOamIiIiIiEj7Zlo6Y64xxg/IAC4AsoFVwI3W2q2HHfNtYLi19i5jzA3A1dbarxtjBgOvAOOAFGAB0N9a+5XDk2PHjrVfTJj0hW3btjFo0KAWvZeOrKu/fxERERER6TyMMWustWNPdFxrXEM7Dthprd3d9ML/Aa4Eth52zJXAQ03rbwBPGGNM0/b/WGvrgD3GmJ1Nv295K+QSERERkc6kvoryt++mcs8qDgT1RTcyFDk9vqh0zv7mb92O0Spao9B2B/Yf9jgbGH+8Y6y1XmNMGRDbtP3zLz23+7FexBhzJ3AnQHp6eivEFhEREZEOY+3z2AU/J7K6CK8NJ72m2O1EIh1WXl2R2xFaTYeZ5dha+0/gn+CccuxyHBERERFpC+W5sPhRWP00BwOS+Vbdg8y64UauGJHidjKRDivR7QCtqDUK7QEg7bDHqU3bjnVMtjHGH4gCik/yuSIiIiLS1fgaYNVT8OmjUFPCsqBJ3FV2CxOH9Oby4clupxORdqI1ZjleBfQzxvQyxgQCNwBzvnTMHGB20/q1wCfWmY1qDnBD0yzIvYB+wMpWyCQiIiIiHVXJbvjTCPjwJ3ijevDHnv/g5rJvM2VEX/4xawzOVCwiIq0wQtt0Tex3gXmAH/C0tXaLMeZhYLW1dg7wb+CFpkmfSnBKL03HvYYzgZQX+M6JZjgWERERkU7KWsjbCC9eC7Vl7Dj/b9y1Kok9e2u5ZnQqv/7aMLcTikg70yrX0Fpr5wJzv7TtwcPWa4HrjvPcR4BHWiNHe/P4449z5513EhoaetS+Z599ltWrV/PEE0+4kExERESkHfB5oSgDds6HfZ9B/lYoy8IGR/GnlEd5fF40USE+/jFzNBcN1WnGInK0DjMpVEf0+OOPM3PmzGMWWhEREZEuy1sHZdnwxjchd4OzLSwe0s+Gid/jifyhPL6shLvO78P/TutHSKCfu3lFpN1SoW0lVVVVXH/99WRnZ+Pz+bjuuuvIyclhypQpxMXFsXDhQp555hl+/etfEx0dzYgRIwgKCnI7toiIiEjbyvgIXp8NDdXO45Ez4axbqU8Yyc7CKt7bmMPflu1idHo0P54xAI9H18uKyPF1zkL7wU8gb1Pr/s6kYXDxb467+8MPPyQlJYX3338fgLKyMp555hkWLlxIXFwcubm5/OxnP2PNmjVERUUxZcoURo0a1boZRURERNqz2jJ4+y4IjYVzH6EksDtzyvsw7/0iVu+bR4PPuTPjhYMT+cP1I1RmReSEOmehdcGwYcP44Q9/yD333MNll13Gueeee8T+FStWMHnyZOLj4wH4+te/TkZGhhtRRURERNre9vdhzveguhhu+w+kjWPmn5awNXcHYYF+XDGiO+f0iWVSvzgSI4PdTisiHUTnLLRfMZJ6pvTv35+1a9cyd+5c7r//fqZNm9bmGURERETapbe/A+tfBP9gyi76Kw8uCyAjfwnbcsv53tS+/OCC/roVj4iclta4D60AOTk5hIaGMnPmTO6++27Wrl1LREQEFRUVAIwfP55PP/2U4uJiGhoaeP31111OLCIiInKG1VfDKzc5ZTZtPHx/M/fvGcI763OICPbn62PTmHV2T5VZETltnXOE1gWbNm3i7rvvxuPxEBAQwN///neWL1/ORRddREpKCgsXLuShhx7i7LPPJjo6mpEjR7odWUREROTM8NbDuufhowehoQoSh8LMN9lc1Mi7G3K4bHgyT9w02u2UItIJGGut2xlO2dixY+3q1auP2LZt2zYGDRrkUiL3dfX3LyIiIu2EtfDC1bB7IQSEwuR7YeL3eG31fn78xkYC/Awr75tOt7BAt5OKSDtmjFljrR17ouM0QisiIiIirWfPYqfMJgyB2xdAYCgPv7uVp5ftIcjfw5OzxqjMikirUaEVERERkdaz6NcQEAa3vAeBoby6Kounl+2hd3wYL9w2nu7RIW4nFJFOpFMVWmttl5xUoCOeNi4iIiKd0L7lkLUcLvoNVX6R/OjFNXywOQ+AV+6YoNvxiEir6zSFNjg4mOLiYmJjY7tUqbXWUlxcTHCw/g9CREREXGQtzLsPQmP5vNtl3P6rj6ms85IWE8Lr3zpHZVZEzohOU2hTU1PJzs6msLDQ7ShtLjg4mNTUVLdjiIiISFdWlAk5a/Ge9S1mPb+JBp/l7hkDuG1SL4ID/NxOJyKdVKcptAEBAfTq1cvtGCIiIiJd0473Afik23U0+Aq4e8YAvjOlr8uhRKSz87gdQEREREQ6OF8DrH8FmzyCv6ypIzYskFsnaqBBRM48FVoRERERaZlNb0DRDvb0v5VNB8r4/gX9CQnUacYicuap0IqIiIjI6as5CG/fBd168vOdfYkI9udro7q7nUpEuggVWhERERE5fWueBeDg5F/x6a4yLhiUSFhQp5mmRUTaORVaERERETk9jY2w4klIGMynvuEA3DKxp7uZRKRLUaEVERERkdOTuw4qcuHs77Aoo4iYsECGpES5nUpEuhAVWhERERE5PVveAuOhOHUaH2zOY3L/ePw8xu1UItKFqNCeCTUH3U4gIiIicmbVlsH6V6DPVJ74vIQ6byN3nNfb7VQi0sXoiv3Wtn0uvHozpJ4Fxs/5yz5tHIQngCcAIpIgOh0Sh0JYbPPzGhvBGOfncI0+KNgG2Suh92SI7gm1pXBwD1QfhIoc6D4WAsOgughi+0FwZBu+YREREemSlv8VakooGPt/PPPsXqYPSmRgUoTbqUSki1GhbW2JgyF1HGSvcopreAJseAW8tV860EDiEAiOckru3mXOyG5wFFQVQLdeUJ4DvrovPc0DtvGrM4QlQPIIiE6Dokyoq4CwOEgaBvGDIGWkU3w9pzhAby1UFUFoDHg68b3lrAVv0+feUO283y+2A1Tmw/6VENLN+ZwbvRAQ4vx8WWGGc3zaePAPbJv8IiIibSHrc0gazqsHEoBS/mdyb8yXv5gXETnDVGhbW7ee+L754ZHXj9SUOkUwMsWZ2t5X74ywluc42zM+gpieMPgK2PeZU2iDImDgJeAfDDF9ICQaGmqcEd+gCKf4hkRDZCocWO3sK81yyteBtU6hPrAawpOc1y3YDrs+aS7DwdEw5T4IjYWcdbBvmVPMyrKdYt13Ohzc6xxXXwUNVc5ocfkB5/npZ0PhdgiLd0aba8uckhuWAH4BznMik53SFxoLQ69xRpFbW0W+U9hjeh9Z0HM3QESK8/p+x/nPvDzXydZQDaX7ICjSeU9rnoOMD5qPC4qCujJn3S/Q+fP7suBouOyPzudQlAm7F0HBliOPie0L598Dgy53yq+vAfI2QeEOSB0Lcf1a9FGIiIi0mfoq2L8CO3o2r6/Jpm9COKPTu7mdSkS6IGO/GHXqQMaOHWtXr17tdoxjWrW3hO+9so4LByfSaKGyzsuYHt3o3i2Eem8jw7pHkRwV/NXfYNZXHVH+aup9lFTXkxgRhL9fc2krq25gZ2Elw7pHEejvobHR4vmqiRgaG2HvElj7HGx+s3m78YOkoU75a6iFunI4uM8psX2mQkCoc5yvHsITndJXVeiU6Mr8IwtfYATUVxzjxY1T2EJinFHsfjOgzxTwDzqJT/Xw9+BzivPeZfDRT50y/sV7sD5nPSzeyffFaLZ/08ipfyBEpUNoN6jIg6KMr36taQ9C5gJnpD13gzPKnTjUOaU7LN4po4FhTile+5zzhQKAx9/5cgCczy9ltPPlxYaXm3+3JwAaG45+zYt+63wB4a11vlTw+B99Cnl5rlPCY/uc2mcnIiLSWlb+C+b+iLyrXmfCfxq4/9JB3H6urp8VkdZjjFljrR17wuNUaFvX5gNlXP/kcqrrnXIVEuBHTYPvqON6xoYysW8ckSEBJEYEMX9bPhn5lfRLCGdjdhkXD01ib3EVlXU+tuWWAxAdGkCf+HAOVteTU1pDbUPzqcdB/h7qvI30SwinZ1wYQ1OiSIkOZm9xFRW1XmLDghiRFsXg5EjiI4IwjT5nJLim1ClQIdFHBrTWKbCnUjitda4BrjnYfNpu0Q5nBHXXJ5C9GvI2Hvkcv0Bn5PLqJ51Tor8o+tY6pS1jHiz+nTOaWX7A2eYf7BS+0DgYfr1TXEuzIH+zMyIdP8AplA3VsH+VU5zrK50SazxOCY5Mcd5bwpDmwlhdDGO+6Vz/HBD8lW+1wddIwGFfLlBf7Vzn7AmAHuc478PnxRoPXotzbG0ZrHvJ+TzKsqCq2DntPGGQcwrzyieP82rGyRoa6yxLdjubw+Jh8JXO6DDW+az8g6HvNIhKczIUZcCexc5oe/JIiOzufDb+gTD+Luf1RURETtVrs2Hr27x44Vrun7Odj394Pn3iw91OJSKdiAqti2obfFTWeQkO8CMs0I+1WaVsyy2nX0I4r6zMoqbBx77iag6U1lBR64zkBQd4mNQ3jgXbCg79nu7RIQQHeEiOCiE+IoiKWi+5ZTUkRAQREuhHQkQwvkZLQUUtDT7L5gNlpMeEsiWn/IgSHRHsf+h1APw8hitHpvDApYOJDg1g04Eylu8qpsHXSPbBGnbkV3DpsGQafJYAP0NNvY9ar4+6hkZW7i2hvKaBmRN6kJlfSUJkEEO7R1FW00BooB/x4UHERwSRVVJNclQI3buF4O8xBAc0XXPb6IPiXU6xXflP2L+i+YML6eaMZtZXQe76I687jh/kjEhufw/Ouh0ikmHcHc6p163IWsuHm/O4+42NjO7RjbKaBgYmRrB0ZxHxEUGEBflRWt3AlpzyQ5/toORIkiKDeeTqoRhjyC2t4cPNeWQWVLJ+fykHq+sZkhLJiNRo7jivN3Hhx/mSoGQ35DS976oiZ/S70esUbW+tU5p3fQyJw5ztvrrmcnu6whKaJyoLjXXKtsffGU3vOw1K9zuFO36AM8JffsD5IqT6oDP6fKrXYYuISMdnLTw2CHpM5Pqi2zlQWsPSe6bo+lkRaVUqtB2E19dIZkElqd1CiAgOwFrL8l3FjErvRkjg6U+8tLeoCp+1JEcFExroT15ZLbsKK5mzPodXV+8/dJzHQOMp/CcQHOA5YmT4ZF00JImh3SPpmxDBiLQokqMOm0CpZI9zzWnGh871pB5/SB7ujMAmDoGek446vbbB18jfF+1i4Y4CGnyN9E+I4EBpDTFhgfSJD+e/a7MZ2j0KjzEkRwdjMAT4GVJjQokKCSCntIb5W/NJ6xZCg8/y/qZcBiZFUFrdQF65U6RHpEaxIbvs0Og3QExYIPHhQcSEBbJ+fyndQgOICQ9kS045USEBVNV5afA1f6BhgX4MSIogs6CSiloviZFBTB2Y4PyZlNeyPquU/PJaBiVHkhgZzBM3jWou/yejqhjWv+SM1EalNW0rhJ3znfW+051rg/2Dm0e/66ucsrziSedU6i+u6fYLdH7K9h/9Ooef0n24kBhnJD5hoFOyZ73tjIbnb3X+TBOHwKiZR8/eLSIiHdeexfDc5ew+62dMXTKAH13Yn+9O1TwQItK6VGjluBobLZ/vKWZTdhllNQ2kxYQyqW8c0aEBhAT4sauwiiWZhSRFBXNe/3iC/f3wNVp81hIW6Ie1sC2vnMjgAAor66iq81JR62V7XgVp3UIoqapnc045A5Mi2JJTRk29j893lxwxahwdGsCotGimDExgyoAE0mJCj5u3vLaBOetzKCivJaeslqiQABZuL2B3URURwf4EB/hRWFF33OefiL/H4LMWa+GSYUmc0yeOC4ckkhDRfNpxbYPvqKLpa7R4DBhjWLSjgDnrc8DAqLRoIoKdke9bzul56L29uyGH33ywnYraBirqvFgL3ZpOI994oIz6ptKcHhNKVkk1I9KiGZPejdjwQAL9PMSEBZIUFczLK7Ko9zUyeUA8lw5LJjo0EGsthRV1VNf7SI8JPXQtdZ3XR35ZHQmRQZTXNhAZHIC1zsBqkP9xinNtmXNtVEi0M2q+/G/QfbQzGhwa64zeeuuhYKvzBURxpnO6eN6mYxdfj78zApwyypkQK3GY8wWFSq6ISMf06izInM9DfV/j7YxaPr932ql9GSsichJUaKXd2ZpTztxNuby7MYeUqBCW7y4+tG/awATO6RvH2B7dqPc1snbfQbJKqvloa/4xy2pyVDB3zxjAFSNS8PfzYK2lqt5Ho7V4jCEkwI+SqnoC/TxEhvhT522kpt7Hwep61maVktYthBFp0QT5O6fMtvVpUuW1DZRVNxwqu/XeRr7/6jp2FVRRWlNPfvnJFfTjjbB3Cw2gss5LgJ/n0PXcX3bZ8GRuGpdOfEQQqd1CCQ7wtOxz2P0pZH7kXNMbkeTcHmrNs869mYMjnYmx6pxTtYnr74y8D7rcubb3i1sjiYhI+1ZbDr/rA2O+yVV7riQ4wMN/7jzb7VQi0gmp0Eq7l1Naw3PL9/LRlnzqvY0cKK056hinkAbw22uGMSQliso6L30TOv+kE41NI+K1DT7Kahoor/FS0+Bl/tYCzu0XR7/EcLYcKOfDzXm8uno/Fw9NYnNOGSNSoymqrGNbbgWNjZaR6dGUVNWzJaecyQPi6RETSnmtly05ZWTkVx7ztf09BgvcdX5vSqsb6Bkbxtl9YimvaWDV3oMUVjqj5LPP6Ul8eBDeRudLBL+vmmEbnIm99ix2Ti3PmOfcKulwsf2gWw/oPRnO+X+t8TGKiEhr27kAXryG6hveZPjz9dxxXm/uuWig26lEpBNSoZUOpbHRsi2vnG25Ffh7DCPSokmOCtYpTCfBWnvUyGqd14ev0RIaePxbTWfkV7B230Eq67xk5FeQV15HYkQQGfkVbMguO6UMX4wUv3HX2RRX1TN/az77iqsYnhrNTy4eeOSM0E5oZ9bpz/4C4QnOzNNFGc5pzF+I6QMzHoFe5zmzR/sHnlImERE5A+b9FD7/G69MW8a97+3h1TsnML53rNupRKQTUqEVkdO2Zt9BkqKCCfb3sGhHIf5+hpzSWmLCAkiKcmauXrGnhHc35JBXVsvlI5J5bXX2V/7OtJgQrh+TxtDUKM7tG3fEPZUPqSyADa/AJ484szgfrtd5zghv2ni44WXnvsAiItJ2rIXHBkPKKL5v7ubt9TlkPnLx0V9aioi0AhVaEWlTBRW1LNxeQGJkMLFhQfRJCGPupjz+uXgX+4qriQj2p6iyHnCugR7fK4arRnXnrJ4xhAUdYyS5ZLdTbDe/4TyO7tF8mnJQJAy81Ln+Nn2CMymVR6P5IiJnVFEmPDEWLnucqZ/2ondcOE/NPuG/NUVETosKrYi0K15fI5sOlDF3Uy5LMovYnldxaF9ksD8p0SEMTIrggsFJXDo8+di/pCIfCrbAot/C/s+bt8f2g6v/4dwvNyjiDL8TEZEuasU/4YO7KfzmCs76+y5+eEF//t803a5HRM6Mky20x7/ATkSkFfn7eRiV3o1R6d0AyC+v5YlPdhIb7tzTNzO/krfX5/D2+hzueyuAi4cmcf9lgwk/fPQ2ItH56TMV6iph02uw/mXnfrpPTXOOuf55Z7+KrYhI69rzKUT34NntzsPzB8S7m0dEBI3Qikg7sqeoiqeX7uGFz/cdte+BywazNaecy0YkM7l//JETYeVuhDnfdYrtFyKSYcavYMjVuuetiEhLWQt/GIDtfT4z9s0kKiSA1+86x+1UItKJ6ZRjEemwquq8fLy9gN/P20FWSfVR+68YkcLsc3rSKy6MmLDDZj+uKoa3/wcy5x35hLPugHP/DyJTznByEZFOqmQ3/HkUJef/mtHzevDzK4Yw+5yebqcSkU6sTU45NsbEAK8CPYG9wPXW2oPHOG42cH/Tw19aa59r2r4ISAa+uAHphdbagpZkEpGOLyzInytGpHDpsGSq6714jKGkqp7vvLyWjdllzNmQw5wNOYeO/9XVw7hxXBomLBZufs3ZeHAfLH4U1r8Cq/7l/Fz4CESlOqcj953m0rsTEemA9i4DYGPAUKCCMT26uZtHRKRJi0ZojTGPAiXW2t8YY34CdLPW3vOlY2KA1cBYwAJrgDHW2oNNhfZH1tpTGm7VCK1I17azoII/fJTB0swiKuq8AIxIi+a8fnH88MIBRz9h2Z9h/gNHbgtPgtlznImkRETkq715B+z6hHt6/Ze3N+Sw6aEZBPrrdj0icua0ySnHxpgdwGRrba4xJhlYZK0d8KVjbmw65ltNj59sOu4VFVoRaanGRsstz65icUYhAImRQdw+qTe3TeqFx3PYtbPVJbD0MSjeBTvmOtsiu8MPtugaWxGRr+Ktg0d7YwdfxeiNVzJ5QAJ//PpIt1OJSCd3soW2pV+tJVprc5vW84DEYxzTHdh/2OPspm1feMYYs94Y84Ax+leliJwaj8fw/K3jWPnTaZzfP5788joembuN3vfN5dEPt+NrbPrSLjQGLvwl3PgKPFQGI2+G8gPwzCXga3D3TYiItGc7F0B9JQeSL+BgdQNn94l1O5GIyCEnLLTGmAXGmM3H+Lny8OOsM9R7qsO9N1trhwHnNv3M+oocdxpjVhtjVhcWFp7iy4hIZ5cQEcxzt47jyVljiG2aKOpvi3bR5765/ODV9ez/8uRSlz3uLLM+g1/EwdLH2zixiEgHsWMuBEWx0DsUgAm9VGhFpP044aRQ1trpx9tnjMk3xiQfdsrxsSZ0OgBMPuxxKrCo6XcfaFpWGGNeBsYBzx8nxz+Bf4JzyvGJcotI1zRjSBIzhiRRXtvA3xft4uUVWby17gBvrTvA/ZcOYkBSBKGB/s6EJvfsg49+ChtfhwU/g6zPocfZMGqWM6IrItLV1ZZBxjzoO5VVWRUkRQaTFhPidioRkUNaeg3t74DiwyaFirHW/vhLx8TgTAQ1umnTWmAMUA5EW2uLjDEBwCvAAmvtP070urqGVkROxb8W7+aRuduO2DYoOZJ/zx5LSnQINNTCm7fB9veaD+gzFW54GQL0DzcR6cI+vBc+/xt880OmvF5H/8Rwnpx1wkvaRERarK2uof0NcIExJhOY3vQYY8xYY8xTANbaEuAXwKqmn4ebtgUB84wxG4H1OCO5/2phHhGRo9xxXm/W3D+dm8enH7rVxLbccmY+tcI5ICAYbngJfpIFF//O2bbrE3gkCZ6+CPavcim5iIjLts6B+IGUJYxlT1EVw1Oj3U4kInKEFo3QukUjtCLSUrP+vYIlmUX89JJB3HFe76MP2PIWvH5L8+PpD8GkH7RROhGRdmD7XPjPjXDxoyyNuYaZ/17B87eO47z+8W4nE5EuoK1GaEVEOqQnbnSugnhk7jZue3YVe4qqjjxgyNXObMhX/tV5vOAhZ7R279K2DSoi4pbdC8HjD6NmsXpfCcbAyHSN0IpI+6JCKyJdUlRoAC/dPh6Aj7cXcPlflrKzoOLoA0fNhBtecdazlsOzl8Kc74HP24ZpRURcsHcppJ8NgaEsySxicHIkkcEBbqcSETmCCq2IdFkT+8ax9zeX8o+ZY6j3NjL9scUMfvBDvvXCavLKapsPHHiJM1r7rcXO47XPwS9i4bMn3AkuInKmFWVCwVYYeBmFFXWs2XeQGUOS3E4lInIUFVoR6fIuGprEW985h1kTegAwb0s+E379MY/Nz6C2wdd8YPIIeLAEuvVyHn/0U3jmUmiocSG1iMgZtOVtZznocuZvzQdg2qAEFwOJiBybCq2ICDAkJYoYfN98AAAgAElEQVRfXDWUVT+dzswJ6QT6e/jzx5kMfOBDbvrX580Hevzgf9fDj/c4j/cthceHQXWJO8FFRFqbtbDpNUgbD1HdWZJZSEpUMIOTI91OJiJyFBVaEZHDhAX588urhrHxZxcyc0I6AJ/tKuaCxz7liFnhQ2Pgzk9h2PVQVQhzf+RSYhGRVpbxIRRlwIgbyS+vZUlmEZP6xWGMcTuZiMhRVGhFRI4hOMCPX141jCU/ngJAZkEll/x5KZ/tLGo+KGUkXP0PCI2DzW/CuhddSisi0oo2vgYBYTBqJv9ZuZ+qei+3n3uM25uJiLQDKrQiIl8hLSaUZT+ZSkiAH9tyy7npqRXc+99N+BqbRms9fnDnQmf9ne/AQ1Gw6in3AouItIS3HnYugKFfA78A3ll/gLE9utE/McLtZCIix6RCKyJyAt2jQ9j2i4uY9/3zAHhlZRZ97pvLG2uynQOi02Hmm81PeP+HsOzPUJbtQloRkRbYtxTqymHgpXyyPZ/dRVWa3VhE2jUVWhGRkzQgKYLMRy4mNNAPgB+9voFFOwqcnX2nw4MH4fZPnMfzH4A/DoHXvgGNvuP8RhGRdmbbu+AfAr3OZ2lmMR4DN45LdzuViMhxqdCKiJyCAD8PWx++iEU/mgzALc+s4jsvr6Xe2wgeD6SOgZ9kwRV/cZ6w9R14OAYKtrkXWkTkZFjr/J3VezIEhrJu/0FGpkUTFuTvdjIRkeNSoRUROQ0948K4bZJzP9r3N+by2ur9zTuDo2D0N+C+XBjzTWfbS9dppFZE2reS3VBdDP1nUFRZx/r9pZzXP97tVCIiX0mFVkTkND1w2WA2PHghAPe/vZkGX+ORBwSGwuWPw9QHoGw/fPILF1KKiJyknHXOsvtoPtlWgLVwweBEdzOJiJyACq2ISAtEhQbwq6uHATD64fl8mlFIbcOXRmIn/R+EJ8LSPzb/g1FEpL3Zsxj8AiFhMB9tzaN7dAiDkyPdTiUi8pVUaEVEWujGcWkAVNR5mf30Sm55ZuWRB3g8cOXfnPV/TYO6ijZOKCJyAvXVsOVtSBzKrpI6Fmwr4JJhSRhj3E4mIvKVVGhFRFrIGMOGBy8kOMD5K/Xz3SXc+uwq5m7KbT6o33RnJmTrg5dvAF+DS2lFRI5hy1tQVwaTfsDzn+0F4Joxqe5mEhE5CSq0IiKtICo0gO2/uJiV901jRFo0n2wv4NsvrWXoz+axv6TaOeim1+Cc7zn3eVz4iLuBRUS+4PM6l0SEJcDAy/hsVzHje8UwMEmnG4tI+6dCKyLSihIig3nnOxNZfPcU/DyGyjov5z66kA8354LHDy78BSQNd/7xuHWO23FFRCDrMyjOhGkPkFtRR2ZBJdMHaTIoEekYVGhFRM6A9NhQdv3qEn551VAAvvvyOirrvM7OS//gLF+b5YyMiIi4aftc8A+GodewNLMIgEn94lwOJSJyclRoRUTOoJkTevD6XWfjbbQ8+PZmZ2PaOBh7m7P+wd3uhRMRATiwGrqPgcAwFu4oIC48iIFJEW6nEhE5KSq0IiJn2Fk9Y+geHcJ/1x0gM79phuNL/wCBEbD6aVj/irsBRaTr8nkhbzMkj8TXaFm0o5BJfWM1u7GIdBgqtCIibeChK4YAMPPfK2hstGAMfHelU2qXPe5yOhHpsooywFsDySPYU1RFdb2PiX11urGIdBwqtCIibeCCwYn0iA0lv7yOe/+7ydkYmQJT7oPC7fCHgbqVj4i0vazlzrL7aLbmlgMwOEWzG4tIx6FCKyLSRt7/3rmc2y+OV1fv5+UVWc7G0d+AyFSoyIUdc90NKCJdz4650K0XxPZly4Ey/D2GvgnhbqcSETlpKrQiIm0kPMifZ245i+jQAO57axPZB6shKBy+txb8Q+CzJ9yOKCJdSW057P4UBl6Kz8IrK7PomxBOkL+f28lERE6aCq2ISBvy9/Pw4GWDAZj175VNG4Ng3O2QvRL2LHExnYh0KbsXQmMDDLiY+VvzKK/1ctP4dLdTiYicEhVaEZE2duXI7gDsKari893FzsZJ/wdRafDKDVCa5WI6Eekydi6AwHBIG89rq7MJ8vdww1kqtCLSsajQioi0MT+P4c3/OQeA259bTVl1A4TGwDfegfpK+NMIKM9xOaWIdGqNjbD2BUifwM7iWj7ZXsA3J/Yi0F//NBSRjkV/a4mIuGBMj248fOUQKuu8PPzeVmdjbB+44i9gG+FfU537Q4qInAmF2wALPSby0dZ8AGaf08PdTCIip0GFVkTEJd84uyfTBibw5tpsdhZUOBtHfwPO/ZEz6/H6F90NKCKdV8Y8ZzniRtbsPUi/hHCSo0LczSQichpUaEVEXHTd2FQApj+2mJKqemfj1PshcSi8+7/grXcxnYh0WlnLIX4gRCazPa9C954VkQ5LhVZExEUzhiQxc4IzCct7G5uumzUGxtzirP8ywbnWTUSktTQ2QtYKSJ9AWU0DB0prGJikQisiHZMKrYiIi4wx/PyKoUQE+fPcZ3ux1jo7xt0BITGAhQU/czWjiHQyueugrgzSJrAjz7ncYWByhMuhREROjwqtiIjL/DyGH17Yn12FVdz31ubmHT/e7Sw/+zM01LgTTkQ6n+V/c5Y9zmFd1kEABmmEVkQ6KBVaEZF24IZxzmnHr6zMYldhpbPRGOf+tACPJEFDrUvpRKTTsBb2fQZpE7DR6by+JpvhqVEkRQW7nUxE5LSo0IqItAPBAX4sv3cqQf4ernpiGfuKq5wdUx8A/6aZR5+a5l5AEekcyrKhIgeGXkNWSTU7Cyq5YkSK26lERE6bCq2ISDuRHBXCb68ZTkWdl2v+vpx6byN4PPDTXOeA/M2w+PfuhhSRjm3xo84y7Sw+310MwPn9410MJCLSMiq0IiLtyFWjuvPjiwZQVFnHq6v3OxuNgXv2Oeuf/ALWPOdeQBHp2HLWORPOJY/kw815xIUH0Tch3O1UIiKnTYVWRKSd+Z/z+zA4OZIH3t6M19d0y56QaPjuGmf9098618GJiJwKbx0UbIcxs/FZWJxZxLDukRhj3E4mInLaVGhFRNoZYwzXjEkFYPjPP2q+lU9cX7jscSg/AAfWuphQRDqkwu3Q2ABJw9ldWImv0TJlYILbqUREWqRFhdYYE2OMmW+MyWxadjvOcR8aY0qNMe99aXsvY8wKY8xOY8yrxpjAluQREeksbp3YE2Ogut7Hc5/tbd4x9GvOJFHrXnAtm4h0UHlNtwVLGs78bfkATFWhFZEOrqUjtD8BPrbW9gM+bnp8LL8DZh1j+2+BP1pr+wIHgdtamEdEpFMwxrDrkUsAeOjdrfgam0Zpg6Ng4KWw5hnIWuFiQhHpcLJXQVAUxPRi9d6D9E8MJ7VbqNupRERapKWF9krgi9lJngOuOtZB1tqPgYrDtxnngo2pwBsner6ISFfk8RguHZYMQJ/75jafejz6G87ytVlQc9CldCLS4exbBukTsMbDuqyDjEo75ol1IiIdSksLbaK1tul+EuQBiafw3Fig1FrrbXqcDXQ/3sHGmDuNMauNMasLCwtPL62ISAfz2NdHHFp/askeZ6X3+TDzv1CZD/PudymZiHQolYVQlAE9zmFvcTUHqxsYlR7tdioRkRY7YaE1xiwwxmw+xs+Vhx9nnaGDMzbtprX2n9basdbasfHxul+aiHQNQf5+bHroQgAembuNbbnlzo6+0yD9bFj/IlSXuJhQRDqErM+cZY+JPLVkNwAjVWhFpBM4YaG11k631g49xs87QL4xJhmgaVlwCq9dDEQbY/ybHqcCB071DYiIdHYRwQH8/ebRAFz8pyXsKapydkz/ubP8/O8uJRORDmPvMggIhZSRrNpbQkpUMAMSI9xOJSLSYi095XgOMLtpfTbwzsk+sWlEdyFw7ek8X0SkK7l4WDI/vKA/APf+d6OzMX08JAyBxY9CUaaL6USk3cv8CNLGUVYPGfmV3DguXfefFZFOoaWF9jfABcaYTGB602OMMWONMU99cZAxZgnwOjDNGJNtjJnRtOse4P+MMTtxrqn9dwvziIh0Wv9vWj++ObEna/eVUlBR62yc9oCzfGKse8FEpH2rLICDe6D3ZD7Y5Ex9MipdE0KJSOfQokJrrS221k6z1vZrOjW5pGn7amvt7Ycdd661Nt5aG2KtTbXWzmvavttaO85a29dae521tq5lb0dEpHO7eXw69b5GPtri3EOSARdDaJyz/vHD7gUTkfZryR+cZe/JLN1ZBKAJoUSk02jpCK2IiLShPvHhxIUH8uHmvOaN3/7cWS75A+RtdieYiLRfOz6AmD6QMoqN2WVcPDSJsCD/Ez9PRKQDUKEVEelAjDHcOqkXS3cW8eePm66bDY+Hm1531ldogigROUxlAZTug1EzOVhVT1ZJNSPSNDorIp2HCq2ISAcza0IPeseH8dj8DHbkVTgb+18Io2bC5regrtLdgCLSfmz+r7PscQ6bc8oAGN49ysVAIiKtS4VWRKSDiQgO4KXbxwPwi/e2Nu8YNQsaqmDr2y4lE5F2J3uVs0w9i4x858uuAUm6XY+IdB4qtCIiHVByVAhXjEhh6c4idhY0jcimjYfYvrDuJXfDiUj74K2HzW/A0GvB48fSzEKiQwOICQt0O5mISKtRoRUR6aD+d3o/AP6zMsvZYAyMvBmyPoOinS4mE5F24cAaZ5nq3NZra245SZHBuv+siHQqKrQiIh1Un/hwzu8fz1NL97BmX4mzccSNzvKJMdBQ6144EXHftjng8Ydh11FQUUt+eR3Xjkl1O5WISKtSoRUR6cDuOr8PANf8fbmzITIZep3nrM+7z6VUItIu7P7UuRQhLI7NB5wJoYZpQigR6WRUaEVEOrCz+8QyODkSgN/P2+FsnPWOs1z3onMNnYh0PflboWALDLgYgJV7DmIMDFGhFZFORoVWRKSDe+G2cQA8sXAnOwsqwOOBS/8AvjrY8b7L6UTEFZteA+MHw78OwLsbchjXM4bwIH+Xg4mItC4VWhGRDi42PIg/3TASgGeW7XU2Dr/BWe5c4E4oEXGPrwGW/825/CA8gfzyWg6U1jB9UKLbyUREWp0KrYhIJ3DlyO5cOTKFdzfkUOf1QVC4c6uOdS/C/pVuxxORtrRvmXOGRp8pAPzlk0wAzukb62YqEZEzQoVWRKSTuGZ0KuW1Xj7eVuBsmPQD8AuCf18A+z5zN5yItJ09i53lsOsA+DSjkF5xYQxJ0fWzItL5qNCKiHQSE/vGkRQZzJtrsp0NSUPh+uec9Wcuhvpq98KJSNvZuwy6j4XIFIor69hfUsN1Y3W7HhHpnFRoRUQ6CT+P4apR3VmUUUhhRZ2zccDFMP0hZ/3fF0BDjVvxRKQt1FfD/s+h50QAPtySB8Do9G5uphIROWNUaEVEOpFrx3TH12iZ+dSK5o0Tvw/j74L8zfDBj90LJyJn3vammc3TxgOwLqsUgFHp0W4lEhE5o1RoRUQ6kb4JEYQE+LEjv4J3N+Q4G42Bi38Lo2bB2ufhl0nQ6HM3qIicGXkbnWXf6TQ2Wv67NpsLBycS5O/nbi4RkTNEhVZEpJNZfu9UAO5/ezPW2uYdF/7SWXprYPObLiQTkTPKWmdm85TR4B9ERkEFjRaGdddkUCLSeanQioh0MtGhgcw+uwdlNQ1szS1v3hESDQ8UOev/vQOqitwJKCJnRlEm1JRA0jAA5m3OB+CKkSluphIROaNUaEVEOqHZ5/QE4KMt+Ufu8AuAK55w1n/XB2oOtm0wETlzdn3iLM/9PwBW7S1hYFIEPWLDXAwlInJmqdCKiHRCvePDGZkWzdNL91DvbTxy5+hZcMPLzvrzV7V9OBE5M3YvhJje0K0nB6vqWbariPP6x7udSkTkjFKhFRHppC4bnkxFnZfpj31KnfdLk0ANvBQCwiB3Pez+1J2AItJ6Gmog40PoPQWADzbnYS3MGJLocjARkTNLhVZEpJO6bVIvLhmWRFZJNd95ad3RB3x/k7Nc8WTbBhOR1rfuRWfZx5kU7k8fZxAZ7M/INN1/VkQ6NxVaEZFOyhjDo9eOAGDBtnzmbso98oCwWJjwbcj8CKpLXEgoIq3mwBpnOfBSckpryC+v48IhSfh5jLu5RETOMBVaEZFOLDzInyU/dk5B/PZLa1mz70uTQA2/Hhob4I9DXEgnIq1i0xuw+b8w8mYwhg835wFw0/h0l4OJiJx5KrQiIp1cWkwoK386jbjwIK77x2dkFVc370we6SwbqmH7XHcCisjpK9kDb94GthEm/QCAdftLCfTzMDI12uVwIiJnngqtiEgXkBARzMNXDqHRwnm/W8j+kqZSawzctcxZ/8+NUJF//F8iIu3P1red5beXQ1w/rLWs2lPCjKFJeHS6sYh0ASq0IiJdxCXDkvnWeb0BeHLxruYdSUPhyr86689fCT6vC+lE5LRkrYCYPhDXD4BtuRXklddyVk9NBiUiXYMKrYhIF3LvJYMYkRrFi59nMX/rYaOxo2ZC3wugcBssftS9gCJy8nxe2LcMek46tGnVXmeCt3G9YtxKJSLSplRoRUS6mN9eOxxj4I7nV7Nm32GzG9/0GvgFObfxqchzL6CInJy8DVBXDr3OO7Rp84EyYsMCGZAY4WIwEZG2o0IrItLFDEyKZNGPJgMw++lVzTs8Hjj/bqgthT8MAGvdCSgiJ2fPEmfZVGjrvY28viabId2jMEbXz4pI16BCKyLSBfWIDePyESlU1nl5acW+5h1nf7d5/a/j2j6YiJy83YsgfhCEJwCwLsu5Ldc5fWJdDCUi0rZUaEVEuqjHrh9BVEgAb6090LwxIATuL3DWizIgd6M74UTkq5XnwN4l0P/CQ5uW7SrGY+DGcbr/rIh0HSq0IiJdVICfhzvO7cXqfQc5UFrTvMM/CP53IwRGwJPnQX2VeyFF5Ng2vgqNXhh+w6FNy3YWMTw1mqiQABeDiYi0LRVaEZEu7IoR3QGY+JtP8Poam3d06wHTfwZY2PGBO+FE5PjWvQjdx0DCIABySmtYs+8gE/vqdGMR6VpUaEVEurD02NBD62+tO3DkzpE3O8s3bwNfQxumEpGvVJ4LxTth6LXQNPnTextzAJg6MMHNZCIibU6FVkSki9v88xkA3P3GRvYUHXZ6cWAoRCQ763sWu5BMRI4pe6WzTGueuG3NvoMkRAQxpofuPysiXYsKrYhIFxce5M8zt5wFwFV/XUZ1vbd553ebbuvz4tegYJsL6UTkKPtXOveMThqOtZbHF2Qwb0s+l49IcTuZiEibU6EVERGmDEzgxnHplNU08Lt5O5p3BEXAJb931l/4mu5NK9IeZK+ClJHgH8j2vAoeX5BJ9+gQvnV+b7eTiYi0uRYVWmNMjDFmvjEms2nZ7TjHfWiMKTXGvPel7c8aY/YYY9Y3/YxsSR4RETl9v/7aMOLCA3lm2d4jTz0edwdM+DZU5MDBPe4FFBHw1kHOukOnG3+wOQ+AN/7nbBIigt1MJiLiipaO0P4E+Nha2w/4uOnxsfwOmHWcfXdba0c2/axvYR4REWmBu87vA8CU3y/CHj4aO/ImZ/nPyW0fSkSa7V8JvnpImwDAyj3FDOseRXJUiMvBRETc0dJCeyXwXNP6c8BVxzrIWvsxUNHC1xIRkTNs9jk9CQv0A2Dy7xc170gc6ixry6Ch5ugnikjb2DHXuX6292TqvD7WZZVyVk9NBCUiXVdLC22itTa3aT0PSDyN3/GIMWajMeaPxpig4x1kjLnTGLPaGLO6sLDwtMKKiMhXC/DzsO7BCwHYV1xNRn7Td5HGwM1vOOuPJIG33qWEIl3crk+g50QICmdTdhl13kbG91ahFZGu64SF1hizwBiz+Rg/Vx5+nHXOTTvV2ULuBQYCZwExwD3HO9Ba+09r7Vhr7dj4+PhTfBkRETlZgf4eFv5oMn4ew2V/WUpNvc/Z0e8CGNz0V/+jvaDR515Ika6oIh8Kt0Ov8wFYsacEQCO0ItKlnbDQWmunW2uHHuPnHSDfGJMM0LQsOJUXt9bmWkcd8Aww7kTPERGRM69XXBg/vWQQ9d5GPs047K/2a5+F6B5QXwlLHnMtn0iXtG2Os+w7HYDlu4oZkBhBTFigi6FERNzV0lOO5wCzm9ZnA++cypMPK8MG5/rbzS3MIyIireTasakA3PXiWnyNTSfgeDzwvXXO+sJfQs1Bl9KJdEHb3oXYfpA0lLyyWpbvLmbKwAS3U4mIuKqlhfY3wAXGmExgetNjjDFjjTFPfXGQMWYJ8DowzRiTbYyZ0bTrJWPMJmATEAf8soV5RESklUQGBxxaX7PvsOLq8YMr/+qsb3qjjVOJdFG1ZbB3KQy6DIC31h3A12i5elR3l4OJiLirRYXWWltsrZ1mre3XdGpySdP21dba2w877lxrbby1NsRam2qtnde0faq1dljTKcwzrbWVLXs7IiLSmj6/dxoA1z+5nMKKuuYdw29wlnN/BL4GF5KJdDGZ88H6oI/zv8k31uxnaPdI+ieGuxxMRMRdLR2hFRGRTiwpKphvndcbgFufXdW8w88fptzvrO9b5kIykS5m+3vOMnUsJVX17Cqs4tJhKThXbYmIdF0qtCIi8pXuvWQQlwxLYtOBMsqqDxuNnXCXs3zvB+4EE+kqfF7Y/akzOhsQwvr9ziUAo9KjXQ4mIuI+FVoRETmhmeN7APCD19Y3bwyKgJg+ULIbVvzTpWQiXUDhNqgpgUGXA7B2Xyl+HsPw1CiXg4mIuE+FVkRETuisXs59Lj/ZXoBz2/Emty9wlh/cDXsWu5BMpAvIafoiqeckANZmHWRgUgShgf4uhhIRaR9UaEVE5IQC/Dw8cvVQAGb+e0XzjtAYuHWes/7c5S4kE+kCctdDoHNGhK/RsmF/KaPTu7mdSkSkXVChFRGRk3LNaOe+tMt2FvP9/6xr3pE+AcKTnPX6aheSiXRyuxdB8nDweNiaU05VvY/RPXT9rIgIqNCKiMhJCg7wY8ODFwLw9vocduRVNO+84i/O8lfJcPgpySLSMmUHoHgnJA0DYP62fDwGzusX73IwEZH2QYVWREROWlRoAKt+Oh2AGY8vpsHX6OzoO635oP0rjvFMETktW99xlqNnA7A4o5CRadHEhge5GEpEpP1QoRURkVMSHxHEtIEJAKzLKnU2evxg9rvO+scPu5RMpBPKWQcRKZA4mPLaBjZmlzKxb5zbqURE2g0VWhEROWW/v24EAL98f2vzxl7nQWR3Z0ZWnXYs0nLWwv7PIWUUAAu3F9BoYZIKrYjIISq0IiJyyrqFBdI7PoyN2WXsK65q3jHlp9BQBXmb3Asn0lnkb4bSLBhwEQAfbcknPiKIs3rGuBxMRKT9UKEVEZHT8u/ZZwHw5OLdzRv7XQgY2PGBO6FEOpOMplti9ZtBndfHoh0FTB+UgMdj3M0lItKOqNCKiMhp6RkbCsDLK7KobfA5G8PjIa4/LPmDi8lEOondi5zZjSMS2ZRdRlW9j/P7a3ZjEZHDqdCKiMhpMcbw5xuda/tGPTyfOm9TqU2fAL46eOZSF9OJdHANNbB/JfQ6H4DXV2cD6HRjEZEv+f/s3Xd8n1Xd//HXSdJ0r3Q33bt00qYtZY8yBBQQBL1V0FtFRbndioK3uADvn4rzVnEAAg5uUTbInt2LLrrbNN1p2jRdaZvk+v1xfds0bUpLk+bKN3k9H488zvmec77fvMFa8sl1XedY0EqSTtj7RnVneG4b9uwv5/N/mRMPXvjduM1/A3ZsTC6clM4KpsW/GOp7NgBvLN9Cnw4tPK5Hkg5jQStJqpF/fvYMAJ5ftCkeaN4ern887nvrsXRi1kwDAvQ6jXXFe1hXvIerTu2RdCpJqncsaCVJNZKdlcHN5w8AYPWW1I7H/c6BgRfD7Adg5+YE00lpqmAadBwIzdqyYN12AM4e5HE9knQ4C1pJUo29f0x85ejcH79SuUHUpNuhbA/869OJ5ZLS0t4dsOIlGHQxAAvXbScjwJCubRIOJkn1jwWtJKnG+nZsebD/1+lr4k7noXG74iWYdV/dh5LS1ZalQAQ9JwCwcH0JAzq3onl2ZrK5JKkesqCVJNWK5T98DwD/mrMuHggBrron7j/xBVj5akLJpDRTuCRuOw5mX1kFLy7ezLDubZPNJEn1lAWtJKlWZGVmcNmIbixYt5395RXx4Kjr4NSPxv2nvpJcOCmdFEyHpm2gQ39eWRI/gz6xX4eEQ0lS/WRBK0mqNZeP7EZFBG8VFFcOXvEr6DoCipa5QZR0PAqmQY9xkJHJtFVbaZqVwZWn5iadSpLqJQtaSVKtOS11FWnKiqKqE+fcErcLHqnjRFKa2VMMm9+GXqcBsGh9CYO7tiY7yx/ZJKk6/u0oSao17VtmM6hLK37y/FIqKqLKiaGXQ/s+8MLtSUWT0sP6OUAEPfIoKd3PzPytB39RJEk6kgWtJKlWnd4/Pitz/B0vVp3oNBTKSmHj/ARSSWniwP8/uo7ijWVb2F8eMWlol2QzSVI9ZkErSapV37x0CABbdu5lTdHuyonLfhy3s+5PIJWUJjbOhza50LIDry4ppHWzLMb0apd0KkmqtyxoJUm1qmlWJk/efCYAv31tReVE2x4w4EKY8XsoWnGUd0uN3Jqp0HUkAFNWFnFavw5kZfrjmiQdjX9DSpJq3fDctlx1ai7/mLWWTSWllRNnfy1uJ/8imWBSfbazELavgT5nsnbbbtZs3e1xPZJ0DBa0kqST4qZz+7OvrIJvPDKvcrDXBBh2FSz4F+zbffQ3S43RxtT/V7qNZNrKrQBM7G9BK0nvxIJWknRSDOjcCoBXlhTyypJDzp/N+wTs3Q5TfpVQMqmeOlDQdh3BvLXFtMzOZFCX1slmkqR6zoJWknRShBC47bKhAHzs3hksWl8ST/Q9C3pOgLl/gSh6h0+QGpkN86BtL6Jm7Xht2RZG9mhHZkZIOpUk1WsWtJKkk+YTZ/ale9tmANz00KzKiTHXw7ZVsParVWQAACAASURBVG7WUd4pNUIb50O3kWwq2cuqLbu4eJjH9UjSsVjQSpJOmhACb95yPtmZGawu2s2O0v3xxND3QmZTmPdwsgGl+mLvTihaDl1HsrJwJwADvd1Yko7JglaSdFKFELj9fcMAeG7hpniwWVsYdDEs/CeUlyWYTqonNi8CIug6gsUbdwCVz6FLko7OglaSdNJ9aHxPurdtxjMLNlYOjrwWdhVC/pvJBZPqiw1vxW23kcxes43ubZvRpU2zZDNJUhqwoJUknXQhBC4e3pVXl26m5MBtx33OjNunv5pcMKm+WP06NM+BNrnMzt/GmN7tk04kSWnBglaSVCcuH9md/eURD01dEw80T/3AvmUp7NuVXDCpPlg/B3qfzoaSUtZvLyXPglaSjosFrSSpTozp1Q6AHz27mN37Us/NXv9Y3K54OaFUUj2wpxiK10DuGOav3Q7AyJ7tEg4lSenBglaSVCdCCHz67H4A/PH1VfFg7zPi2yznPJhgMilh6+fEbbfRvL1hByHAkK7ucCxJx6NGBW0IISeE8HwIYVmqPeL+mBDC6BDClBDCwhDCvBDCdYfM9Q0hTAshLA8h/D2EkF2TPJKk+u2blw6lf6eWPDp3HVEUQWYTyB0Ly/4NFeVJx5OSsfhJCBnQI4+3N5TQp0NLWmRnJZ1KktJCTa/Q3gK8GEXRQODF1OvD7Qauj6JoGHAJ8LMQwoH7aH4E3B1F0QBgG/CJGuaRJNVznz6nPysKd/H0/NSOxwMvgqgC7n+vz9Kq8dlfCm/9HXqMI2rahumrtzKse5ukU0lS2qhpQXsFcH+qfz9w5eELoihaGkXRslR/PbAZ6BRCCMD5wD/e6f2SpIblfaO607l1Ux6cmh8PjLw2bvPfhF+MSS6YlISZf4J9O+Ccr7N+eylbd+1jQt+cpFNJUtqoaUHbJYqiDan+RqDLOy0OIYwHsoEVQAegOIqi1M4grAVya5hHklTPNWuSyXXjejJlZRH5RbugeTu4+o/x5L6dyYaT6trq1+O23/m8vb4EgFO8QitJx+2YBW0I4YUQwoJqvq44dF0URREQvcPndAMeAD4eRVHFuw0aQrgxhDAzhDCzsLDw3b5dklSPXDysKwA3PTQ7HhhxDZz/7bigLV6TYDKpjhUuhlOugIwMFm0oIQQY3NWCVpKO1zEL2iiKJkVRNLyar8eATalC9UDBurm6zwghtAGeAm6NomhqargIaBdCOLDrQQ9g3TvkuCeKorwoivI6dep0/P+EkqR658AzggtTV6QA6HNW3D58QwKJpASUboetq6DzMADe3lBC75wWtGrqhlCSdLxqesvx48CBnzxuAB47fEFq5+J/AX+OoujA87IHrui+DFzzTu+XJDU8IQRuf+8pACzdtCMe7DEubreugOioN/xIDceaaUAEvScCsGhDibcbS9K7VNOC9i7gwhDCMmBS6jUhhLwQwh9Sa64FzgY+FkKYm/oanZr7BvDlEMJy4mdq/1jDPJKkNHHWoPhum2kri+KBjAx478/jq1aFixNMJtWR/Dchownk5rGjdD/5Rbs5pZsFrSS9GzW6pyWKoiLggmrGZwKfTPUfBB48yvtXAuNrkkGSlJ76dGgJwLcfW8hHJ/aJB/un/pMy9yG46AfJBJPqSsF06DYKsluwZPVWAIZa0ErSu1LTK7SSJJ2QzIzAOamrtCsLU7sbt+sJrbvDutkJJpPqQEU5bHgLcscC8e3G4A7HkvRuWdBKkhLzgyuHA/DC25sqB4e+N74Vc+G/Ekol1YGN82H/LuiRB8DUlUW0b9GErm2aJRxMktKLBa0kKTE9c1owpGtr7p+cXzl42mfj9skvJRNKqgtrUoc+9D4dgPnrtjOuTw4hhARDSVL6saCVJCVqYJfWrCvew869ZfFATl8Y96l4c6jtRz3NTUpvBdOgTQ9o24OS0v0UbN3DqJ7tkk4lSWnHglaSlKgPjusJwPDv/Jvpq+KNcRh7A0QV8eZQUkNUMB16xvtiLli3Hag8n1mSdPwsaCVJiZrYr8PB/rW/mxJ3uo6ATkNh5p+goiKhZNJJsi0fStYeLGhfWLSZEODUnu0TDiZJ6ceCVpKUqIyMwJu3nA9ATstsoiiKJyZ+DnZsgH98LLlw0skw/Z64HXgRe8vK+b9ZBYzs0Y62LZokm0uS0pAFrSQpcbntmvO9K4axddc+5hQUx4OjPxy3ix6Dyb9KLpxU21a8BC06QIf+/HXaGnaUlnHzeQOSTiVJacmCVpJUL+T1zgHg0w/MigcyMuD8b8f9526FvTsSSibVoqIVsHkRjLgWgMkrigA4b0jnJFNJUtqyoJUk1QundG9D344t2b23jP3lqedmz/4qvPfncf+ha5MLJ9WW/MlxO+ajACzfvJNLhnUlM8PjeiTpRFjQSpLqja9fPJhd+8qZt7a4cnDsx+J2zWQo25dILqnWFEyD5u2h01B27i1jddEuBnVplXQqSUpbFrSSpHpjYv94x+N/zFpbdeL82+L2B53qOJFUywqmQ4/xkJHBjFVbqYhgfN8Ox36fJKlaFrSSpHqjXYts+ndqyaINhz0vO+z9lf3S7XUbSqotu7fCliUHj+uZsrKI7KwM8vp4XI8knSgLWklSvTJpaBfeKijmoWn5lYMd+sNH/hn37+oFOzcnE06qibUz47bnBADmr93OkK6tadYkM8FQkpTeLGglSfXKJ8/qB8CdTy+uOjHggsr+vz5dh4mkWpL/BmRkQe4YKioiFqzfzojctkmnkqS0ZkErSapXOrVuysfP6MOufWUUbN1ddfITL8Rt8Zq6DybV1NLnoPfpkN2S5YU72VFaxqie7ZJOJUlpzYJWklTvfGh8L6IIHpl92OZQPcfBxXdA0XIoWZ9MOOlEbFwAhW/DwIsBmL5qKwAT+uYkmUqS0p4FrSSp3hnUpTUDO7fiH7PWEkVR1cmep8XtlF/XfTDpRK1+PW4HXgjA7PxtdGyVTa+cFgmGkqT0Z0ErSaqX/vPMvqzdtodbHplfdSJ3TNxO+ZU7Hit9rJsNrbtBp8EAzF6zjVN7tSeEkHAwSUpvFrSSpHrp2ryeAPx9ZgF/emNV5UQIMOg9cX/qbxNIJp2A9bOhe/zLmMIde1ldtJuxvT2uR5JqyoJWklQvZWYEvjhpIADfe3IRpfvLKyevezBuV75S98Gkd2tPcfzcd+6pADw4NT6S6swBHZNMJUkNggWtJKne+uKkQXz1okEAfOFvcyonMrPgvFthzeR4sx2pPluf+rObukI7ecUWmjfJZFj3NgmGkqSGwYJWklSvff78+CrtvxduqrpB1Ihr4nbpMwmkkt6FtTOAALljWFe8hxmrt3HN2B4+PytJtcCCVpJU733uvP4AfPqBWZWDOf2g42BYMy2hVNJxWvgv6DIcmrdn+qoiAK4Y3T3hUJLUMFjQSpLqvYuHdQXguUWb2F9eUTkx4IL4OdqdhckEk46lcAlsXgSnfhiA2fnFtMzO5NRebgglSbXBglaSVO+N7NGOz583AIBrfjuFfWWponbsx6BiP8z4fXLhpHeyZmrcDrwIgFn58XE9mRnebixJtcGCVpKUFg7sePxWQTHfeTy1EVSnwZA7Fl79EezdmWA66SjWzoDm7SGnHzv3lrF4YwljPK5HkmqNBa0kKS1kZWZwx1UjAFiwrqRyou/ZcfvqXQmkko5h9RvQYxyEwIzVW6mI8PxZSapFFrSSpLTxHxN6cfP5A1iwfjs7SvfHgxd8B0IGTP4llO9PNqB0qD3bYNsq6H06AE/P2wDA6J7tkkwlSQ2KBa0kKa2M75tDFMHfZxTEAyHAuE/G/cm/SC6YdLjNb8dt52EALN64gwGdW9G2eZMEQ0lSw2JBK0lKK+P65ADwg6feprwidS7tRT+M2xUvJ5RKqsb6uXHbdQSrtuxi/rrtXOlxPZJUqyxoJUlppVmTTLq3bQbAX6eviQezsuG0m2D167Bvd4LppEOsmQLtekObbryxfAsA5w7unHAoSWpYLGglSWnnkZviZxKnrCyqHOx3btw+8/U6zyMdobwM8t+EXhMBmFdQTE7LbIZ1b5NwMElqWCxoJUlpp1vb5nzktF48NW8Dxbv3xYMDJsXtnAe8SqvkbZwHu4ug//kAzFu7nZE92hKC589KUm2yoJUkpaX3joyfRXz8rfXxQEZmfNsxwJ/fl1AqKaVgetz2OZPd+8pYtnkHI3u4u7Ek1TYLWklSWhrfN4emWRnMXVNcOXjBf8ft2hkQRckEkwAKpkGbHtA2lwXrSqiIYFSPtkmnkqQGx4JWkpSWQgicP6QzbyzfQll5RTzYpDmcfnPc37wouXBq3KIIVr8BPccDMGfNNgCv0ErSSWBBK0lKW1edmsvmHXt5ZsHGysGxH4/bf96YTChpxwbYtRl6x5uXPbNgI307tqRT66YJB5OkhseCVpKUts4f0pncds35/esrKwfb943bTQuSCSVtWRq3HQdRsHU3cwuKuXpMbrKZJKmBsqCVJKWtrMwMBnRuxby123l+0aZ4MCMDLr4z7hevSS6cGq/CVEHbaTDTV20F4MJTuiYYSJIarhoVtCGEnBDC8yGEZam2fTVrRocQpoQQFoYQ5oUQrjtk7r4QwqoQwtzU1+ia5JEkNT53XT0ibp95u3IwdVQKbz+RQCI1epsXQrO20KoLcwuKaZmdyYDOrZJOJUkNUk2v0N4CvBhF0UDgxdTrw+0Gro+iaBhwCfCzEMKhuyJ8LYqi0amvuTXMI0lqZLq1bc7Efh1YUbiLBeu2x4OdBkObXPj3t6C0JNmAanw2zINuoyAEpq/aysge7cjM8PxZSToZalrQXgHcn+rfD1x5+IIoipZGUbQs1V8PbAY61fD7SpJ00K2XDQXgFy8uiwdCgN5nxP25f0kolRql8v2waSF0G8XufWUs2bSDCf1ykk4lSQ1WTQvaLlEUbUj1NwJd3mlxCGE8kA2sOGT4h6lbke8OIRx1+78Qwo0hhJkhhJmFhYU1jC1JakiG57alR/vmvLW2mOjA+bPv+0XcPvuN5IKp8SlcAuV7odtolm7aCcCQrm0SDiVJDdcxC9oQwgshhAXVfF1x6Loo/gniqKfYhxC6AQ8AH4+iKHVgIN8EhgDjgBzgqD91RFF0TxRFeVEU5XXq5AVeSVJVnzqrH5tK9lZuDtWkeeVkyYbq3yTVto3z4rbrSBauj2+BH9qtdYKBJKlhO2ZBG0XRpCiKhlfz9RiwKVWoHihYN1f3GSGENsBTwK1RFE095LM3RLG9wL3A+Nr4h5IkNT4XDYtvErrxgVmVg1f+Nm5/OgQqKqp5l1TLCpdARhPo0J8Zq7bSqXVTeuW0SDqVJDVYNb3l+HHghlT/BuCxwxeEELKBfwF/jqLoH4fNHSiGA/Hztx4aKEk6Id3aNmds73iz/WcXpK7IjryuckHh29W8S6plRcshpx9RyGDqyq2M75ND/GOOJOlkqGlBexdwYQhhGTAp9ZoQQl4I4Q+pNdcCZwMfq+Z4nodCCPOB+UBH4Ac1zCNJasS+d8UwAD7z4Ox4ICMDPjcj7q+ZklAqNRple6FgGnQYwJJNO9hYUsqZAzsmnUqSGrSsmrw5iqIi4IJqxmcCn0z1HwQePMr7z6/J95ck6VDDurfllG5tWLShhF17y2jZNAs6DIDsVvDUV+DUj0LWUfcflGpm+YuwqxBOuYKZq7cBcHr/DgmHkqSGraZXaCVJqle+fslgAF5cnNrWISMjPkoFYPIvE0qlRmHVa3E78EJm52+jY6tsn5+VpJPMglaS1KCc2jN+jva//jqncvCTL8Rt/uQEEqnRWDcLuo8hat6eaau2MrZ3e5+flaSTzIJWktSgtG3RhNbN4idqyitSp8l1GwljPwbrZrrbsU6Osr2wYS70Pp2iXftYV7yH8X293ViSTjYLWklSg3PHVSMAmLe2uHKw52lQuh0e/WxCqdSgTf0NlO+DnuNZsXknAAM7t0o4lCQ1fBa0kqQG54wBHcnMCDw2d33l4OBL4nbe3yqfqZVqywvfidveZ7KicBcA/S1oJemks6CVJDU4OS2zyW3XnPsmr2bZph3xYPP2cF1q0/2VrySWTQ3QgV+QdB8DLTuwdNMOWmRn0q1Ns2RzSVIjYEErSWqQPntufwD+Mn1N5eCASdCsHSx4JKFUapDWpc49PvOLACxcv51TurUhI8MNoSTpZLOglSQ1SB8c1xOAe99cTcWBzaGaNIe+Z7nbsWrXgeN6+pxFRUXEovUlDOveJtlMktRIWNBKkhqkQ49LeWr+hsqJPmdBcT4UrUgglRqk1a9BlxHQIofVRbvYta+cYd3bJp1KkhoFC1pJUoP12tfOA+Dmv86hdH95PDjo4rhd+mxCqdSgFC6Nr9C2yAFgwfoSAIbleoVWkuqCBa0kqcHq1aEFp/ePzwJduH57PNi+D3QaakGr2rHk6bgd90kAlm/aQUaAAe5wLEl1woJWktSg/fTa0QC8sayocnDwJfFztKXbE0qlBmPRo9CiIwx9LwArt+yiR/sWNM3KTDiYJDUOFrSSpAata9tmNMkM3P3CUop27o0HB14EFWUe36OaiSIoLoCuIyAEoihi/rrt9O3YMulkktRoWNBKkhq8Hu1bAHD3C0vjgdy8uH34+rgokU7E9gLYvQWGXAbA3IJi8ot2c9bAjgkHk6TGw4JWktTgPf75M8jOzGDOmuJ4ICsbuo2K+xvnJRdM6W393LjNHQPA06ndtC8b2S2pRJLU6FjQSpIavNbNmnDz+QNYuL6k8rbj6x6K29+dnVwwpbfNbwMh3mQMWLxxB7ntmtOtbfNkc0lSI2JBK0lqFM5I3Qb6vl+9GQ+061k5uXdHAomU9jYvinfNzm5BRUXE3IJizhncKelUktSoWNBKkhqFkbltAVhXvIfi3fviwQ/9LW4X/iuhVEprix6FDv0BeH35FnaUlnFqz3YJh5KkxsWCVpLUKGRlZvD4588A4NkFG+PBQZdATn+Y93CCyZSW1s2K2+z4vNkv/G0OAOP75iSVSJIaJQtaSVKjMSK3LbntmvPKksJ4IAQYeS2sfh22r002nNJL/uS4Pf/bRFFE8e79APTu4JE9klSXLGglSY1GCIEJfXOYsXor0YHjekZ8IG7vHgb7S5MLp/Qy+wFo1xs6DmBF4U4A7nr/iIRDSVLjY0ErSWpUJvbvQNGufSzaUBIPdOgPWaldaX82PLlgSh9l+2DbaugaF7BTVhQB8Z8tSVLdsqCVJDUq5wyKd6E9eNsxwBfeittdhdW8QzrMpgVQvheGXw3A5BVF5LZrTq+cFgkHk6TGx4JWktSodG7TjFO6teHVQwva1l3g1I/E/V+NTyaY0seBDaF65LF9z36eWbCR/p1bEUJINpckNUIWtJKkRufcwZ2YtWYbJaX7KwfHfSputyyBivJkgik9FEyHlp2hbU8ef2s9UHkslCSpblnQSpIanXMHd6a8IuK5hZsqB7uPhovvjPt/ujiZYKr/oijeFbvPmRACzy/aRFZG4IuTBiadTJIaJQtaSVKjc2qvdgB89f/eqtztGCp3PF47w2N8VL0ty2DHBuh7NgD5RbsY0LkVWZn+SCVJSfBvX0lSo9MkM4MLT+kCwLRVWysnWnWCm2fH/buHw96dCaRTvTb9nrjtdw7bd+8nv2g3l4/slmwmSWrELGglSY3SD6+Mj+j54D1Tq0506A/Nc4AI7syNbzGVDtg4H7KaQU4/5q/bDsDonu0TDiVJjZcFrSSpUercptnB/q69ZVUnP/1aZX/BI3WUSPXevl3xDscTPgPA4o3xWcZDu7VOMpUkNWoWtJKkRuv31+cB8ERqp9qD2vWEax+I+//+Vh2nUr21ZgpU7Ie+ZwGwbNNOOrbKpkOrpgkHk6TGy4JWktRoTRramYGdW/Ho3HVHTp7yPugxDnZugpL1R86r8VmXer6652kALN28g4GdvTorSUmyoJUkNVohBM4Y0JGpK7eyeUfpkQvO/HLc/umSug2m+qloBbTJhaat2FRSypw1xWRlhqRTSVKjZkErSWrULh0R71A7/ocvUlZeUXVy8HugXS8ozoe/fTiBdKpXNs6HTkMAeG1pIQAfntAryUSS1OhZ0EqSGrXxfXMO9j9+34yqkyHAja/G/cVPwpNfqsNkqldKS2DzIug5AYDH5q6nTbMsLjqla8LBJKlxs6CVJDV6r37tXABeX7blyMkWOXD1H+P+zD/FhY0an3WzgAh65BFFEYs3ltC3Y0syMrzlWJKSZEErSWr0endoyYfG9wTgg/dMOXLBiGvg/b+P+2/9rQ6Tqd5Yn9oQqvuprN22hy0793FNXs9kM0mSLGglSQK4fGR3AKau3EpFRXTkguHXQEYWLH++jpOpXiiYAR0GQIsc5hYUA3Bqz3YJh5IkWdBKkgScMaAj33xPvOHPTQ/NPnJBRgaM/Rgse67y+BY1DlEEBdMOHtczt6CYplkZDO7qkT2SlLQaF7QhhJwQwvMhhGWptn01a3qHEGaHEOaGEBaGED5zyNzYEML8EMLyEMIvQgg+jCJJSsRlI+Mdj59duJEtO/ceuWD8jXH7h0mwvZqza9UwFUyDPVuh53gApq0qYlTPdjTJ9LqAJCWtNv4mvgV4MYqigcCLqdeH2wBMjKJoNDABuCWE0D019xvgU8DA1JeH/UmSEtGjfQue/q+zAPjQPVOPvPW402Bo0hKicrj7lPjKnRq+yb+M275nU7x7HwvXl3BG/47JZpIkAbVT0F4B3J/q3w9cefiCKIr2RVF04FfdTQ983xBCN6BNFEVToyiKgD9X935JkurKKd3bALBs805O/X41z8t+4L7K/nd9hrLBiyIomA5D3wc5fXl4ZgFRBGcM6JB0MkkStVPQdomiaEOqvxHoUt2iEELPEMI8oAD4URRF64FcYO0hy9amxiRJSsxT/3UmANv37Oet1AZABw26CL6RX/m6fH8dJlOd274Wdm2GvmcDMDs//vMwyg2hJKleOK6CNoTwQghhQTVfVxy6LnWVtdr7r6IoKoiiaCQwALghhFBt4fsOGW4MIcwMIcwsLCx8N2+VJOldGda9LZ88sy8An/zzzCMXNG8HH0od3/PEF+owmercullxmzuWioqIaauKuGZsD5+flaR64rj+No6iaFIURcOr+XoM2JS6dfjALcSbj/FZ64EFwFnAOqDHIdM9UmPVve+eKIryoijK69Sp0/HEliTphN12+Sm0zM6kcMdeSveXH7lg4MXQrhfMfQhK1td9QNWNgumQ2RS6DGPJph1s272fif283ViS6ova+PXi48ANqf4NwGOHLwgh9AghNE/12wNnAktStyqXhBBOS+1ufH1175ckKQnfed8wAD78h2lHTmZkwJW/jfs/Heqtxw3V5oXQZRhkNWX6qq0ATOiXk3AoSdIBtVHQ3gVcGEJYBkxKvSaEkBdC+ENqzVBgWgjhLeBV4MdRFM1Pzd0E/AFYDqwAnqmFTJIk1diVo+NtHWblb+PvM9YcuaD36ZX95S/WUSrVmYoK2PAWdD4FgJWFO2nVNIvcds0TDiZJOqDGBW0URUVRFF0QRdHA1K3JW1PjM6Mo+mSq/3wURSOjKBqVau855P0zU7cv94+i6POp53AlSUpcdlYGr33tPAC+8ch8Hpt72FMxIcCX3477f72ujtPppMt/E/ZsO/iLi+WFO+nbsSXxTWWSpPrAHQ0kSXoHvTq04BOpDaK+8Le5Rz5P26Z7ZT9/Sh0m00m3PHVsU6/TKK+IeKtgO6N6tk02kySpCgtaSZKO4dZLh/KFCwYC8PT8DUcu+HxqJ9x7L6nDVDrptiyDnH7QoT9LN+1g594y8nr7/Kwk1ScWtJIkHUNGRuCz5/YH4KXF1Wzm33FAZX/Z83WUSifdutmQmwdw8Dzi0Z4/K0n1igWtJEnHoVmTTN5/ai5PzttAftGuIxd86qW4fega2FfNvNJLyXrYuRFyxwAwZ00xbZs3oXeHFgkHkyQdyoJWkqTj9OHTegHwy5eWU1Fx2B6GuWMr+2um1mEqnRRrUs9Dp/53nZm/lbG927shlCTVMxa0kiQdp7Gp5yf/MWstg79dzSlzX10Wt6/cVYepdFKseAmatYPcsWzdtY8VhbvI69M+6VSSpMNY0EqS9C5874phAOwvj/jIH6ZVnWzVGU77HKydDmumVfNupYWKCpjzYHxcT0YmP31+CQDj+rghlCTVNxa0kiS9C9dP7MNzXzobgDeWb+GI49Mn3hS3S56q42SqNW/8NG5z+gEwZUUR/Tq1JK+3V2glqb6xoJUk6V0a1KU1t102FIAHpuZXnWzbAzoMgK0rE0imWrHhrbg971a279nPisJdvP/UXJ+flaR6yIJWkqQT8B8T4g2i/vuxhZTuL6862XkovP1EvFOu0ksUxRtCjbwOslswb+2B43q8OitJ9ZEFrSRJJ6BFdhY/u240ANf/aXrVydTZpfx0KOzYVMfJVCNblsGuQuh9BgCz8+OCdkSPtkmmkiQdhQWtJEkn6PKR3QCYvmorb28oqZw4/ebK/rO31HEq1cja1C8nek0EYP667Qzo3Iq2zZskGEqSdDQWtJIknaCszAx+dPUIAK773ZTKiYxM+HYRZDaFhf+EVa8nlFDv2rrZ0LRN/Bw0sHhjCf07tUw4lCTpaCxoJUmqgevGxc/SlpSWsXXXvsqJzCz4j7/H/Wm/TSCZTkjBdOg6EjIyWF+8h7Xb9jC+b4ekU0mSjsKCVpKkGvrJB0YB8OWH51ad6H9e3C5+so4T6YTs3QGb5kOf+PnZ/5u5FsDjeiSpHrOglSSpht4/JheAV5YUsmH7nqqTnYbE7dZVdZxK79rGBXHbfQwAywt3AjCqZ7ukEkmSjsGCVpKkGgoh8LWLBwMw8c6XqKiIKic/+Je4/f15UFGRQDodt9WpZ527jQRg4brtTBraJcFAkqRjsaCVJKkWfO68AQf7K7fsrJzI6Re3e7bB97x1tV57NOI8CgAAHD5JREFU+Ydx27ob2/fsZ+WWXYzyuB5JqtcsaCVJqiXPfvEsAH7zysrKwRDg5tmVr7fl13EqHZeS9ZX9EHirID5/dqS3G0tSvWZBK0lSLRnUuTXj+rTnsbnr2FG6v3KiQ3/4/My47wZR9dP0e+L2Ey8A8PKSzTRrksG4Pl5Vl6T6zIJWkqRakpER+MpFgymriJi8oqjqZMeBcfvvb8HurXUfTu9s4wJo0hJ6jgPgqXkbGNOrPS2ysxIOJkl6Jxa0kiTVojG92tMyO5PXlxUeOTnqQ3H7yCfqNpSObdMCGPpeAB6eWcDmHXvJbdc84VCSpGOxoJUkqRZlZ2Uwskc75qaewaziiv+N2xUvQRQdOa9k7CqCHRug63AAXl0S/zLiKxcNTjKVJOk4WNBKklTLBndtzYJ1JfS55SnKDz3CJyMDLvx+3H/gymTC6Ugb58Vtl+FEUcSs/G1cNrIbXds2SzaXJOmYLGglSaplHzmt98H+I7PXVp0c9cG4XfkKLH2u7kLp6A78cqHrSFYX7WZjSSkT+3VINpMk6bhY0EqSVMsGdG7FU/91JgBf/8c8CnfsrZxs1Rk+8kjcX/JUAulURWlJZb9lB2aujjfsGt83J6FAkqR3w4JWkqSTYFj3tnw0daX2vx9bUHVywCTocxYUTE8gmapYlzpO6arfAfDW2mJaN81iQKdWCYaSJB0vC1pJkk6S710xDIBnFmw8crLTENi8CGY/UMepVMXKVyFkwOBLAZi5ehundG9DRkZIOJgk6XhY0EqSdJKEEMjOiv9Tu2tvWdXJEdfE7eOfh7K9KCFz/wIdB0GzNuwo3c/ijTs4zednJSltWNBKknQS/emGcQA8ODW/6kSv0+CUK+L+rPvrOJUA2LERdm0++L/DovXx87Sje7ZLMpUk6V2woJUk6SQ6rV+8udCdzyzmS3+fW3Xy6j9BVjMomJZAMvHG3XHb9xwAXnh7EwDDctsklUiS9C5Z0EqSdBJlZWZw49n9APjXnHXsK6uonMzMgn7nwaYFR3m3TqqtK+O29+kA/P71VQB0bu35s5KULixoJUk6yb516VDuev8IAJ5ZsKHqZOehULjYzaHqWmkJLHsOuoyAEFi+eScATbP80UiS0ol/a0uSVAc+kNeTFtmZ/PyFZURRVDnR/dS4ffzzsGVZMuEao4X/jNv28dFK/zezAIBHP3dGUokkSSfAglaSpDqQmRFo1TSLlVt28e+FmyonhlwO534z7j//38mEa2yiCJ74Qtz/QLwh16INJfTu0IKh3Xx+VpLSiQWtJEl15MFPTgDgMw/OqhzMyIBzb4Gh74UlT8c77+rk2hY/K8uo/4ifYwbe3lDCuD45CYaSJJ0IC1pJkurIoC6t6dY23nBo6sqiqpPDU+fSvnIX7NgUX0XUybFudtye9lkANu8oZcvOfZzi1VlJSjsWtJIk1aHffXQsAB+8ZyqFO/ZWThw8k/Ze+MkgmO3ZtCfN3L9AZna8IReV58+e0t2CVpLSjQWtJEl1aGSPdmSEuP+7V1dUToRQdeHbT9ZdqMakvAzWz4mL2cwmAHzs3hmABa0kpSMLWkmS6tiKOy6lfYsmLEsdFXPQF+fDWV+Jn+1cMwX270kmYENWMBX2bI3/HRPfbnxAm2ZNkkolSTpBNSpoQwg5IYTnQwjLUm37atb0DiHMDiHMDSEsDCF85pC5V0IIS1Jzc0MInWuSR5KkdBBC4KJTuvLq0kJWFh5S1LbrBRf8Nwy/GvbthMm/TC5kQ7XqNSDAqA8CMHXlVgAe+ezEBENJkk5UTa/Q3gK8GEXRQODF1OvDbQAmRlE0GpgA3BJC6H7I/IejKBqd+tpcwzySJKWFId1aAzDpp68eOdn/PGjSEt74GWxbXbfBGrrlL0CPPGjeDoApK4po3TSLUT3aJRxMknQialrQXgEc2LXifuDKwxdEUbQviqIDu140rYXvKUlS2vvY6X0AqIjg5cWH/T43IxNO+wzs3wU/H1X34Rqq0u2wbhb0OfPg0NSVRYzvm0NWpj+eSFI6qunf3l2iKNqQ6m8EulS3KITQM4QwDygAfhRF0fpDpu9N3W787RAO3xGjymfcGEKYGUKYWVhYWMPYkiQlK4TAy189F4AXF286csGIayv7s/9cN6Eauvsuj9suwwFYUbiTVVt2MbF/hwRDSZJq4pgFbQjhhRDCgmq+rjh0XRRFEVDtoXlRFBVEUTQSGADcEEI4UPh+OIqiEcBZqa+PHi1HFEX3RFGUF0VRXqdOnY7zH0+SpPqrb8eWTOibc/DYmCo6D4GvLI37j99ct8Eaol1bYOO8uD/sKgB+/fJyAAtaSUpjxyxooyiaFEXR8Gq+HgM2hRC6AaTad3wGNnVldgFx8UoURetS7Q7gL8D4mv3jSJKUXoZ2a8PsNcXkF+06crL1ITc+/ebMI+d1/BY/Fbfn3xbf0g1s2bkPgGHd2yaVSpJUQzW95fhx4IZU/wbgscMXhBB6hBCap/rtgTOBJSGErBBCx9R4E+By4mJXkqRGY0DnVgCc8/9eoXR/+ZELbpoWt5vme4xPTax+A1p2grO+CsCqLbt4bWkhnVs3TTiYJKkmalrQ3gVcGEJYBkxKvSaEkBdC+ENqzVBgWgjhLeBV4MdRFM0n3iDq36lna+cC64Df1zCPJElp5b0jKzf+//6Ti45c0HkIXHVP3J91X92EamhevhPmPwzt+0Bqu477J68G4AuTBiaXS5JUYyF+9DW95OXlRTNnzkw6hiRJtaK8IqL/t54GYPkP33PkjrulJXBXz7h/+/Y6TpfmDv13d+VvYfSHiKKIiXe+RFlFxMzbJiWbT5JUrRDCrCiK8o61zj3qJUlKWGZG4LbLhgLw1xkFRy5o1gaGpHbo3VTNVVwd3YGr2qliFmB10W42lpR6dVaSGgALWkmS6oEPT+gNwLcfXUCfW55iVv62qgsu+kHc/mYibF1Vx+nS2NJnodvog8UswKNz1gFwhrsbS1Las6CVJKkeaJ6dWeX1H15fWXVBTt/K/sPX10GiBmDV65D/JrTrWWV42qoisrMy6NuxZULBJEm1xYJWkqR64pHPTmREblvyerdnZv42yisO2+fi9u3QtE18nuorP0omZLq451y4P3Wbdp+zDg7v3FvGnDXFXJfXk5DaIEqSlL4saCVJqifG9s7hiZvP5KMTe1O4Y+/BW2OreO/P4vaVO+D2trBvd92GTAebFsL6OZWvx33qYPf1pYXsLavg0hHdEggmSaptFrSSJNUzk4Z2AeDBafmUlVdUnRx+NVz648rXf7qoDpOlgd1b4Tenx/1zvhFf1c6o/HHnhbc307Z5E8b1aZ9QQElSbbKglSSpnmnZNIsbz+7HnDXFvO9Xbx65YPyn4DvFcX/jfHjyy3UbsL6a8xD8zyHPGp/3rSrTURQxZcUWzhjQ4cijkSRJacm/zSVJqofOHtgJgEUbSlhXvOfIBSHAZT+J+zP/CGV76zBdPbF2Znzb9c9GxLdeP3ZT5dx/PnfE8vsnr2b99lJG9mhXhyElSSeTBa0kSfXQmQM78uf/HA/AGXe9VP2icZ+Eq34X95+7rY6S1SNPfilui9fAHalnYnuMj28z7jWhytKKiojbn4jP8H3vqO51mVKSdBJZ0EqSVE+dPajTwf7DMwqqXzTsqridfg/sL62DVAmLovg52d1bYdMC6HV61fnrH632bQvWbwdgYOdW5LZrfrJTSpLqiAWtJEn12P9cPRKArz8yjyiKjlyQ1RTOuzXuP/KJOkyWkHsvjZ+T/Z++EFXAe+6CryyNr1bf8ARkV3+27GtLCwH4642n1WVaSdJJZkErSVI9du24nnRs1RSAN5cXVb/ozNSmUIufhNKSOkqWgKIVsGZy1bGuI6F1l/h54r5nV/u20v3l/Pi5pXRq3fTgv0tJUsNgQStJUj33xM1nAPCDpxZVvyAzq/Ks1bt61lGqOrb0OfjlmLg//GroPgZu3RRvjvUOKioihnz7WQAuGNL5ZKeUJNUxC1pJkuq5bm3jZz4Xb9zBhu3V7HgMcMmdlf1nvwXlZXWQrI789UPwlw9Uvr7mT3Djy9Ck2THf+vKSzQf7d75/xMlIJ0lKkAWtJElp4L6PjwNg4p0vsWVnNUf0ZDaBS+6K+1N/Da/ceeSadLR1JSx5uvL1LUfZHOso/jFrLV3aNGXZD99DOMbVXElS+rGglSQpDRw4lxbgG/+YV/2i8TdCiw5x//UfQ0VFHSQ7SfInw/99HH5xavw6pz98bQU0a/OuPuaZBRsZ1yeHJpn+yCNJDZF/u0uSlAYyMgKvf/08AF5cvJlXU7v2Vl2UCV9fWfk8bf4bdZjwBOzZBre3jb9mP1A5XrgE7n0PLPxn5djNs6Blx+P+6Psnr6bPLU8B0KZ5k9pKLEmqZ0K1RwDUc3l5edHMmTOTjiFJUp27781V3P5E5eZQK++4lIyMw26l3b01PtYG4NaN0KQenbu6bxf8T38o2wNZzaDsOM7O/fiz0HvicX+L8oqI/t+qvE151m2T6ODuxpKUVkIIs6IoyjvWOq/QSpKURm44vQ9DurY++PrBaflHLmqRU9lf+K86SHWcdmyEH/WJi1l452K2bU+Y8BnoMhy6jz7ub7G/vOJgMduvU0sWf/8Si1lJasC8QitJUhras6+cof8dH0fz/SuH89HTelddUL4fvp+6RbfnBPjEc3Wc8DCPfArmP3zk+Nlfh3O+AXfmQhRBeWrDq2+th+yW7+pbFGzdzVn/8/LB10t/8B6ys/zdvSSlo+O9QptVF2EkSVLtap6dyeUju/HkvA18+9EFfGRCr6q7+GY2gdEfgbkPQsE0+G57+OwU6Dyk7sMuebZqMXvjK9D91Pj24wNF622b4raiPH4W+DhsLinli3+fyxkDOvK58wZUKWa/+Z4hFrOS1Ah4hVaSpDT221dXcNczi/n6JYO56dwBRy6Y8Ud46suVr6/9M5xyRd0FXPpc5Rmy19wbf+/jLFiP5WP3TueVJUdujrXqzks9okeS0pzP0EqS1Ah8JHWr8a9fWk61v6Qe9wm4/O7K1w9fX0fJiI8NOlDMAgx/f60Vs3vLypmdv+2I8Rm3TrKYlaRGxIJWkqQ01qppFtdP7M2ufeXc9czi6hfl/Sfcvh06DoJm7eouXOEheW7fXisf+fcZa+hzy1MMvu1ZSkrLuPdj4/jeFcMA+NHVI+jU2g2gJKkxsaCVJCnN3fKe+LnY3722kvKKd3iUaNQHobQY3vo75E8++cHWzojbz8+qlY8r3r2Pbzwyv8rYWQM7cv3EPqy+6zKuG9erVr6PJCl9WNBKkpTmWmRncemIrgBMuOOFoy/sNDRu/3Uj3PseuLMnlNbOldNqrZ0BzdtDh/41/qjNJaWM/t7zVcZe+eq5ZGX6o4wkNWb+V0CSpAbg/10zCoAtO/exdNOO6hf1OGxvjb0lcFcv+N+JULK+9kOtnQE9xkENn2n9+QvLGH/Hiwdfr7zjUmbdNok+Hd/dsT6SpIbHglaSpAagZdMsvnDBQAAuuvs1/jJtzZGLWnWG6x+H82+Lj/Q5YPMi+OnQ+BzY2rJ1VfwMbfcxNfqYop17ufuFpQdfP/vFs8jICHRo5bOykiQLWkmSGozPnFN5a+93n1hY/aJ+58DZX4Mrfw3fKYbcsZVzM/4AZXtrXthGEfxidNzvf94JfcTijSX8bfoaxv6g8hbqJ28+kyFd29QsmySpQbGglSSpgWiencnquy5j0tAu7C2rYMnGo9x6fEAI8KmX4ObZ8eunvwo/6AzfbQdrpp1YiCiK3w/QshP0Ou2oS/eWlXPpz1+nzy1P8fyiTQfHC3fs5ZKfvc4t/6zcAGrlHZcyPLftiWWSJDVYodoz6+q5vLy8aObMmUnHkCSpXlq+eSeTfvoqAKvuvPT4zmWd8mv497eqjnU+Jb4d+cP/gIEXHt83n/MgPPa5uH9LATSrekV1b1k5H793BnPWFDM8tw0zVlc9S7ZNsyxKSsuqjD30yQmcMaDj8X1/SVKDEEKYFUVR3rHWeYVWkqQGZkDnVgf7Ly3efHxvmvg5uGkqfOA+yGgSj21eFLcPXQPlZUd9KwBPfhlub1tZzH5z3RHF7M69ZVz168lMXlHEnv3lRxSzQJVi9v1jcunQMpuRPbwyK0mqngWtJEkN0LRvXQDAJ+6fyYNT84/vTZ2HwrCr4NuFR85tmn/k2AF/+zDM/GPl6wGToGmrKktuf3whw7/zbxZtKKkyfvnIbiz47sVHfOT0Wy/g/10ziinfvIDWzZocX35JUqPjLceSJDVQF9/9GktSR/hcPaYH/335KbRtcZzFYeESqCiPr7LePSweu/FV6D66cs22fNiyDB66On49+iMw4hrod26Vo3omL9/Cf/yh8pnc+/9zPOcM6sTabbvJbdecEAIHfh7Zvmc/LbKzyM7yd+6S1Jgd7y3HFrSSJDVQe/aVc+aPXqJo176DYy995Rz6dWr1Du+qxu2H3PJ7WyFkZce7If+gc+X4ebfCOV8/4q3TV23l2t9NAeBn143mkuFdadYk8919f0lSo+MztJIkNXLNszOZ9e0Lef+Y3INjBzaLeldu3w7tesX9H3aBuX+pWsz2O7faYraiIjpYzAJcMbq7xawkqVZZ0EqS1MD95AOjWJh6TjUz4zh2PK7O51N3RkUV8OhnK8c//ix89NFq37Js886D/ePebVmSpHfBglaSpAYuhEDLpll8/rwBVEQwbWURSzcd44zaw2U1hWv/XPl6/I3w1eXQe2KV52UPNTN/KwCvfPVci1lJ0kmRlXQASZJUNwZ3bU15RcR190wF4LzBnfjdR/OOfwOmU66A/5oDTdtAy2OfCzsrfxsdWmbTu0OLmsSWJOmoanyFNoSQE0J4PoSwLNW2f4e1bUIIa0MIvzpkbGwIYX4IYXkI4RfBX+FKknRSjMitep7ry0sKGXTbM1z56zfZXFJ6fB+S0++4ilmA2fnbGNu7vVdnJUknTW3ccnwL8GIURQOBF1Ovj+b7wGuHjf0G+BQwMPV1SS1kkiRJh+nTsSVfuXAQp/fvwKAulTsdzy0oZvwdL1KbJx8sWLed1UW7GdWzXa19piRJh6uNgvYK4P5U/37gyuoWhRDGAl2A5w4Z6wa0iaJoahT/V/TPR3u/JEmquZsvGMhfPnUaz33pHFbdeWmVwvYnzy1lU0kp+8sravQ9Nmzfw+W/fAOASUO71OizJEl6J7VR0HaJomhDqr+RuGitIoSQAfwE+OphU7nA2kNer02NSZKkkyyEwHNfOofXvnYeAL96eTkT7niRgbc+w31vrjqhzywrr2DinS8BML5PDoO7tq61vJIkHe64CtoQwgshhAXVfF1x6LrUVdbq7le6CXg6iqK11cwdlxDCjSGEmSGEmYWFhSf6MZIk6TC9OrTgR1ePqDJ2+xOL6HPLU/S55Sm++8TC4/qcKIr49mMLAMjOyuChT02o9aySJB0q1PR5mRDCEuDcKIo2pG4hfiWKosGHrXkIOAuoAFoB2cD/Aj8HXo6iaEhq3YdSn/Xpd/qeeXl50cyZM2uUW5IkVbW5pJQde8t4eEYBv3ttZZW5t793Cc2zM6t9X3lFRP9vPV1lbPkP30NWpqcDSpJOTAhhVhRFecdaVxv/pXkcuCHVvwF47PAFURR9OIqiXlEU9SG+7fjPURTdkrpVuSSEcFpqd+Prq3u/JEk6+Tq3aUb/Tq345qVDad0sPtkvKyPeofiXLy076vtGffe5Kq9vu2yoxawkqU7Uxjm0dwEPhxA+AeQD1wKEEPKAz0RR9MljvP8m4D6gOfBM6kuSJCVoxq2TANi7v4JR33uO/31lBcNz23LpiG4H1/x74Ub+95UV7NxbBsC82y+iTbMmieSVJDVONb7lOAnecixJUt354t/m8Ojc9QD8+4tnM7hra5Zv3smkn756cM2fPpbH+UPc0ViSVDuO95bj2rhCK0mSGrCfffBU2rXI5r7Jq7n4Z6/Rp0MLVhftPjh/38fHce7gzgkmlCQ1Vj7gIkmSjun29w072D+0mF1xx6UWs5KkxFjQSpKk4/LCl8852J/+rQtYfddlZKY2jZIkKQnecixJko7LgM6tWH3XZUnHkCTpIK/QSpIkSZLSkgWtJEmSJCktWdBKkiRJktKSBa0kSZIkKS1Z0EqSJEmS0pIFrSRJkiQpLVnQSpIkSZLSkgWtJEmSJCktWdBKkiRJktKSBa0kSZIkKS1Z0EqSJEmS0pIFrSRJkiQpLVnQSpIkSZLSkgWtJEmSJCktWdBKkiRJktKSBa0kSZIkKS1Z0EqSJEmS0pIFrSRJkiQpLYUoipLO8K6FEAqB/KRzqF7oCGxJOoRUS/zzrIbEP89qaPwzrYYkHf48946iqNOxFqVlQSsdEEKYGUVRXtI5pNrgn2c1JP55VkPjn2n9//bu3kWuMowC+HkwqL2FhR+oICKk0cJGlG0EC1EQEcXONAn4B2hlmc5CLKL4gQZUQkgRVLALBhRMqwlKiIWxCYqIsRMfi0wR15Bk3Jm9815/v27ussMpDpd79p3dnZM59dlHjgEAABiSQQsAAMCQDFpG99bUAWCF9Jk50WfmRqeZk9n02e/QAgAAMCQntAAAAAzJoAUAAGBIBi0AAABDMmiZpap6pKoOVdXbVfXl1Hlgp6pqq6pOLnq9NXUe2Imqun/R5aNVdWDqPLATVXVPVb1TVUenzgL/1cg9NmjZOFX1blVdqKpvtl1/vKq+q6qzVfXy1d6ju0929/4knyR5f5154VpW0ekkneRikpuTnF9XVriWFd2jzyzu0c8meXideeFqVtTnc929b71JYXnL9HvkHvsrx2ycqno0lx7cP+juvYtrNyT5PsljufQwfyrJ80luSHJw21u82N0XFt93JMm+7v59l+LDv6yi00l+7u6/qurWJK919wu7lR8ut6p7dFU9meRAksPd/eFu5YfLrfiZ42h3P7Nb2eFalul3d59efH24Hu+ZOgBs191fVNVd2y4/lORsd59Lkqr6OMlT3X0wyRNXep+qujPJb8YsU1tVpxd+TXLTOnLC9VhVn7v7eJLjVfVpEoOWSaz4/gwbZZl+Jzm9u+lWx0eOGcVtSX687PX5xbWr2ZfkvbUlgp1ZqtNV9XRVvZnkcJI31pwNlrVsn7eq6vVFpz9bdzhY0rJ9vqWqDiV5oKpeWXc42KEr9nvkHjuhZba6+9WpM8CqdPexJMemzgGr0N0nkpyYOAasRHf/kmT/1DlgJ0busRNaRvFTkjsue3374hqMSqeZE31mTvSZOZtdvw1aRnEqyb1VdXdV3ZjkuSTHJ84EO6HTzIk+Myf6zJzNrt8GLRunqj5K8lWS+6rqfFXt6+4/k7yU5PMkZ5Ic6e5vp8wJ10unmRN9Zk70mTn7v/Tbv+0BAABgSE5oAQAAGJJBCwAAwJAMWgAAAIZk0AIAADAkgxYAAIAhGbQAAAAMyaAFgA1VVRenzgAAm8ygBYCBVNWeqTMAwKYwaAFgw1XVVlWdrKrjSU5PnQcANoWf8gLAGB5Msre7f5g6CABsCie0ADCGr41ZAPgngxYAxvDH1AEAYNMYtAAAAAzJoAUAAGBI1d1TZwAAAIClOaEFAABgSAYtAAAAQzJoAQAAGJJBCwAAwJAMWgAAAIZk0AIAADAkgxYAAIAhGbQAAAAM6W9vSGOUoarWVAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a, *_ = transform(votes, votes, votes)\n",
    "train_model((a, answers), (a, answers), 64, True)[1].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(train, val, test):\n",
    "    trans = compose.ColumnTransformer(\n",
    "        [(\"onehot_id\", preprocessing.OneHotEncoder(handle_unknown=\"ignore\"), [0])],\n",
    "        remainder=\"passthrough\",\n",
    "        sparse_threshold=0\n",
    "    )\n",
    "    train = pd.DataFrame(trans.fit_transform(train), index=train.index)\n",
    "    val = pd.DataFrame(trans.transform(val), index=val.index)\n",
    "    test = pd.DataFrame(trans.transform(test), index=test.index)\n",
    "    to_drop = []\n",
    "    for i in train.columns[:-4]:\n",
    "        if train[i].sum() < 20:\n",
    "            to_drop.append(i)\n",
    "    return train.drop(to_drop, axis=1), val.drop(to_drop, axis=1), test.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "FOLDS = 10\n",
    "\n",
    "def train_oof(train_set, test_set, units=64):\n",
    "    x_train, y_train = train_set\n",
    "    x_test = test_set\n",
    "\n",
    "    y_oof = pd.DataFrame(0, index=y_train.index, columns=COORDINATES)\n",
    "    y_pred = pd.DataFrame(0, index=x_test.index.unique(), columns=COORDINATES)\n",
    "    scores = []\n",
    "    folds = model_selection.KFold(n_splits=FOLDS, shuffle=True)\n",
    "    \n",
    "    for n, (index_train, index_valid) in enumerate(folds.split(y_train), 1):\n",
    "        print(f\"\\nFold - {n} / {FOLDS}\")\n",
    "        \n",
    "        ids_train = y_train.index[index_train]\n",
    "        ids_valid = y_train.index[index_valid]\n",
    "        \n",
    "        x_train_t, x_val_t, x_test_t = transform(x_train.loc[ids_train], x_train.loc[ids_valid], x_test)\n",
    "        \n",
    "        data_train = (x_train_t, y_train.loc[ids_train])\n",
    "        data_val = (x_val_t, y_train.loc[ids_valid])\n",
    "        \n",
    "        rez, cycle, model = train_model(data_train, data_val, units)\n",
    "        \n",
    "        cycle.plot()\n",
    "        pd.DataFrame(rez.history)[[\"loss\", \"val_loss\"]].plot(figsize=(16, 8))\n",
    "        scores.append(min(rez.history[\"val_loss\"]))\n",
    "        \n",
    "        feat = yield_batch_test(data_val[0])\n",
    "        df = model.predict_generator(feat, steps=len(data_val[0].index.unique()))\n",
    "        df = scaler.inverse_transform(df)\n",
    "        y_oof.loc[ids_valid] = df\n",
    "        \n",
    "        feat = yield_batch_test(x_test_t)\n",
    "        df = model.predict_generator(feat, steps=len(x_test.index.unique()))\n",
    "        df = scaler.inverse_transform(df)\n",
    "        y_pred += df / FOLDS\n",
    "\n",
    "    print(f\"IOU  : \" + str(-np.round(sorted(scores), 5)))\n",
    "    print(f\"IOU : {-np.mean(scores):0.5f} +/- {2 * np.std(scores) / len(scores) ** 2:0.5f}\")\n",
    "\n",
    "    subdir = time.strftime('%Y-%m-%d_%H-%M')\n",
    "    path = pathlib.Path(f\"../processed/{subdir}\")\n",
    "    path.mkdir(exist_ok=True)\n",
    "    y_oof.to_csv(path / f\"oof-{-np.mean(scores):0.5f}.csv\", header=False)\n",
    "    y_pred.to_csv(path / f\"sub-{-np.mean(scores):0.5f}.csv\", header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold - 1 / 10\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 29)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 25)     0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 1)      26          lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None, 4)      0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 5)      0           conv1d_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 64)     1024        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, None, 64)     12352       conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, None, 64)     12352       conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, None, 64)     12352       conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, None, 64)     12352       conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, None, 64)     12352       conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, None, 4)      260         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 4)            0           conv1d_8[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 63,070\n",
      "Trainable params: 63,070\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.7310 - intersection_over_union: -0.0070 - val_loss: 0.6450 - val_intersection_over_union: -0.0282\n",
      "Epoch 1/100\n",
      "  34/1000 [>.............................] - ETA: 20s - loss: -0.0458"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WLMike/Documents/PycharmProjects/ML_venv/lib/python3.6/site-packages/ipykernel_launcher.py:78: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.3183 - val_loss: -0.3479\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to -0.34790, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.1663\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.3761 - val_loss: -0.3558\n",
      "\n",
      "Epoch 00002: val_loss improved from -0.34790 to -0.35580, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0518\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4105 - val_loss: -0.3869\n",
      "\n",
      "Epoch 00003: val_loss improved from -0.35580 to -0.38689, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0290\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4362 - val_loss: -0.4049\n",
      "\n",
      "Epoch 00004: val_loss improved from -0.38689 to -0.40492, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0313\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4296 - val_loss: -0.3751\n",
      "\n",
      "Epoch 00005: val_loss did not improve from -0.40492\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0121\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4412 - val_loss: -0.4215\n",
      "\n",
      "Epoch 00006: val_loss improved from -0.40492 to -0.42152, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0147\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4392 - val_loss: -0.4355\n",
      "\n",
      "Epoch 00007: val_loss improved from -0.42152 to -0.43553, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0028\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4642 - val_loss: -0.3942\n",
      "\n",
      "Epoch 00008: val_loss did not improve from -0.43553\n",
      "Learning rate: 7.4e-04\n",
      "Speed per epoch: -0.0150\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4748 - val_loss: -0.4455\n",
      "\n",
      "Epoch 00009: val_loss improved from -0.43553 to -0.44549, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.4e-04\n",
      "Speed per epoch: -0.0117\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4844 - val_loss: -0.4455\n",
      "\n",
      "Epoch 00010: val_loss improved from -0.44549 to -0.44554, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.4e-04\n",
      "Speed per epoch: -0.0113\n",
      "Wait to stop: 2\n",
      "\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4755 - val_loss: -0.4550\n",
      "\n",
      "Epoch 00011: val_loss improved from -0.44554 to -0.45499, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.4e-04\n",
      "Speed per epoch: -0.0061\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4718 - val_loss: -0.4224\n",
      "\n",
      "Epoch 00012: val_loss did not improve from -0.45499\n",
      "Learning rate: 6.3e-04\n",
      "Speed per epoch: 0.0019\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5021 - val_loss: -0.4429\n",
      "\n",
      "Epoch 00013: val_loss did not improve from -0.45499\n",
      "Learning rate: 5.0e-04\n",
      "Speed per epoch: -0.0160\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4840 - val_loss: -0.4405\n",
      "\n",
      "Epoch 00014: val_loss did not improve from -0.45499\n",
      "Learning rate: 5.0e-04\n",
      "Speed per epoch: -0.0014\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5075 - val_loss: -0.4599\n",
      "\n",
      "Epoch 00015: val_loss improved from -0.45499 to -0.45987, saving model to ../processed/model3.h5\n",
      "Learning rate: 5.0e-04\n",
      "Speed per epoch: -0.0061\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5212 - val_loss: -0.4601\n",
      "\n",
      "Epoch 00016: val_loss improved from -0.45987 to -0.46012, saving model to ../processed/model3.h5\n",
      "Learning rate: 5.0e-04\n",
      "Speed per epoch: -0.0099\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5177 - val_loss: -0.4636\n",
      "\n",
      "Epoch 00017: val_loss improved from -0.46012 to -0.46363, saving model to ../processed/model3.h5\n",
      "Learning rate: 5.0e-04\n",
      "Speed per epoch: -0.0048\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5094 - val_loss: -0.4909\n",
      "\n",
      "Epoch 00018: val_loss improved from -0.46363 to -0.49088, saving model to ../processed/model3.h5\n",
      "Learning rate: 3.2e-04\n",
      "Speed per epoch: -0.0015\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5332 - val_loss: -0.5089\n",
      "\n",
      "Epoch 00019: val_loss improved from -0.49088 to -0.50890, saving model to ../processed/model3.h5\n",
      "Learning rate: 3.2e-04\n",
      "Speed per epoch: -0.0072\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5209 - val_loss: -0.4956\n",
      "\n",
      "Epoch 00020: val_loss did not improve from -0.50890\n",
      "Learning rate: 3.1e-04\n",
      "Speed per epoch: -0.0035\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5230 - val_loss: -0.4966\n",
      "\n",
      "Epoch 00021: val_loss did not improve from -0.50890\n",
      "Learning rate: 2.6e-04\n",
      "Speed per epoch: 0.0010\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5391 - val_loss: -0.4957\n",
      "\n",
      "Epoch 00022: val_loss did not improve from -0.50890\n",
      "Learning rate: 2.3e-04\n",
      "Speed per epoch: -0.0041\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5460 - val_loss: -0.5154\n",
      "\n",
      "Epoch 00023: val_loss improved from -0.50890 to -0.51541, saving model to ../processed/model3.h5\n",
      "Learning rate: 2.3e-04\n",
      "Speed per epoch: -0.0087\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5476 - val_loss: -0.4985\n",
      "\n",
      "Epoch 00024: val_loss did not improve from -0.51541\n",
      "Learning rate: 2.3e-04\n",
      "Speed per epoch: -0.0029\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5486 - val_loss: -0.4999\n",
      "\n",
      "Epoch 00025: val_loss did not improve from -0.51541\n",
      "Learning rate: 1.9e-04\n",
      "Speed per epoch: -0.0016\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5597 - val_loss: -0.5022\n",
      "\n",
      "Epoch 00026: val_loss did not improve from -0.51541\n",
      "Learning rate: 1.9e-04\n",
      "Speed per epoch: -0.0074\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5597 - val_loss: -0.5029\n",
      "\n",
      "Epoch 00027: val_loss did not improve from -0.51541\n",
      "Learning rate: 1.9e-04\n",
      "Speed per epoch: -0.0055\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5548 - val_loss: -0.5278\n",
      "\n",
      "Epoch 00028: val_loss improved from -0.51541 to -0.52784, saving model to ../processed/model3.h5\n",
      "Learning rate: 1.6e-04\n",
      "Speed per epoch: 0.0015\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5660 - val_loss: -0.4944\n",
      "\n",
      "Epoch 00029: val_loss did not improve from -0.52784\n",
      "Learning rate: 1.5e-04\n",
      "Speed per epoch: -0.0006\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5699 - val_loss: -0.5241\n",
      "\n",
      "Epoch 00030: val_loss did not improve from -0.52784\n",
      "Learning rate: 1.1e-04\n",
      "Speed per epoch: -0.0082\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5777 - val_loss: -0.5239\n",
      "\n",
      "Epoch 00031: val_loss did not improve from -0.52784\n",
      "Learning rate: 1.1e-04\n",
      "Speed per epoch: -0.0060\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5789 - val_loss: -0.5199\n",
      "\n",
      "Epoch 00032: val_loss did not improve from -0.52784\n",
      "Learning rate: 1.1e-04\n",
      "Speed per epoch: -0.0049\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5694 - val_loss: -0.5320\n",
      "\n",
      "Epoch 00033: val_loss improved from -0.52784 to -0.53205, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.8e-05\n",
      "Speed per epoch: 0.0009\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5804 - val_loss: -0.5285\n",
      "\n",
      "Epoch 00034: val_loss did not improve from -0.53205\n",
      "Learning rate: 5.1e-05\n",
      "Speed per epoch: -0.0047\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5810 - val_loss: -0.5260\n",
      "\n",
      "Epoch 00035: val_loss did not improve from -0.53205\n",
      "Learning rate: 5.1e-05\n",
      "Speed per epoch: -0.0025\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5865 - val_loss: -0.5237\n",
      "\n",
      "Epoch 00036: val_loss did not improve from -0.53205\n",
      "Learning rate: 5.1e-05\n",
      "Speed per epoch: -0.0016\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5911 - val_loss: -0.5270\n",
      "\n",
      "Epoch 00037: val_loss did not improve from -0.53205\n",
      "Learning rate: 4.9e-05\n",
      "Speed per epoch: -0.0031\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5923 - val_loss: -0.5278\n",
      "\n",
      "Epoch 00038: val_loss did not improve from -0.53205\n",
      "Learning rate: 4.9e-05\n",
      "Speed per epoch: -0.0028\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5858 - val_loss: -0.5198\n",
      "\n",
      "Epoch 00039: val_loss did not improve from -0.53205\n",
      "Learning rate: 3.2e-05\n",
      "Speed per epoch: 0.0005\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5854 - val_loss: -0.5258\n",
      "\n",
      "Epoch 00040: val_loss did not improve from -0.53205\n",
      "Learning rate: 1.8e-05\n",
      "Speed per epoch: 0.0019\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5982 - val_loss: -0.5266\n",
      "\n",
      "Epoch 00041: val_loss did not improve from -0.53205\n",
      "Learning rate: 1.2e-05\n",
      "Speed per epoch: -0.0063\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5925 - val_loss: -0.5291\n",
      "\n",
      "Epoch 00042: val_loss did not improve from -0.53205\n",
      "Learning rate: 1.1e-05\n",
      "Speed per epoch: 0.0017\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.6043 - val_loss: -0.5288\n",
      "\n",
      "Epoch 00043: val_loss did not improve from -0.53205\n",
      "Learning rate: 8.0e-06\n",
      "Speed per epoch: -0.0058\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.6060 - val_loss: -0.5294\n",
      "\n",
      "Epoch 00044: val_loss did not improve from -0.53205\n",
      "Learning rate: 7.9e-06\n",
      "Speed per epoch: -0.0059\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5988 - val_loss: -0.5295\n",
      "\n",
      "Epoch 00045: val_loss did not improve from -0.53205\n",
      "Learning rate: 7.4e-06\n",
      "Speed per epoch: 0.0019\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5974 - val_loss: -0.5288\n",
      "\n",
      "Epoch 00046: val_loss did not improve from -0.53205\n",
      "Learning rate: 3.1e-06\n",
      "Speed per epoch: -0.0001\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5976 - val_loss: -0.5295\n",
      "\n",
      "Epoch 00047: val_loss did not improve from -0.53205\n",
      "Learning rate: 2.0e-06\n",
      "Speed per epoch: 0.0024\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.6056 - val_loss: -0.5284\n",
      "\n",
      "Epoch 00048: val_loss did not improve from -0.53205\n",
      "Learning rate: 1.3e-06\n",
      "Speed per epoch: -0.0035\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5999 - val_loss: -0.5281\n",
      "\n",
      "Epoch 00049: val_loss did not improve from -0.53205\n",
      "Learning rate: 1.0e-06\n",
      "Speed per epoch: -0.0000\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5938 - val_loss: -0.5283\n",
      "\n",
      "Epoch 00050: val_loss did not improve from -0.53205\n",
      "Learning rate: 4.4e-07\n",
      "Speed per epoch: 0.0024\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.6054 - val_loss: -0.5281\n",
      "\n",
      "Epoch 00051: val_loss did not improve from -0.53205\n",
      "Learning rate: 3.3e-07\n",
      "Speed per epoch: -0.0024\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5934 - val_loss: -0.5280\n",
      "\n",
      "Epoch 00052: val_loss did not improve from -0.53205\n",
      "Learning rate: 1.8e-07\n",
      "Speed per epoch: 0.0031\n",
      "Wait to stop: 2\n",
      "\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.6013 - val_loss: -0.5280\n",
      "\n",
      "Epoch 00053: val_loss did not improve from -0.53205\n",
      "Learning rate: 8.0e-08\n",
      "Speed per epoch: -0.0024\n",
      "Wait to stop: 1\n",
      "\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5977 - val_loss: -0.5281\n",
      "\n",
      "Epoch 00054: val_loss did not improve from -0.53205\n",
      "Learning rate: 5.3e-08\n",
      "Speed per epoch: -0.0017\n",
      "Wait to stop: 0\n",
      "\n",
      "\n",
      "Fold - 2 / 10\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 27)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 23)     0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 1)      24          lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None, 4)      0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 5)      0           conv1d_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 64)     1024        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, None, 64)     12352       conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, None, 64)     12352       conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, None, 64)     12352       conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, None, 64)     12352       conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, None, 64)     12352       conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, None, 4)      260         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 4)            0           conv1d_8[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 63,068\n",
      "Trainable params: 63,068\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.7783 - intersection_over_union: -0.0037 - val_loss: 0.6913 - val_intersection_over_union: -0.0281\n",
      "Epoch 1/100\n",
      "  33/1000 [..............................] - ETA: 22s - loss: -0.0199"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WLMike/Documents/PycharmProjects/ML_venv/lib/python3.6/site-packages/ipykernel_launcher.py:78: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.3099 - val_loss: -0.3424\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to -0.34244, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.1932\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.3691 - val_loss: -0.3837\n",
      "\n",
      "Epoch 00002: val_loss improved from -0.34244 to -0.38372, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0554\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4220 - val_loss: -0.4207\n",
      "\n",
      "Epoch 00003: val_loss improved from -0.38372 to -0.42070, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0529\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4364 - val_loss: -0.4472\n",
      "\n",
      "Epoch 00004: val_loss improved from -0.42070 to -0.44724, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0342\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4298 - val_loss: -0.4343\n",
      "\n",
      "Epoch 00005: val_loss did not improve from -0.44724\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0139\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4461 - val_loss: -0.4306\n",
      "\n",
      "Epoch 00006: val_loss did not improve from -0.44724\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0133\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4421 - val_loss: -0.4519\n",
      "\n",
      "Epoch 00007: val_loss improved from -0.44724 to -0.45193, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0034\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4522 - val_loss: -0.4619\n",
      "\n",
      "Epoch 00008: val_loss improved from -0.45193 to -0.46189, saving model to ../processed/model3.h5\n",
      "Learning rate: 6.2e-04\n",
      "Speed per epoch: -0.0123\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4700 - val_loss: -0.4447\n",
      "\n",
      "Epoch 00009: val_loss did not improve from -0.46189\n",
      "Learning rate: 6.2e-04\n",
      "Speed per epoch: -0.0126\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4669 - val_loss: -0.4524\n",
      "\n",
      "Epoch 00010: val_loss did not improve from -0.46189\n",
      "Learning rate: 6.2e-04\n",
      "Speed per epoch: -0.0071\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4840 - val_loss: -0.4658\n",
      "\n",
      "Epoch 00011: val_loss improved from -0.46189 to -0.46583, saving model to ../processed/model3.h5\n",
      "Learning rate: 6.2e-04\n",
      "Speed per epoch: -0.0100\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4881 - val_loss: -0.4698\n",
      "\n",
      "Epoch 00012: val_loss improved from -0.46583 to -0.46984, saving model to ../processed/model3.h5\n",
      "Learning rate: 6.2e-04\n",
      "Speed per epoch: -0.0056\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4794 - val_loss: -0.4545\n",
      "\n",
      "Epoch 00013: val_loss did not improve from -0.46984\n",
      "Learning rate: 5.5e-04\n",
      "Speed per epoch: -0.0016\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4868 - val_loss: -0.4842\n",
      "\n",
      "Epoch 00014: val_loss improved from -0.46984 to -0.48418, saving model to ../processed/model3.h5\n",
      "Learning rate: 4.7e-04\n",
      "Speed per epoch: -0.0017\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4987 - val_loss: -0.5109\n",
      "\n",
      "Epoch 00015: val_loss improved from -0.48418 to -0.51094, saving model to ../processed/model3.h5\n",
      "Learning rate: 4.6e-04\n",
      "Speed per epoch: -0.0071\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5129 - val_loss: -0.5178\n",
      "\n",
      "Epoch 00016: val_loss improved from -0.51094 to -0.51783, saving model to ../processed/model3.h5\n",
      "Learning rate: 4.6e-04\n",
      "Speed per epoch: -0.0108\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5149 - val_loss: -0.4974\n",
      "\n",
      "Epoch 00017: val_loss did not improve from -0.51783\n",
      "Learning rate: 4.6e-04\n",
      "Speed per epoch: -0.0087\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5194 - val_loss: -0.4603\n",
      "\n",
      "Epoch 00018: val_loss did not improve from -0.51783\n",
      "Learning rate: 4.6e-04\n",
      "Speed per epoch: -0.0074\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5138 - val_loss: -0.5032\n",
      "\n",
      "Epoch 00019: val_loss did not improve from -0.51783\n",
      "Learning rate: 4.6e-04\n",
      "Speed per epoch: 0.0002\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5208 - val_loss: -0.4947\n",
      "\n",
      "Epoch 00020: val_loss did not improve from -0.51783\n",
      "Learning rate: 3.1e-04\n",
      "Speed per epoch: -0.0024\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5286 - val_loss: -0.5138\n",
      "\n",
      "Epoch 00021: val_loss did not improve from -0.51783\n",
      "Learning rate: 3.1e-04\n",
      "Speed per epoch: -0.0040\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5440 - val_loss: -0.4687\n",
      "\n",
      "Epoch 00022: val_loss did not improve from -0.51783\n",
      "Learning rate: 3.1e-04\n",
      "Speed per epoch: -0.0063\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5388 - val_loss: -0.4870\n",
      "\n",
      "Epoch 00023: val_loss did not improve from -0.51783\n",
      "Learning rate: 3.1e-04\n",
      "Speed per epoch: -0.0033\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5418 - val_loss: -0.5062\n",
      "\n",
      "Epoch 00024: val_loss did not improve from -0.51783\n",
      "Learning rate: 3.0e-04\n",
      "Speed per epoch: -0.0040\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5407 - val_loss: -0.4867\n",
      "\n",
      "Epoch 00025: val_loss did not improve from -0.51783\n",
      "Learning rate: 2.9e-04\n",
      "Speed per epoch: -0.0027\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5521 - val_loss: -0.5176\n",
      "\n",
      "Epoch 00026: val_loss did not improve from -0.51783\n",
      "Learning rate: 2.8e-04\n",
      "Speed per epoch: -0.0053\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5414 - val_loss: -0.5225\n",
      "\n",
      "Epoch 00027: val_loss improved from -0.51783 to -0.52250, saving model to ../processed/model3.h5\n",
      "Learning rate: 2.2e-04\n",
      "Speed per epoch: 0.0006\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5535 - val_loss: -0.5419\n",
      "\n",
      "Epoch 00028: val_loss improved from -0.52250 to -0.54190, saving model to ../processed/model3.h5\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.0060\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5568 - val_loss: -0.5497\n",
      "\n",
      "Epoch 00029: val_loss improved from -0.54190 to -0.54971, saving model to ../processed/model3.h5\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.0050\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5577 - val_loss: -0.5338\n",
      "\n",
      "Epoch 00030: val_loss did not improve from -0.54971\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.0013\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5563 - val_loss: -0.5281\n",
      "\n",
      "Epoch 00031: val_loss did not improve from -0.54971\n",
      "Learning rate: 1.2e-04\n",
      "Speed per epoch: 0.0027\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5697 - val_loss: -0.5522\n",
      "\n",
      "Epoch 00032: val_loss improved from -0.54971 to -0.55220, saving model to ../processed/model3.h5\n",
      "Learning rate: 8.2e-05\n",
      "Speed per epoch: -0.0077\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5792 - val_loss: -0.5477\n",
      "\n",
      "Epoch 00033: val_loss did not improve from -0.55220\n",
      "Learning rate: 8.2e-05\n",
      "Speed per epoch: -0.0083\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5676 - val_loss: -0.5428\n",
      "\n",
      "Epoch 00034: val_loss did not improve from -0.55220\n",
      "Learning rate: 6.6e-05\n",
      "Speed per epoch: 0.0009\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5831 - val_loss: -0.5514\n",
      "\n",
      "Epoch 00035: val_loss did not improve from -0.55220\n",
      "Learning rate: 6.0e-05\n",
      "Speed per epoch: -0.0065\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5768 - val_loss: -0.5493\n",
      "\n",
      "Epoch 00036: val_loss did not improve from -0.55220\n",
      "Learning rate: 5.9e-05\n",
      "Speed per epoch: -0.0006\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5929 - val_loss: -0.5519\n",
      "\n",
      "Epoch 00037: val_loss did not improve from -0.55220\n",
      "Learning rate: 5.9e-05\n",
      "Speed per epoch: -0.0060\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5833 - val_loss: -0.5505\n",
      "\n",
      "Epoch 00038: val_loss did not improve from -0.55220\n",
      "Learning rate: 4.2e-05\n",
      "Speed per epoch: -0.0045\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5873 - val_loss: -0.5531\n",
      "\n",
      "Epoch 00039: val_loss improved from -0.55220 to -0.55305, saving model to ../processed/model3.h5\n",
      "Learning rate: 4.0e-05\n",
      "Speed per epoch: -0.0040\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5940 - val_loss: -0.5574\n",
      "\n",
      "Epoch 00040: val_loss improved from -0.55305 to -0.55736, saving model to ../processed/model3.h5\n",
      "Learning rate: 4.0e-05\n",
      "Speed per epoch: -0.0010\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5854 - val_loss: -0.5601\n",
      "\n",
      "Epoch 00041: val_loss improved from -0.55736 to -0.56010, saving model to ../processed/model3.h5\n",
      "Learning rate: 2.2e-05\n",
      "Speed per epoch: 0.0027\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5934 - val_loss: -0.5609\n",
      "\n",
      "Epoch 00042: val_loss improved from -0.56010 to -0.56092, saving model to ../processed/model3.h5\n",
      "Learning rate: 1.1e-05\n",
      "Speed per epoch: -0.0057\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5998 - val_loss: -0.5627\n",
      "\n",
      "Epoch 00043: val_loss improved from -0.56092 to -0.56273, saving model to ../processed/model3.h5\n",
      "Learning rate: 1.1e-05\n",
      "Speed per epoch: -0.0058\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5899 - val_loss: -0.5645\n",
      "\n",
      "Epoch 00044: val_loss improved from -0.56273 to -0.56453, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.3e-06\n",
      "Speed per epoch: -0.0006\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5987 - val_loss: -0.5645\n",
      "\n",
      "Epoch 00045: val_loss improved from -0.56453 to -0.56454, saving model to ../processed/model3.h5\n",
      "Learning rate: 6.7e-06\n",
      "Speed per epoch: -0.0020\n",
      "Wait to stop: 2\n",
      "\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5979 - val_loss: -0.5656\n",
      "\n",
      "Epoch 00046: val_loss improved from -0.56454 to -0.56558, saving model to ../processed/model3.h5\n",
      "Learning rate: 5.6e-06\n",
      "Speed per epoch: -0.0010\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5960 - val_loss: -0.5673\n",
      "\n",
      "Epoch 00047: val_loss improved from -0.56558 to -0.56732, saving model to ../processed/model3.h5\n",
      "Learning rate: 3.7e-06\n",
      "Speed per epoch: -0.0005\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5903 - val_loss: -0.5679\n",
      "\n",
      "Epoch 00048: val_loss improved from -0.56732 to -0.56792, saving model to ../processed/model3.h5\n",
      "Learning rate: 1.5e-06\n",
      "Speed per epoch: 0.0011\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.6115 - val_loss: -0.5675\n",
      "\n",
      "Epoch 00049: val_loss did not improve from -0.56792\n",
      "Learning rate: 1.4e-06\n",
      "Speed per epoch: -0.0050\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5926 - val_loss: -0.5669\n",
      "\n",
      "Epoch 00050: val_loss did not improve from -0.56792\n",
      "Learning rate: 1.1e-06\n",
      "Speed per epoch: 0.0037\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.6082 - val_loss: -0.5679\n",
      "\n",
      "Epoch 00051: val_loss did not improve from -0.56792\n",
      "Learning rate: 7.5e-07\n",
      "Speed per epoch: -0.0040\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5899 - val_loss: -0.5674\n",
      "\n",
      "Epoch 00052: val_loss did not improve from -0.56792\n",
      "Learning rate: 4.5e-07\n",
      "Speed per epoch: 0.0055\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5965 - val_loss: -0.5672\n",
      "\n",
      "Epoch 00053: val_loss did not improve from -0.56792\n",
      "Learning rate: 1.8e-07\n",
      "Speed per epoch: -0.0004\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.6037 - val_loss: -0.5673\n",
      "\n",
      "Epoch 00054: val_loss did not improve from -0.56792\n",
      "Learning rate: 1.3e-07\n",
      "Speed per epoch: -0.0041\n",
      "Wait to stop: 2\n",
      "\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5986 - val_loss: -0.5674\n",
      "\n",
      "Epoch 00055: val_loss did not improve from -0.56792\n",
      "Learning rate: 9.0e-08\n",
      "Speed per epoch: -0.0029\n",
      "Wait to stop: 1\n",
      "\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5974 - val_loss: -0.5674\n",
      "\n",
      "Epoch 00056: val_loss did not improve from -0.56792\n",
      "Learning rate: 5.6e-08\n",
      "Speed per epoch: -0.0006\n",
      "Wait to stop: 0\n",
      "\n",
      "\n",
      "Fold - 3 / 10\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 27)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 23)     0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 1)      24          lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None, 4)      0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 5)      0           conv1d_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 64)     1024        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, None, 64)     12352       conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, None, 64)     12352       conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, None, 64)     12352       conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, None, 64)     12352       conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, None, 64)     12352       conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, None, 4)      260         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 4)            0           conv1d_8[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 63,068\n",
      "Trainable params: 63,068\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.7743 - intersection_over_union: -0.0013 - val_loss: 0.7290 - val_intersection_over_union: -0.0116\n",
      "Epoch 1/100\n",
      "  34/1000 [>.............................] - ETA: 19s - loss: -0.0078"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WLMike/Documents/PycharmProjects/ML_venv/lib/python3.6/site-packages/ipykernel_launcher.py:78: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.3034 - val_loss: -0.4311\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to -0.43108, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.1810\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.3619 - val_loss: -0.4137\n",
      "\n",
      "Epoch 00002: val_loss did not improve from -0.43108\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0455\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.3823 - val_loss: -0.4529\n",
      "\n",
      "Epoch 00003: val_loss improved from -0.43108 to -0.45286, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0267\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4083 - val_loss: -0.4188\n",
      "\n",
      "Epoch 00004: val_loss did not improve from -0.45286\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0279\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4305 - val_loss: -0.4149\n",
      "\n",
      "Epoch 00005: val_loss did not improve from -0.45286\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0221\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4348 - val_loss: -0.4718\n",
      "\n",
      "Epoch 00006: val_loss improved from -0.45286 to -0.47184, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0122\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4409 - val_loss: -0.4671\n",
      "\n",
      "Epoch 00007: val_loss did not improve from -0.47184\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0104\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4342 - val_loss: -0.4607\n",
      "\n",
      "Epoch 00008: val_loss did not improve from -0.47184\n",
      "Learning rate: 7.8e-04\n",
      "Speed per epoch: -0.0004\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4617 - val_loss: -0.4727\n",
      "\n",
      "Epoch 00009: val_loss improved from -0.47184 to -0.47273, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.5e-04\n",
      "Speed per epoch: -0.0119\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4751 - val_loss: -0.4610\n",
      "\n",
      "Epoch 00010: val_loss did not improve from -0.47273\n",
      "Learning rate: 7.5e-04\n",
      "Speed per epoch: -0.0151\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4683 - val_loss: -0.5112\n",
      "\n",
      "Epoch 00011: val_loss improved from -0.47273 to -0.51115, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.5e-04\n",
      "Speed per epoch: -0.0074\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4623 - val_loss: -0.5178\n",
      "\n",
      "Epoch 00012: val_loss improved from -0.51115 to -0.51783, saving model to ../processed/model3.h5\n",
      "Learning rate: 5.3e-04\n",
      "Speed per epoch: -0.0020\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4984 - val_loss: -0.5198\n",
      "\n",
      "Epoch 00013: val_loss improved from -0.51783 to -0.51977, saving model to ../processed/model3.h5\n",
      "Learning rate: 5.3e-04\n",
      "Speed per epoch: -0.0114\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5031 - val_loss: -0.4977\n",
      "\n",
      "Epoch 00014: val_loss did not improve from -0.51977\n",
      "Learning rate: 5.3e-04\n",
      "Speed per epoch: -0.0108\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5017 - val_loss: -0.4786\n",
      "\n",
      "Epoch 00015: val_loss did not improve from -0.51977\n",
      "Learning rate: 5.3e-04\n",
      "Speed per epoch: -0.0041\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5042 - val_loss: -0.5314\n",
      "\n",
      "Epoch 00016: val_loss improved from -0.51977 to -0.53137, saving model to ../processed/model3.h5\n",
      "Learning rate: 5.0e-04\n",
      "Speed per epoch: -0.0054\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4983 - val_loss: -0.4944\n",
      "\n",
      "Epoch 00017: val_loss did not improve from -0.53137\n",
      "Learning rate: 4.1e-04\n",
      "Speed per epoch: -0.0020\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5112 - val_loss: -0.5327\n",
      "\n",
      "Epoch 00018: val_loss improved from -0.53137 to -0.53269, saving model to ../processed/model3.h5\n",
      "Learning rate: 4.1e-04\n",
      "Speed per epoch: -0.0043\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5193 - val_loss: -0.5151\n",
      "\n",
      "Epoch 00019: val_loss did not improve from -0.53269\n",
      "Learning rate: 4.0e-04\n",
      "Speed per epoch: -0.0019\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5218 - val_loss: -0.5161\n",
      "\n",
      "Epoch 00020: val_loss did not improve from -0.53269\n",
      "Learning rate: 4.0e-04\n",
      "Speed per epoch: -0.0023\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5178 - val_loss: -0.4765\n",
      "\n",
      "Epoch 00021: val_loss did not improve from -0.53269\n",
      "Learning rate: 3.4e-04\n",
      "Speed per epoch: 0.0018\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5332 - val_loss: -0.5294\n",
      "\n",
      "Epoch 00022: val_loss did not improve from -0.53269\n",
      "Learning rate: 2.4e-04\n",
      "Speed per epoch: -0.0074\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5471 - val_loss: -0.5514\n",
      "\n",
      "Epoch 00023: val_loss improved from -0.53269 to -0.55140, saving model to ../processed/model3.h5\n",
      "Learning rate: 2.4e-04\n",
      "Speed per epoch: -0.0118\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5452 - val_loss: -0.5400\n",
      "\n",
      "Epoch 00024: val_loss did not improve from -0.55140\n",
      "Learning rate: 2.4e-04\n",
      "Speed per epoch: -0.0067\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5395 - val_loss: -0.5401\n",
      "\n",
      "Epoch 00025: val_loss did not improve from -0.55140\n",
      "Learning rate: 1.9e-04\n",
      "Speed per epoch: -0.0024\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5512 - val_loss: -0.5346\n",
      "\n",
      "Epoch 00026: val_loss did not improve from -0.55140\n",
      "Learning rate: 1.8e-04\n",
      "Speed per epoch: -0.0063\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5486 - val_loss: -0.5421\n",
      "\n",
      "Epoch 00027: val_loss did not improve from -0.55140\n",
      "Learning rate: 1.8e-04\n",
      "Speed per epoch: -0.0007\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5568 - val_loss: -0.5394\n",
      "\n",
      "Epoch 00028: val_loss did not improve from -0.55140\n",
      "Learning rate: 1.8e-04\n",
      "Speed per epoch: -0.0015\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5573 - val_loss: -0.5437\n",
      "\n",
      "Epoch 00029: val_loss did not improve from -0.55140\n",
      "Learning rate: 1.8e-04\n",
      "Speed per epoch: -0.0013\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5577 - val_loss: -0.5427\n",
      "\n",
      "Epoch 00030: val_loss did not improve from -0.55140\n",
      "Learning rate: 1.2e-04\n",
      "Speed per epoch: -0.0050\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5652 - val_loss: -0.5407\n",
      "\n",
      "Epoch 00031: val_loss did not improve from -0.55140\n",
      "Learning rate: 1.2e-04\n",
      "Speed per epoch: -0.0029\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5644 - val_loss: -0.5451\n",
      "\n",
      "Epoch 00032: val_loss did not improve from -0.55140\n",
      "Learning rate: 1.1e-04\n",
      "Speed per epoch: -0.0040\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5678 - val_loss: -0.5431\n",
      "\n",
      "Epoch 00033: val_loss did not improve from -0.55140\n",
      "Learning rate: 1.1e-04\n",
      "Speed per epoch: -0.0017\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5693 - val_loss: -0.5536\n",
      "\n",
      "Epoch 00034: val_loss improved from -0.55140 to -0.55364, saving model to ../processed/model3.h5\n",
      "Learning rate: 8.3e-05\n",
      "Speed per epoch: -0.0038\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5774 - val_loss: -0.5516\n",
      "\n",
      "Epoch 00035: val_loss did not improve from -0.55364\n",
      "Learning rate: 8.3e-05\n",
      "Speed per epoch: -0.0044\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5736 - val_loss: -0.5524\n",
      "\n",
      "Epoch 00036: val_loss did not improve from -0.55364\n",
      "Learning rate: 7.3e-05\n",
      "Speed per epoch: -0.0020\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5776 - val_loss: -0.5500\n",
      "\n",
      "Epoch 00037: val_loss did not improve from -0.55364\n",
      "Learning rate: 7.2e-05\n",
      "Speed per epoch: -0.0020\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5762 - val_loss: -0.5573\n",
      "\n",
      "Epoch 00038: val_loss improved from -0.55364 to -0.55733, saving model to ../processed/model3.h5\n",
      "Learning rate: 5.5e-05\n",
      "Speed per epoch: -0.0003\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5851 - val_loss: -0.5444\n",
      "\n",
      "Epoch 00039: val_loss did not improve from -0.55733\n",
      "Learning rate: 5.4e-05\n",
      "Speed per epoch: -0.0019\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5860 - val_loss: -0.5503\n",
      "\n",
      "Epoch 00040: val_loss did not improve from -0.55733\n",
      "Learning rate: 4.0e-05\n",
      "Speed per epoch: -0.0060\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5888 - val_loss: -0.5468\n",
      "\n",
      "Epoch 00041: val_loss did not improve from -0.55733\n",
      "Learning rate: 4.0e-05\n",
      "Speed per epoch: -0.0006\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5985 - val_loss: -0.5468\n",
      "\n",
      "Epoch 00042: val_loss did not improve from -0.55733\n",
      "Learning rate: 3.9e-05\n",
      "Speed per epoch: -0.0048\n",
      "Wait to stop: 2\n",
      "\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5926 - val_loss: -0.5501\n",
      "\n",
      "Epoch 00043: val_loss did not improve from -0.55733\n",
      "Learning rate: 3.6e-05\n",
      "Speed per epoch: -0.0008\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5844 - val_loss: -0.5551\n",
      "\n",
      "Epoch 00044: val_loss did not improve from -0.55733\n",
      "Learning rate: 1.7e-05\n",
      "Speed per epoch: 0.0029\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5895 - val_loss: -0.5519\n",
      "\n",
      "Epoch 00045: val_loss did not improve from -0.55733\n",
      "Learning rate: 8.7e-06\n",
      "Speed per epoch: 0.0011\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5987 - val_loss: -0.5525\n",
      "\n",
      "Epoch 00046: val_loss did not improve from -0.55733\n",
      "Learning rate: 7.2e-06\n",
      "Speed per epoch: -0.0024\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5947 - val_loss: -0.5496\n",
      "\n",
      "Epoch 00047: val_loss did not improve from -0.55733\n",
      "Learning rate: 5.0e-06\n",
      "Speed per epoch: -0.0020\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5935 - val_loss: -0.5516\n",
      "\n",
      "Epoch 00048: val_loss did not improve from -0.55733\n",
      "Learning rate: 4.1e-06\n",
      "Speed per epoch: -0.0007\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.6059 - val_loss: -0.5516\n",
      "\n",
      "Epoch 00049: val_loss did not improve from -0.55733\n",
      "Learning rate: 4.0e-06\n",
      "Speed per epoch: -0.0045\n",
      "Wait to stop: 2\n",
      "\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5952 - val_loss: -0.5527\n",
      "\n",
      "Epoch 00050: val_loss did not improve from -0.55733\n",
      "Learning rate: 2.8e-06\n",
      "Speed per epoch: 0.0001\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5776 - val_loss: -0.5525\n",
      "\n",
      "Epoch 00051: val_loss did not improve from -0.55733\n",
      "Learning rate: 1.0e-06\n",
      "Speed per epoch: 0.0086\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5949 - val_loss: -0.5527\n",
      "\n",
      "Epoch 00052: val_loss did not improve from -0.55733\n",
      "Learning rate: 4.6e-07\n",
      "Speed per epoch: -0.0015\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5982 - val_loss: -0.5530\n",
      "\n",
      "Epoch 00053: val_loss did not improve from -0.55733\n",
      "Learning rate: 4.5e-07\n",
      "Speed per epoch: 0.0005\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.6048 - val_loss: -0.5528\n",
      "\n",
      "Epoch 00054: val_loss did not improve from -0.55733\n",
      "Learning rate: 3.2e-07\n",
      "Speed per epoch: -0.0059\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5956 - val_loss: -0.5527\n",
      "\n",
      "Epoch 00055: val_loss did not improve from -0.55733\n",
      "Learning rate: 2.9e-07\n",
      "Speed per epoch: 0.0013\n",
      "Wait to stop: 2\n",
      "\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.6011 - val_loss: -0.5527\n",
      "\n",
      "Epoch 00056: val_loss did not improve from -0.55733\n",
      "Learning rate: 1.5e-07\n",
      "Speed per epoch: -0.0027\n",
      "Wait to stop: 1\n",
      "\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5996 - val_loss: -0.5527\n",
      "\n",
      "Epoch 00057: val_loss did not improve from -0.55733\n",
      "Learning rate: 1.1e-07\n",
      "Speed per epoch: -0.0015\n",
      "Wait to stop: 0\n",
      "\n",
      "\n",
      "Fold - 4 / 10\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 27)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 23)     0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 1)      24          lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None, 4)      0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 5)      0           conv1d_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 64)     1024        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, None, 64)     12352       conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, None, 64)     12352       conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, None, 64)     12352       conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, None, 64)     12352       conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, None, 64)     12352       conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, None, 4)      260         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 4)            0           conv1d_8[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 63,068\n",
      "Trainable params: 63,068\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.7288 - intersection_over_union: -0.0171 - val_loss: 0.5920 - val_intersection_over_union: -0.1007\n",
      "Epoch 1/100\n",
      "  33/1000 [..............................] - ETA: 20s - loss: -0.0960"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WLMike/Documents/PycharmProjects/ML_venv/lib/python3.6/site-packages/ipykernel_launcher.py:78: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.3197 - val_loss: -0.4177\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to -0.41770, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.1452\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.3772 - val_loss: -0.3508\n",
      "\n",
      "Epoch 00002: val_loss did not improve from -0.41770\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0558\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4048 - val_loss: -0.4127\n",
      "\n",
      "Epoch 00003: val_loss did not improve from -0.41770\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0382\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4097 - val_loss: -0.4712\n",
      "\n",
      "Epoch 00004: val_loss improved from -0.41770 to -0.47122, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0172\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4172 - val_loss: -0.4795\n",
      "\n",
      "Epoch 00005: val_loss improved from -0.47122 to -0.47953, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0094\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4411 - val_loss: -0.4714\n",
      "\n",
      "Epoch 00006: val_loss did not improve from -0.47953\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0173\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4449 - val_loss: -0.4887\n",
      "\n",
      "Epoch 00007: val_loss improved from -0.47953 to -0.48874, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0135\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4598 - val_loss: -0.4590\n",
      "\n",
      "Epoch 00008: val_loss did not improve from -0.48874\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0126\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4604 - val_loss: -0.4855\n",
      "\n",
      "Epoch 00009: val_loss did not improve from -0.48874\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0066\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4615 - val_loss: -0.5169\n",
      "\n",
      "Epoch 00010: val_loss improved from -0.48874 to -0.51686, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0025\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4701 - val_loss: -0.5196\n",
      "\n",
      "Epoch 00011: val_loss improved from -0.51686 to -0.51958, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.1e-04\n",
      "Speed per epoch: -0.0090\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4662 - val_loss: -0.5315\n",
      "\n",
      "Epoch 00012: val_loss improved from -0.51958 to -0.53153, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.1e-04\n",
      "Speed per epoch: -0.0044\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4732 - val_loss: -0.4916\n",
      "\n",
      "Epoch 00013: val_loss did not improve from -0.53153\n",
      "Learning rate: 7.1e-04\n",
      "Speed per epoch: -0.0022\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4807 - val_loss: -0.5354\n",
      "\n",
      "Epoch 00014: val_loss improved from -0.53153 to -0.53543, saving model to ../processed/model3.h5\n",
      "Learning rate: 6.9e-04\n",
      "Speed per epoch: -0.0071\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4812 - val_loss: -0.4738\n",
      "\n",
      "Epoch 00015: val_loss did not improve from -0.53543\n",
      "Learning rate: 6.9e-04\n",
      "Speed per epoch: -0.0010\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4875 - val_loss: -0.5197\n",
      "\n",
      "Epoch 00016: val_loss did not improve from -0.53543\n",
      "Learning rate: 4.9e-04\n",
      "Speed per epoch: -0.0060\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4978 - val_loss: -0.4992\n",
      "\n",
      "Epoch 00017: val_loss did not improve from -0.53543\n",
      "Learning rate: 4.9e-04\n",
      "Speed per epoch: -0.0097\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5035 - val_loss: -0.5533\n",
      "\n",
      "Epoch 00018: val_loss improved from -0.53543 to -0.55328, saving model to ../processed/model3.h5\n",
      "Learning rate: 4.9e-04\n",
      "Speed per epoch: -0.0087\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5073 - val_loss: -0.5105\n",
      "\n",
      "Epoch 00019: val_loss did not improve from -0.55328\n",
      "Learning rate: 4.9e-04\n",
      "Speed per epoch: -0.0050\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5107 - val_loss: -0.5504\n",
      "\n",
      "Epoch 00020: val_loss did not improve from -0.55328\n",
      "Learning rate: 4.4e-04\n",
      "Speed per epoch: -0.0019\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5148 - val_loss: -0.5300\n",
      "\n",
      "Epoch 00021: val_loss did not improve from -0.55328\n",
      "Learning rate: 4.4e-04\n",
      "Speed per epoch: -0.0047\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5242 - val_loss: -0.5501\n",
      "\n",
      "Epoch 00022: val_loss did not improve from -0.55328\n",
      "Learning rate: 4.4e-04\n",
      "Speed per epoch: -0.0034\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5164 - val_loss: -0.5331\n",
      "\n",
      "Epoch 00023: val_loss did not improve from -0.55328\n",
      "Learning rate: 3.9e-04\n",
      "Speed per epoch: -0.0003\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5270 - val_loss: -0.5222\n",
      "\n",
      "Epoch 00024: val_loss did not improve from -0.55328\n",
      "Learning rate: 3.8e-04\n",
      "Speed per epoch: -0.0037\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5345 - val_loss: -0.5255\n",
      "\n",
      "Epoch 00025: val_loss did not improve from -0.55328\n",
      "Learning rate: 2.9e-04\n",
      "Speed per epoch: -0.0119\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5366 - val_loss: -0.5350\n",
      "\n",
      "Epoch 00026: val_loss did not improve from -0.55328\n",
      "Learning rate: 2.9e-04\n",
      "Speed per epoch: -0.0063\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5417 - val_loss: -0.5289\n",
      "\n",
      "Epoch 00027: val_loss did not improve from -0.55328\n",
      "Learning rate: 2.9e-04\n",
      "Speed per epoch: -0.0045\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5390 - val_loss: -0.5486\n",
      "\n",
      "Epoch 00028: val_loss did not improve from -0.55328\n",
      "Learning rate: 2.5e-04\n",
      "Speed per epoch: 0.0019\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5437 - val_loss: -0.5545\n",
      "\n",
      "Epoch 00029: val_loss improved from -0.55328 to -0.55452, saving model to ../processed/model3.h5\n",
      "Learning rate: 1.3e-04\n",
      "Speed per epoch: -0.0035\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5596 - val_loss: -0.5554\n",
      "\n",
      "Epoch 00030: val_loss improved from -0.55452 to -0.55537, saving model to ../processed/model3.h5\n",
      "Learning rate: 1.3e-04\n",
      "Speed per epoch: -0.0078\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5670 - val_loss: -0.5550\n",
      "\n",
      "Epoch 00031: val_loss did not improve from -0.55537\n",
      "Learning rate: 1.3e-04\n",
      "Speed per epoch: -0.0091\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5693 - val_loss: -0.5468\n",
      "\n",
      "Epoch 00032: val_loss did not improve from -0.55537\n",
      "Learning rate: 1.3e-04\n",
      "Speed per epoch: -0.0086\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5704 - val_loss: -0.5591\n",
      "\n",
      "Epoch 00033: val_loss improved from -0.55537 to -0.55914, saving model to ../processed/model3.h5\n",
      "Learning rate: 1.3e-04\n",
      "Speed per epoch: -0.0042\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5713 - val_loss: -0.5520\n",
      "\n",
      "Epoch 00034: val_loss did not improve from -0.55914\n",
      "Learning rate: 1.3e-04\n",
      "Speed per epoch: -0.0004\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5701 - val_loss: -0.5553\n",
      "\n",
      "Epoch 00035: val_loss did not improve from -0.55914\n",
      "Learning rate: 7.1e-05\n",
      "Speed per epoch: -0.0019\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5791 - val_loss: -0.5592\n",
      "\n",
      "Epoch 00036: val_loss improved from -0.55914 to -0.55918, saving model to ../processed/model3.h5\n",
      "Learning rate: 6.9e-05\n",
      "Speed per epoch: -0.0050\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5807 - val_loss: -0.5549\n",
      "\n",
      "Epoch 00037: val_loss did not improve from -0.55918\n",
      "Learning rate: 6.6e-05\n",
      "Speed per epoch: -0.0045\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5831 - val_loss: -0.5569\n",
      "\n",
      "Epoch 00038: val_loss did not improve from -0.55918\n",
      "Learning rate: 6.4e-05\n",
      "Speed per epoch: -0.0053\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5846 - val_loss: -0.5557\n",
      "\n",
      "Epoch 00039: val_loss did not improve from -0.55918\n",
      "Learning rate: 6.4e-05\n",
      "Speed per epoch: -0.0030\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5734 - val_loss: -0.5611\n",
      "\n",
      "Epoch 00040: val_loss improved from -0.55918 to -0.56107, saving model to ../processed/model3.h5\n",
      "Learning rate: 3.9e-05\n",
      "Speed per epoch: 0.0044\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5842 - val_loss: -0.5606\n",
      "\n",
      "Epoch 00041: val_loss did not improve from -0.56107\n",
      "Learning rate: 1.8e-05\n",
      "Speed per epoch: -0.0020\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5956 - val_loss: -0.5622\n",
      "\n",
      "Epoch 00042: val_loss improved from -0.56107 to -0.56222, saving model to ../processed/model3.h5\n",
      "Learning rate: 1.8e-05\n",
      "Speed per epoch: -0.0038\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5818 - val_loss: -0.5616\n",
      "\n",
      "Epoch 00043: val_loss did not improve from -0.56222\n",
      "Learning rate: 1.4e-05\n",
      "Speed per epoch: 0.0033\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5922 - val_loss: -0.5613\n",
      "\n",
      "Epoch 00044: val_loss did not improve from -0.56222\n",
      "Learning rate: 9.2e-06\n",
      "Speed per epoch: -0.0016\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5914 - val_loss: -0.5613\n",
      "\n",
      "Epoch 00045: val_loss did not improve from -0.56222\n",
      "Learning rate: 6.5e-06\n",
      "Speed per epoch: -0.0007\n",
      "Wait to stop: 2\n",
      "\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5903 - val_loss: -0.5616\n",
      "\n",
      "Epoch 00046: val_loss did not improve from -0.56222\n",
      "Learning rate: 4.5e-06\n",
      "Speed per epoch: -0.0001\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5909 - val_loss: -0.5617\n",
      "\n",
      "Epoch 00047: val_loss did not improve from -0.56222\n",
      "Learning rate: 3.2e-06\n",
      "Speed per epoch: -0.0002\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5937 - val_loss: -0.5616\n",
      "\n",
      "Epoch 00048: val_loss did not improve from -0.56222\n",
      "Learning rate: 2.1e-06\n",
      "Speed per epoch: -0.0025\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5909 - val_loss: -0.5613\n",
      "\n",
      "Epoch 00049: val_loss did not improve from -0.56222\n",
      "Learning rate: 1.6e-06\n",
      "Speed per epoch: 0.0008\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5978 - val_loss: -0.5616\n",
      "\n",
      "Epoch 00050: val_loss did not improve from -0.56222\n",
      "Learning rate: 1.3e-06\n",
      "Speed per epoch: 0.0009\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5977 - val_loss: -0.5615\n",
      "\n",
      "Epoch 00051: val_loss did not improve from -0.56222\n",
      "Learning rate: 7.7e-07\n",
      "Speed per epoch: -0.0025\n",
      "Wait to stop: 2\n",
      "\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5948 - val_loss: -0.5615\n",
      "\n",
      "Epoch 00052: val_loss did not improve from -0.56222\n",
      "Learning rate: 5.9e-07\n",
      "Speed per epoch: 0.0003\n",
      "Wait to stop: 1\n",
      "\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.6011 - val_loss: -0.5614\n",
      "\n",
      "Epoch 00053: val_loss did not improve from -0.56222\n",
      "Learning rate: 5.5e-07\n",
      "Speed per epoch: 0.0004\n",
      "Wait to stop: 0\n",
      "\n",
      "\n",
      "Fold - 5 / 10\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 27)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 23)     0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 1)      24          lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None, 4)      0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 5)      0           conv1d_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 64)     1024        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, None, 64)     12352       conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, None, 64)     12352       conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, None, 64)     12352       conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, None, 64)     12352       conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, None, 64)     12352       conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, None, 4)      260         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 4)            0           conv1d_8[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 63,068\n",
      "Trainable params: 63,068\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.7460 - intersection_over_union: -0.0072 - val_loss: 0.6056 - val_intersection_over_union: -0.0442\n",
      "Epoch 1/100\n",
      "  38/1000 [>.............................] - ETA: 16s - loss: -0.0319"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WLMike/Documents/PycharmProjects/ML_venv/lib/python3.6/site-packages/ipykernel_launcher.py:78: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.2916 - val_loss: -0.3836\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to -0.38357, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.2416\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.3687 - val_loss: -0.3938\n",
      "\n",
      "Epoch 00002: val_loss improved from -0.38357 to -0.39378, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0638\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4001 - val_loss: -0.3601\n",
      "\n",
      "Epoch 00003: val_loss did not improve from -0.39378\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0347\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4162 - val_loss: -0.4035\n",
      "\n",
      "Epoch 00004: val_loss improved from -0.39378 to -0.40353, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0232\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4353 - val_loss: -0.3159\n",
      "\n",
      "Epoch 00005: val_loss did not improve from -0.40353\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0207\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4238 - val_loss: -0.4066\n",
      "\n",
      "Epoch 00006: val_loss improved from -0.40353 to -0.40662, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0055\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4493 - val_loss: -0.4094\n",
      "\n",
      "Epoch 00007: val_loss improved from -0.40662 to -0.40939, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0152\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4640 - val_loss: -0.4223\n",
      "\n",
      "Epoch 00008: val_loss improved from -0.40939 to -0.42226, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0104\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4475 - val_loss: -0.4395\n",
      "\n",
      "Epoch 00009: val_loss improved from -0.42226 to -0.43950, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.3e-04\n",
      "Speed per epoch: -0.0035\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4776 - val_loss: -0.4483\n",
      "\n",
      "Epoch 00010: val_loss improved from -0.43950 to -0.44825, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.3e-04\n",
      "Speed per epoch: -0.0109\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4802 - val_loss: -0.4495\n",
      "\n",
      "Epoch 00011: val_loss improved from -0.44825 to -0.44946, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.3e-04\n",
      "Speed per epoch: -0.0103\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4950 - val_loss: -0.4492\n",
      "\n",
      "Epoch 00012: val_loss did not improve from -0.44946\n",
      "Learning rate: 7.3e-04\n",
      "Speed per epoch: -0.0112\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4729 - val_loss: -0.4584\n",
      "\n",
      "Epoch 00013: val_loss improved from -0.44946 to -0.45837, saving model to ../processed/model3.h5\n",
      "Learning rate: 5.6e-04\n",
      "Speed per epoch: 0.0019\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5054 - val_loss: -0.4511\n",
      "\n",
      "Epoch 00014: val_loss did not improve from -0.45837\n",
      "Learning rate: 4.8e-04\n",
      "Speed per epoch: -0.0101\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5067 - val_loss: -0.4895\n",
      "\n",
      "Epoch 00015: val_loss improved from -0.45837 to -0.48946, saving model to ../processed/model3.h5\n",
      "Learning rate: 4.8e-04\n",
      "Speed per epoch: -0.0068\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5112 - val_loss: -0.4705\n",
      "\n",
      "Epoch 00016: val_loss did not improve from -0.48946\n",
      "Learning rate: 4.8e-04\n",
      "Speed per epoch: -0.0059\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5175 - val_loss: -0.4771\n",
      "\n",
      "Epoch 00017: val_loss did not improve from -0.48946\n",
      "Learning rate: 4.8e-04\n",
      "Speed per epoch: -0.0077\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5153 - val_loss: -0.4924\n",
      "\n",
      "Epoch 00018: val_loss improved from -0.48946 to -0.49236, saving model to ../processed/model3.h5\n",
      "Learning rate: 4.6e-04\n",
      "Speed per epoch: -0.0053\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5235 - val_loss: -0.4872\n",
      "\n",
      "Epoch 00019: val_loss did not improve from -0.49236\n",
      "Learning rate: 4.6e-04\n",
      "Speed per epoch: -0.0065\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5174 - val_loss: -0.5017\n",
      "\n",
      "Epoch 00020: val_loss improved from -0.49236 to -0.50170, saving model to ../processed/model3.h5\n",
      "Learning rate: 3.5e-04\n",
      "Speed per epoch: 0.0000\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5362 - val_loss: -0.5030\n",
      "\n",
      "Epoch 00021: val_loss improved from -0.50170 to -0.50298, saving model to ../processed/model3.h5\n",
      "Learning rate: 3.5e-04\n",
      "Speed per epoch: -0.0045\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5305 - val_loss: -0.4997\n",
      "\n",
      "Epoch 00022: val_loss did not improve from -0.50298\n",
      "Learning rate: 3.5e-04\n",
      "Speed per epoch: -0.0021\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5405 - val_loss: -0.5043\n",
      "\n",
      "Epoch 00023: val_loss improved from -0.50298 to -0.50430, saving model to ../processed/model3.h5\n",
      "Learning rate: 3.5e-04\n",
      "Speed per epoch: -0.0024\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5456 - val_loss: -0.5071\n",
      "\n",
      "Epoch 00024: val_loss improved from -0.50430 to -0.50713, saving model to ../processed/model3.h5\n",
      "Learning rate: 3.0e-04\n",
      "Speed per epoch: -0.0064\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5483 - val_loss: -0.5069\n",
      "\n",
      "Epoch 00025: val_loss did not improve from -0.50713\n",
      "Learning rate: 3.0e-04\n",
      "Speed per epoch: -0.0013\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5427 - val_loss: -0.5161\n",
      "\n",
      "Epoch 00026: val_loss improved from -0.50713 to -0.51611, saving model to ../processed/model3.h5\n",
      "Learning rate: 1.9e-04\n",
      "Speed per epoch: 0.0008\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5640 - val_loss: -0.5178\n",
      "\n",
      "Epoch 00027: val_loss improved from -0.51611 to -0.51782, saving model to ../processed/model3.h5\n",
      "Learning rate: 1.6e-04\n",
      "Speed per epoch: -0.0080\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5742 - val_loss: -0.5188\n",
      "\n",
      "Epoch 00028: val_loss improved from -0.51782 to -0.51876, saving model to ../processed/model3.h5\n",
      "Learning rate: 1.6e-04\n",
      "Speed per epoch: -0.0073\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5703 - val_loss: -0.5131\n",
      "\n",
      "Epoch 00029: val_loss did not improve from -0.51876\n",
      "Learning rate: 1.6e-04\n",
      "Speed per epoch: -0.0069\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5635 - val_loss: -0.5210\n",
      "\n",
      "Epoch 00030: val_loss improved from -0.51876 to -0.52104, saving model to ../processed/model3.h5\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: 0.0011\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5758 - val_loss: -0.5175\n",
      "\n",
      "Epoch 00031: val_loss did not improve from -0.52104\n",
      "Learning rate: 1.1e-04\n",
      "Speed per epoch: -0.0030\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5796 - val_loss: -0.4995\n",
      "\n",
      "Epoch 00032: val_loss did not improve from -0.52104\n",
      "Learning rate: 1.1e-04\n",
      "Speed per epoch: -0.0020\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5758 - val_loss: -0.5167\n",
      "\n",
      "Epoch 00033: val_loss did not improve from -0.52104\n",
      "Learning rate: 7.2e-05\n",
      "Speed per epoch: -0.0038\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5876 - val_loss: -0.5141\n",
      "\n",
      "Epoch 00034: val_loss did not improve from -0.52104\n",
      "Learning rate: 7.2e-05\n",
      "Speed per epoch: -0.0057\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5893 - val_loss: -0.5151\n",
      "\n",
      "Epoch 00035: val_loss did not improve from -0.52104\n",
      "Learning rate: 7.2e-05\n",
      "Speed per epoch: -0.0049\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5882 - val_loss: -0.5198\n",
      "\n",
      "Epoch 00036: val_loss did not improve from -0.52104\n",
      "Learning rate: 6.1e-05\n",
      "Speed per epoch: -0.0036\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5801 - val_loss: -0.5214\n",
      "\n",
      "Epoch 00037: val_loss improved from -0.52104 to -0.52144, saving model to ../processed/model3.h5\n",
      "Learning rate: 4.0e-05\n",
      "Speed per epoch: 0.0042\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5993 - val_loss: -0.5153\n",
      "\n",
      "Epoch 00038: val_loss did not improve from -0.52144\n",
      "Learning rate: 3.4e-05\n",
      "Speed per epoch: -0.0051\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5933 - val_loss: -0.5217\n",
      "\n",
      "Epoch 00039: val_loss improved from -0.52144 to -0.52166, saving model to ../processed/model3.h5\n",
      "Learning rate: 3.4e-05\n",
      "Speed per epoch: -0.0038\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5984 - val_loss: -0.5189\n",
      "\n",
      "Epoch 00040: val_loss did not improve from -0.52166\n",
      "Learning rate: 3.0e-05\n",
      "Speed per epoch: -0.0034\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5928 - val_loss: -0.5171\n",
      "\n",
      "Epoch 00041: val_loss did not improve from -0.52166\n",
      "Learning rate: 2.1e-05\n",
      "Speed per epoch: -0.0004\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5921 - val_loss: -0.5213\n",
      "\n",
      "Epoch 00042: val_loss did not improve from -0.52166\n",
      "Learning rate: 1.3e-05\n",
      "Speed per epoch: 0.0027\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.6015 - val_loss: -0.5214\n",
      "\n",
      "Epoch 00043: val_loss did not improve from -0.52166\n",
      "Learning rate: 1.2e-05\n",
      "Speed per epoch: -0.0002\n",
      "Wait to stop: 2\n",
      "\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.6089 - val_loss: -0.5207\n",
      "\n",
      "Epoch 00044: val_loss did not improve from -0.52166\n",
      "Learning rate: 1.1e-05\n",
      "Speed per epoch: -0.0032\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.6024 - val_loss: -0.5216\n",
      "\n",
      "Epoch 00045: val_loss did not improve from -0.52166\n",
      "Learning rate: 9.5e-06\n",
      "Speed per epoch: 0.0021\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5986 - val_loss: -0.5234\n",
      "\n",
      "Epoch 00046: val_loss improved from -0.52166 to -0.52344, saving model to ../processed/model3.h5\n",
      "Learning rate: 3.8e-06\n",
      "Speed per epoch: 0.0011\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5997 - val_loss: -0.5236\n",
      "\n",
      "Epoch 00047: val_loss improved from -0.52344 to -0.52357, saving model to ../processed/model3.h5\n",
      "Learning rate: 1.7e-06\n",
      "Speed per epoch: -0.0016\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.6072 - val_loss: -0.5232\n",
      "\n",
      "Epoch 00048: val_loss did not improve from -0.52357\n",
      "Learning rate: 1.6e-06\n",
      "Speed per epoch: -0.0012\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.6070 - val_loss: -0.5233\n",
      "\n",
      "Epoch 00049: val_loss did not improve from -0.52357\n",
      "Learning rate: 1.3e-06\n",
      "Speed per epoch: -0.0013\n",
      "Wait to stop: 2\n",
      "\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.6006 - val_loss: -0.5233\n",
      "\n",
      "Epoch 00050: val_loss did not improve from -0.52357\n",
      "Learning rate: 8.5e-07\n",
      "Speed per epoch: 0.0024\n",
      "Wait to stop: 1\n",
      "\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5982 - val_loss: -0.5232\n",
      "\n",
      "Epoch 00051: val_loss did not improve from -0.52357\n",
      "Learning rate: 3.4e-07\n",
      "Speed per epoch: 0.0025\n",
      "Wait to stop: 0\n",
      "\n",
      "\n",
      "Fold - 6 / 10\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 29)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 25)     0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 1)      26          lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None, 4)      0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 5)      0           conv1d_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 64)     1024        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, None, 64)     12352       conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, None, 64)     12352       conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, None, 64)     12352       conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, None, 64)     12352       conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, None, 64)     12352       conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, None, 4)      260         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 4)            0           conv1d_8[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 63,070\n",
      "Trainable params: 63,070\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.7429 - intersection_over_union: -0.0089 - val_loss: 0.6407 - val_intersection_over_union: -0.0414\n",
      "Epoch 1/100\n",
      "  37/1000 [>.............................] - ETA: 17s - loss: -0.0406"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WLMike/Documents/PycharmProjects/ML_venv/lib/python3.6/site-packages/ipykernel_launcher.py:78: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.3055 - val_loss: -0.3048\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to -0.30480, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.1703\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.3814 - val_loss: -0.4316\n",
      "\n",
      "Epoch 00002: val_loss improved from -0.30480 to -0.43161, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0683\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4082 - val_loss: -0.3736\n",
      "\n",
      "Epoch 00003: val_loss did not improve from -0.43161\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0402\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4255 - val_loss: -0.4204\n",
      "\n",
      "Epoch 00004: val_loss did not improve from -0.43161\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0289\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4395 - val_loss: -0.4696\n",
      "\n",
      "Epoch 00005: val_loss improved from -0.43161 to -0.46963, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0184\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4511 - val_loss: -0.4200\n",
      "\n",
      "Epoch 00006: val_loss did not improve from -0.46963\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0120\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4457 - val_loss: -0.4784\n",
      "\n",
      "Epoch 00007: val_loss improved from -0.46963 to -0.47841, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0051\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4503 - val_loss: -0.4878\n",
      "\n",
      "Epoch 00008: val_loss improved from -0.47841 to -0.48784, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.1e-04\n",
      "Speed per epoch: -0.0074\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4657 - val_loss: -0.5047\n",
      "\n",
      "Epoch 00009: val_loss improved from -0.48784 to -0.50470, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.1e-04\n",
      "Speed per epoch: -0.0067\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4726 - val_loss: -0.4505\n",
      "\n",
      "Epoch 00010: val_loss did not improve from -0.50470\n",
      "Learning rate: 7.1e-04\n",
      "Speed per epoch: -0.0121\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4812 - val_loss: -0.4883\n",
      "\n",
      "Epoch 00011: val_loss did not improve from -0.50470\n",
      "Learning rate: 7.1e-04\n",
      "Speed per epoch: -0.0119\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4845 - val_loss: -0.4307\n",
      "\n",
      "Epoch 00012: val_loss did not improve from -0.50470\n",
      "Learning rate: 7.1e-04\n",
      "Speed per epoch: -0.0063\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4809 - val_loss: -0.4593\n",
      "\n",
      "Epoch 00013: val_loss did not improve from -0.50470\n",
      "Learning rate: 5.7e-04\n",
      "Speed per epoch: -0.0038\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4860 - val_loss: -0.4781\n",
      "\n",
      "Epoch 00014: val_loss did not improve from -0.50470\n",
      "Learning rate: 5.7e-04\n",
      "Speed per epoch: -0.0028\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4849 - val_loss: -0.5000\n",
      "\n",
      "Epoch 00015: val_loss did not improve from -0.50470\n",
      "Learning rate: 4.8e-04\n",
      "Speed per epoch: -0.0010\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5117 - val_loss: -0.5092\n",
      "\n",
      "Epoch 00016: val_loss improved from -0.50470 to -0.50920, saving model to ../processed/model3.h5\n",
      "Learning rate: 4.8e-04\n",
      "Speed per epoch: -0.0094\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5042 - val_loss: -0.4964\n",
      "\n",
      "Epoch 00017: val_loss did not improve from -0.50920\n",
      "Learning rate: 4.8e-04\n",
      "Speed per epoch: -0.0068\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5093 - val_loss: -0.4898\n",
      "\n",
      "Epoch 00018: val_loss did not improve from -0.50920\n",
      "Learning rate: 4.8e-04\n",
      "Speed per epoch: -0.0076\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5176 - val_loss: -0.5094\n",
      "\n",
      "Epoch 00019: val_loss improved from -0.50920 to -0.50938, saving model to ../processed/model3.h5\n",
      "Learning rate: 4.8e-04\n",
      "Speed per epoch: -0.0074\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5026 - val_loss: -0.4965\n",
      "\n",
      "Epoch 00020: val_loss did not improve from -0.50938\n",
      "Learning rate: 3.2e-04\n",
      "Speed per epoch: 0.0029\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5320 - val_loss: -0.5310\n",
      "\n",
      "Epoch 00021: val_loss improved from -0.50938 to -0.53097, saving model to ../processed/model3.h5\n",
      "Learning rate: 2.1e-04\n",
      "Speed per epoch: -0.0117\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5410 - val_loss: -0.5458\n",
      "\n",
      "Epoch 00022: val_loss improved from -0.53097 to -0.54577, saving model to ../processed/model3.h5\n",
      "Learning rate: 2.1e-04\n",
      "Speed per epoch: -0.0089\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5492 - val_loss: -0.5492\n",
      "\n",
      "Epoch 00023: val_loss improved from -0.54577 to -0.54917, saving model to ../processed/model3.h5\n",
      "Learning rate: 2.1e-04\n",
      "Speed per epoch: -0.0102\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5368 - val_loss: -0.5427\n",
      "\n",
      "Epoch 00024: val_loss did not improve from -0.54917\n",
      "Learning rate: 2.1e-04\n",
      "Speed per epoch: -0.0013\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5396 - val_loss: -0.5141\n",
      "\n",
      "Epoch 00025: val_loss did not improve from -0.54917\n",
      "Learning rate: 1.9e-04\n",
      "Speed per epoch: 0.0017\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5496 - val_loss: -0.5469\n",
      "\n",
      "Epoch 00026: val_loss did not improve from -0.54917\n",
      "Learning rate: 1.5e-04\n",
      "Speed per epoch: -0.0024\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5607 - val_loss: -0.5426\n",
      "\n",
      "Epoch 00027: val_loss did not improve from -0.54917\n",
      "Learning rate: 1.5e-04\n",
      "Speed per epoch: -0.0053\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5514 - val_loss: -0.5532\n",
      "\n",
      "Epoch 00028: val_loss improved from -0.54917 to -0.55315, saving model to ../processed/model3.h5\n",
      "Learning rate: 1.5e-04\n",
      "Speed per epoch: 0.0008\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5573 - val_loss: -0.5394\n",
      "\n",
      "Epoch 00029: val_loss did not improve from -0.55315\n",
      "Learning rate: 1.2e-04\n",
      "Speed per epoch: -0.0013\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5685 - val_loss: -0.5552\n",
      "\n",
      "Epoch 00030: val_loss improved from -0.55315 to -0.55517, saving model to ../processed/model3.h5\n",
      "Learning rate: 1.2e-04\n",
      "Speed per epoch: -0.0046\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5682 - val_loss: -0.5642\n",
      "\n",
      "Epoch 00031: val_loss improved from -0.55517 to -0.56416, saving model to ../processed/model3.h5\n",
      "Learning rate: 1.2e-04\n",
      "Speed per epoch: -0.0045\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5686 - val_loss: -0.5586\n",
      "\n",
      "Epoch 00032: val_loss did not improve from -0.56416\n",
      "Learning rate: 9.1e-05\n",
      "Speed per epoch: -0.0051\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5681 - val_loss: -0.5575\n",
      "\n",
      "Epoch 00033: val_loss did not improve from -0.56416\n",
      "Learning rate: 9.1e-05\n",
      "Speed per epoch: -0.0017\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5613 - val_loss: -0.5562\n",
      "\n",
      "Epoch 00034: val_loss did not improve from -0.56416\n",
      "Learning rate: 3.8e-05\n",
      "Speed per epoch: 0.0013\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5809 - val_loss: -0.5602\n",
      "\n",
      "Epoch 00035: val_loss did not improve from -0.56416\n",
      "Learning rate: 3.5e-05\n",
      "Speed per epoch: -0.0058\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5772 - val_loss: -0.5593\n",
      "\n",
      "Epoch 00036: val_loss did not improve from -0.56416\n",
      "Learning rate: 3.5e-05\n",
      "Speed per epoch: -0.0034\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5871 - val_loss: -0.5592\n",
      "\n",
      "Epoch 00037: val_loss did not improve from -0.56416\n",
      "Learning rate: 3.5e-05\n",
      "Speed per epoch: -0.0030\n",
      "Wait to stop: 2\n",
      "\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5832 - val_loss: -0.5577\n",
      "\n",
      "Epoch 00038: val_loss did not improve from -0.56416\n",
      "Learning rate: 3.5e-05\n",
      "Speed per epoch: -0.0003\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5858 - val_loss: -0.5661\n",
      "\n",
      "Epoch 00039: val_loss improved from -0.56416 to -0.56607, saving model to ../processed/model3.h5\n",
      "Learning rate: 2.6e-05\n",
      "Speed per epoch: -0.0024\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5916 - val_loss: -0.5614\n",
      "\n",
      "Epoch 00040: val_loss did not improve from -0.56607\n",
      "Learning rate: 2.5e-05\n",
      "Speed per epoch: -0.0046\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5885 - val_loss: -0.5571\n",
      "\n",
      "Epoch 00041: val_loss did not improve from -0.56607\n",
      "Learning rate: 2.5e-05\n",
      "Speed per epoch: 0.0003\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5810 - val_loss: -0.5590\n",
      "\n",
      "Epoch 00042: val_loss did not improve from -0.56607\n",
      "Learning rate: 1.2e-05\n",
      "Speed per epoch: 0.0051\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5853 - val_loss: -0.5659\n",
      "\n",
      "Epoch 00043: val_loss did not improve from -0.56607\n",
      "Learning rate: 5.2e-06\n",
      "Speed per epoch: -0.0012\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5990 - val_loss: -0.5627\n",
      "\n",
      "Epoch 00044: val_loss did not improve from -0.56607\n",
      "Learning rate: 5.2e-06\n",
      "Speed per epoch: -0.0035\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5900 - val_loss: -0.5642\n",
      "\n",
      "Epoch 00045: val_loss did not improve from -0.56607\n",
      "Learning rate: 4.8e-06\n",
      "Speed per epoch: 0.0020\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5820 - val_loss: -0.5656\n",
      "\n",
      "Epoch 00046: val_loss did not improve from -0.56607\n",
      "Learning rate: 1.8e-06\n",
      "Speed per epoch: 0.0021\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5900 - val_loss: -0.5652\n",
      "\n",
      "Epoch 00047: val_loss did not improve from -0.56607\n",
      "Learning rate: 1.0e-06\n",
      "Speed per epoch: -0.0005\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5978 - val_loss: -0.5648\n",
      "\n",
      "Epoch 00048: val_loss did not improve from -0.56607\n",
      "Learning rate: 8.2e-07\n",
      "Speed per epoch: -0.0042\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5850 - val_loss: -0.5653\n",
      "\n",
      "Epoch 00049: val_loss did not improve from -0.56607\n",
      "Learning rate: 4.9e-07\n",
      "Speed per epoch: 0.0019\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5903 - val_loss: -0.5649\n",
      "\n",
      "Epoch 00050: val_loss did not improve from -0.56607\n",
      "Learning rate: 2.7e-07\n",
      "Speed per epoch: 0.0013\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5899 - val_loss: -0.5651\n",
      "\n",
      "Epoch 00051: val_loss did not improve from -0.56607\n",
      "Learning rate: 1.3e-07\n",
      "Speed per epoch: -0.0008\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5973 - val_loss: -0.5651\n",
      "\n",
      "Epoch 00052: val_loss did not improve from -0.56607\n",
      "Learning rate: 1.3e-07\n",
      "Speed per epoch: -0.0008\n",
      "Wait to stop: 2\n",
      "\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.6007 - val_loss: -0.5653\n",
      "\n",
      "Epoch 00053: val_loss did not improve from -0.56607\n",
      "Learning rate: 1.3e-07\n",
      "Speed per epoch: -0.0021\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5866 - val_loss: -0.5653\n",
      "\n",
      "Epoch 00054: val_loss did not improve from -0.56607\n",
      "Learning rate: 5.5e-08\n",
      "Speed per epoch: 0.0019\n",
      "Wait to stop: 2\n",
      "\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5872 - val_loss: -0.5653\n",
      "\n",
      "Epoch 00055: val_loss did not improve from -0.56607\n",
      "Learning rate: 2.0e-08\n",
      "Speed per epoch: 0.0025\n",
      "Wait to stop: 1\n",
      "\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5894 - val_loss: -0.5653\n",
      "\n",
      "Epoch 00056: val_loss did not improve from -0.56607\n",
      "Learning rate: 8.5e-09\n",
      "Speed per epoch: 0.0002\n",
      "Wait to stop: 0\n",
      "\n",
      "\n",
      "Fold - 7 / 10\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 28)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 24)     0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 1)      25          lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None, 4)      0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 5)      0           conv1d_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 64)     1024        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, None, 64)     12352       conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, None, 64)     12352       conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, None, 64)     12352       conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, None, 64)     12352       conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, None, 64)     12352       conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, None, 4)      260         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 4)            0           conv1d_8[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 63,069\n",
      "Trainable params: 63,069\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.7750 - intersection_over_union: -0.0072 - val_loss: 0.6733 - val_intersection_over_union: -0.0478\n",
      "Epoch 1/100\n",
      "  40/1000 [>.............................] - ETA: 15s - loss: -0.0696"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WLMike/Documents/PycharmProjects/ML_venv/lib/python3.6/site-packages/ipykernel_launcher.py:78: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.2960 - val_loss: -0.4268\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to -0.42677, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.1763\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.3906 - val_loss: -0.4216\n",
      "\n",
      "Epoch 00002: val_loss did not improve from -0.42677\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0799\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4203 - val_loss: -0.4810\n",
      "\n",
      "Epoch 00003: val_loss improved from -0.42677 to -0.48099, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0452\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4127 - val_loss: -0.4355\n",
      "\n",
      "Epoch 00004: val_loss did not improve from -0.48099\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0185\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4330 - val_loss: -0.4530\n",
      "\n",
      "Epoch 00005: val_loss did not improve from -0.48099\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0171\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4428 - val_loss: -0.4959\n",
      "\n",
      "Epoch 00006: val_loss improved from -0.48099 to -0.49590, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0130\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4560 - val_loss: -0.4535\n",
      "\n",
      "Epoch 00007: val_loss did not improve from -0.49590\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0106\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4522 - val_loss: -0.4954\n",
      "\n",
      "Epoch 00008: val_loss did not improve from -0.49590\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0080\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4477 - val_loss: -0.4777\n",
      "\n",
      "Epoch 00009: val_loss did not improve from -0.49590\n",
      "Learning rate: 7.8e-04\n",
      "Speed per epoch: -0.0010\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4666 - val_loss: -0.4775\n",
      "\n",
      "Epoch 00010: val_loss did not improve from -0.49590\n",
      "Learning rate: 7.0e-04\n",
      "Speed per epoch: -0.0093\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4683 - val_loss: -0.4819\n",
      "\n",
      "Epoch 00011: val_loss did not improve from -0.49590\n",
      "Learning rate: 7.0e-04\n",
      "Speed per epoch: -0.0085\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4715 - val_loss: -0.4992\n",
      "\n",
      "Epoch 00012: val_loss improved from -0.49590 to -0.49918, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.0e-04\n",
      "Speed per epoch: -0.0067\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4739 - val_loss: -0.5055\n",
      "\n",
      "Epoch 00013: val_loss improved from -0.49918 to -0.50546, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.0e-04\n",
      "Speed per epoch: -0.0040\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4783 - val_loss: -0.4608\n",
      "\n",
      "Epoch 00014: val_loss did not improve from -0.50546\n",
      "Learning rate: 7.0e-04\n",
      "Speed per epoch: -0.0031\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4844 - val_loss: -0.5067\n",
      "\n",
      "Epoch 00015: val_loss improved from -0.50546 to -0.50673, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.0e-04\n",
      "Speed per epoch: -0.0070\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4871 - val_loss: -0.5426\n",
      "\n",
      "Epoch 00016: val_loss improved from -0.50673 to -0.54260, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.0e-04\n",
      "Speed per epoch: -0.0024\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4886 - val_loss: -0.5138\n",
      "\n",
      "Epoch 00017: val_loss did not improve from -0.54260\n",
      "Learning rate: 6.7e-04\n",
      "Speed per epoch: -0.0021\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4948 - val_loss: -0.5202\n",
      "\n",
      "Epoch 00018: val_loss did not improve from -0.54260\n",
      "Learning rate: 6.7e-04\n",
      "Speed per epoch: -0.0015\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4858 - val_loss: -0.5088\n",
      "\n",
      "Epoch 00019: val_loss did not improve from -0.54260\n",
      "Learning rate: 3.1e-04\n",
      "Speed per epoch: 0.0010\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5230 - val_loss: -0.5501\n",
      "\n",
      "Epoch 00020: val_loss improved from -0.54260 to -0.55013, saving model to ../processed/model3.h5\n",
      "Learning rate: 2.9e-04\n",
      "Speed per epoch: -0.0127\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5383 - val_loss: -0.5422\n",
      "\n",
      "Epoch 00021: val_loss did not improve from -0.55013\n",
      "Learning rate: 2.9e-04\n",
      "Speed per epoch: -0.0159\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5320 - val_loss: -0.5470\n",
      "\n",
      "Epoch 00022: val_loss did not improve from -0.55013\n",
      "Learning rate: 2.9e-04\n",
      "Speed per epoch: -0.0090\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5425 - val_loss: -0.5554\n",
      "\n",
      "Epoch 00023: val_loss improved from -0.55013 to -0.55544, saving model to ../processed/model3.h5\n",
      "Learning rate: 2.9e-04\n",
      "Speed per epoch: -0.0071\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5404 - val_loss: -0.5432\n",
      "\n",
      "Epoch 00024: val_loss did not improve from -0.55544\n",
      "Learning rate: 2.9e-04\n",
      "Speed per epoch: -0.0046\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5453 - val_loss: -0.5358\n",
      "\n",
      "Epoch 00025: val_loss did not improve from -0.55544\n",
      "Learning rate: 2.9e-04\n",
      "Speed per epoch: -0.0025\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5413 - val_loss: -0.5463\n",
      "\n",
      "Epoch 00026: val_loss did not improve from -0.55544\n",
      "Learning rate: 2.3e-04\n",
      "Speed per epoch: -0.0005\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5510 - val_loss: -0.5502\n",
      "\n",
      "Epoch 00027: val_loss did not improve from -0.55544\n",
      "Learning rate: 1.6e-04\n",
      "Speed per epoch: -0.0080\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5637 - val_loss: -0.5602\n",
      "\n",
      "Epoch 00028: val_loss improved from -0.55544 to -0.56023, saving model to ../processed/model3.h5\n",
      "Learning rate: 1.6e-04\n",
      "Speed per epoch: -0.0064\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5510 - val_loss: -0.5612\n",
      "\n",
      "Epoch 00029: val_loss improved from -0.56023 to -0.56117, saving model to ../processed/model3.h5\n",
      "Learning rate: 9.2e-05\n",
      "Speed per epoch: -0.0075\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5716 - val_loss: -0.5662\n",
      "\n",
      "Epoch 00030: val_loss improved from -0.56117 to -0.56622, saving model to ../processed/model3.h5\n",
      "Learning rate: 9.2e-05\n",
      "Speed per epoch: -0.0091\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5683 - val_loss: -0.5563\n",
      "\n",
      "Epoch 00031: val_loss did not improve from -0.56622\n",
      "Learning rate: 9.2e-05\n",
      "Speed per epoch: -0.0017\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5717 - val_loss: -0.5699\n",
      "\n",
      "Epoch 00032: val_loss improved from -0.56622 to -0.56989, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.4e-05\n",
      "Speed per epoch: -0.0036\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5715 - val_loss: -0.5783\n",
      "\n",
      "Epoch 00033: val_loss improved from -0.56989 to -0.57826, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.2e-05\n",
      "Speed per epoch: -0.0004\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5838 - val_loss: -0.5732\n",
      "\n",
      "Epoch 00034: val_loss did not improve from -0.57826\n",
      "Learning rate: 7.1e-05\n",
      "Speed per epoch: -0.0054\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5828 - val_loss: -0.5723\n",
      "\n",
      "Epoch 00035: val_loss did not improve from -0.57826\n",
      "Learning rate: 7.0e-05\n",
      "Speed per epoch: -0.0031\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5747 - val_loss: -0.5721\n",
      "\n",
      "Epoch 00036: val_loss did not improve from -0.57826\n",
      "Learning rate: 5.4e-05\n",
      "Speed per epoch: 0.0019\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5809 - val_loss: -0.5683\n",
      "\n",
      "Epoch 00037: val_loss did not improve from -0.57826\n",
      "Learning rate: 4.1e-05\n",
      "Speed per epoch: 0.0009\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5778 - val_loss: -0.5727\n",
      "\n",
      "Epoch 00038: val_loss did not improve from -0.57826\n",
      "Learning rate: 2.3e-05\n",
      "Speed per epoch: -0.0007\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5827 - val_loss: -0.5786\n",
      "\n",
      "Epoch 00039: val_loss improved from -0.57826 to -0.57863, saving model to ../processed/model3.h5\n",
      "Learning rate: 1.6e-05\n",
      "Speed per epoch: -0.0031\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5891 - val_loss: -0.5778\n",
      "\n",
      "Epoch 00040: val_loss did not improve from -0.57863\n",
      "Learning rate: 1.6e-05\n",
      "Speed per epoch: -0.0049\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5928 - val_loss: -0.5717\n",
      "\n",
      "Epoch 00041: val_loss did not improve from -0.57863\n",
      "Learning rate: 1.6e-05\n",
      "Speed per epoch: -0.0031\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5917 - val_loss: -0.5776\n",
      "\n",
      "Epoch 00042: val_loss did not improve from -0.57863\n",
      "Learning rate: 1.6e-05\n",
      "Speed per epoch: -0.0035\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5823 - val_loss: -0.5789\n",
      "\n",
      "Epoch 00043: val_loss improved from -0.57863 to -0.57891, saving model to ../processed/model3.h5\n",
      "Learning rate: 1.0e-05\n",
      "Speed per epoch: 0.0030\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5946 - val_loss: -0.5769\n",
      "\n",
      "Epoch 00044: val_loss did not improve from -0.57891\n",
      "Learning rate: 7.1e-06\n",
      "Speed per epoch: -0.0031\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5990 - val_loss: -0.5781\n",
      "\n",
      "Epoch 00045: val_loss did not improve from -0.57891\n",
      "Learning rate: 7.1e-06\n",
      "Speed per epoch: -0.0044\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5908 - val_loss: -0.5771\n",
      "\n",
      "Epoch 00046: val_loss did not improve from -0.57891\n",
      "Learning rate: 5.6e-06\n",
      "Speed per epoch: 0.0012\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5897 - val_loss: -0.5768\n",
      "\n",
      "Epoch 00047: val_loss did not improve from -0.57891\n",
      "Learning rate: 2.3e-06\n",
      "Speed per epoch: 0.0002\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5890 - val_loss: -0.5769\n",
      "\n",
      "Epoch 00048: val_loss did not improve from -0.57891\n",
      "Learning rate: 1.1e-06\n",
      "Speed per epoch: 0.0010\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5934 - val_loss: -0.5769\n",
      "\n",
      "Epoch 00049: val_loss did not improve from -0.57891\n",
      "Learning rate: 7.5e-07\n",
      "Speed per epoch: 0.0003\n",
      "Wait to stop: 2\n",
      "\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.6069 - val_loss: -0.5769\n",
      "\n",
      "Epoch 00050: val_loss did not improve from -0.57891\n",
      "Learning rate: 7.2e-07\n",
      "Speed per epoch: -0.0052\n",
      "Wait to stop: 1\n",
      "\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5904 - val_loss: -0.5767\n",
      "\n",
      "Epoch 00051: val_loss did not improve from -0.57891\n",
      "Learning rate: 4.6e-07\n",
      "Speed per epoch: 0.0024\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5986 - val_loss: -0.5770\n",
      "\n",
      "Epoch 00052: val_loss did not improve from -0.57891\n",
      "Learning rate: 2.9e-07\n",
      "Speed per epoch: 0.0003\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5998 - val_loss: -0.5773\n",
      "\n",
      "Epoch 00053: val_loss did not improve from -0.57891\n",
      "Learning rate: 2.4e-07\n",
      "Speed per epoch: -0.0001\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5888 - val_loss: -0.5773\n",
      "\n",
      "Epoch 00054: val_loss did not improve from -0.57891\n",
      "Learning rate: 9.3e-08\n",
      "Speed per epoch: 0.0038\n",
      "Wait to stop: 2\n",
      "\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5971 - val_loss: -0.5773\n",
      "\n",
      "Epoch 00055: val_loss did not improve from -0.57891\n",
      "Learning rate: 4.9e-08\n",
      "Speed per epoch: -0.0002\n",
      "Wait to stop: 1\n",
      "\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5952 - val_loss: -0.5773\n",
      "\n",
      "Epoch 00056: val_loss did not improve from -0.57891\n",
      "Learning rate: 2.7e-08\n",
      "Speed per epoch: -0.0016\n",
      "Wait to stop: 0\n",
      "\n",
      "\n",
      "Fold - 8 / 10\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 28)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 24)     0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 1)      25          lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None, 4)      0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 5)      0           conv1d_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 64)     1024        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, None, 64)     12352       conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, None, 64)     12352       conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, None, 64)     12352       conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, None, 64)     12352       conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, None, 64)     12352       conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, None, 4)      260         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 4)            0           conv1d_8[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 63,069\n",
      "Trainable params: 63,069\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6945 - intersection_over_union: -0.0411 - val_loss: 0.5716 - val_intersection_over_union: -0.1480\n",
      "Epoch 1/100\n",
      "  34/1000 [>.............................] - ETA: 19s - loss: -0.1581"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WLMike/Documents/PycharmProjects/ML_venv/lib/python3.6/site-packages/ipykernel_launcher.py:78: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.3297 - val_loss: -0.3413\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to -0.34133, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0984\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.3933 - val_loss: -0.3671\n",
      "\n",
      "Epoch 00002: val_loss improved from -0.34133 to -0.36715, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0550\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.3992 - val_loss: -0.3176\n",
      "\n",
      "Epoch 00003: val_loss did not improve from -0.36715\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0280\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4445 - val_loss: -0.4256\n",
      "\n",
      "Epoch 00004: val_loss improved from -0.36715 to -0.42564, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0346\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4482 - val_loss: -0.4050\n",
      "\n",
      "Epoch 00005: val_loss did not improve from -0.42564\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0180\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4573 - val_loss: -0.3914\n",
      "\n",
      "Epoch 00006: val_loss did not improve from -0.42564\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0158\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4588 - val_loss: -0.4333\n",
      "\n",
      "Epoch 00007: val_loss improved from -0.42564 to -0.43333, saving model to ../processed/model3.h5\n",
      "Learning rate: 7.9e-04\n",
      "Speed per epoch: -0.0116\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4554 - val_loss: -0.4213\n",
      "\n",
      "Epoch 00008: val_loss did not improve from -0.43333\n",
      "Learning rate: 6.5e-04\n",
      "Speed per epoch: -0.0052\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4638 - val_loss: -0.3837\n",
      "\n",
      "Epoch 00009: val_loss did not improve from -0.43333\n",
      "Learning rate: 6.5e-04\n",
      "Speed per epoch: -0.0061\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4825 - val_loss: -0.4275\n",
      "\n",
      "Epoch 00010: val_loss did not improve from -0.43333\n",
      "Learning rate: 6.5e-04\n",
      "Speed per epoch: -0.0130\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4828 - val_loss: -0.3975\n",
      "\n",
      "Epoch 00011: val_loss did not improve from -0.43333\n",
      "Learning rate: 6.5e-04\n",
      "Speed per epoch: -0.0091\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4856 - val_loss: -0.4273\n",
      "\n",
      "Epoch 00012: val_loss did not improve from -0.43333\n",
      "Learning rate: 6.5e-04\n",
      "Speed per epoch: -0.0014\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4908 - val_loss: -0.3913\n",
      "\n",
      "Epoch 00013: val_loss did not improve from -0.43333\n",
      "Learning rate: 5.7e-04\n",
      "Speed per epoch: -0.0038\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4967 - val_loss: -0.4374\n",
      "\n",
      "Epoch 00014: val_loss improved from -0.43333 to -0.43738, saving model to ../processed/model3.h5\n",
      "Learning rate: 5.7e-04\n",
      "Speed per epoch: -0.0040\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5024 - val_loss: -0.4013\n",
      "\n",
      "Epoch 00015: val_loss did not improve from -0.43738\n",
      "Learning rate: 5.7e-04\n",
      "Speed per epoch: -0.0047\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5147 - val_loss: -0.4592\n",
      "\n",
      "Epoch 00016: val_loss improved from -0.43738 to -0.45917, saving model to ../processed/model3.h5\n",
      "Learning rate: 5.7e-04\n",
      "Speed per epoch: -0.0044\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5055 - val_loss: -0.4426\n",
      "\n",
      "Epoch 00017: val_loss did not improve from -0.45917\n",
      "Learning rate: 4.1e-04\n",
      "Speed per epoch: -0.0050\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5187 - val_loss: -0.4615\n",
      "\n",
      "Epoch 00018: val_loss improved from -0.45917 to -0.46147, saving model to ../processed/model3.h5\n",
      "Learning rate: 4.1e-04\n",
      "Speed per epoch: 0.0003\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5234 - val_loss: -0.3765\n",
      "\n",
      "Epoch 00019: val_loss did not improve from -0.46147\n",
      "Learning rate: 4.0e-04\n",
      "Speed per epoch: -0.0028\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5274 - val_loss: -0.4630\n",
      "\n",
      "Epoch 00020: val_loss improved from -0.46147 to -0.46304, saving model to ../processed/model3.h5\n",
      "Learning rate: 4.0e-04\n",
      "Speed per epoch: -0.0020\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5307 - val_loss: -0.4828\n",
      "\n",
      "Epoch 00021: val_loss improved from -0.46304 to -0.48277, saving model to ../processed/model3.h5\n",
      "Learning rate: 4.0e-04\n",
      "Speed per epoch: -0.0035\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5504 - val_loss: -0.4801\n",
      "\n",
      "Epoch 00022: val_loss did not improve from -0.48277\n",
      "Learning rate: 4.0e-04\n",
      "Speed per epoch: -0.0097\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5320 - val_loss: -0.4554\n",
      "\n",
      "Epoch 00023: val_loss did not improve from -0.48277\n",
      "Learning rate: 3.1e-04\n",
      "Speed per epoch: -0.0010\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5438 - val_loss: -0.4711\n",
      "\n",
      "Epoch 00024: val_loss did not improve from -0.48277\n",
      "Learning rate: 2.9e-04\n",
      "Speed per epoch: -0.0003\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5497 - val_loss: -0.4333\n",
      "\n",
      "Epoch 00025: val_loss did not improve from -0.48277\n",
      "Learning rate: 2.5e-04\n",
      "Speed per epoch: -0.0054\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5615 - val_loss: -0.4761\n",
      "\n",
      "Epoch 00026: val_loss did not improve from -0.48277\n",
      "Learning rate: 2.5e-04\n",
      "Speed per epoch: -0.0109\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5603 - val_loss: -0.4861\n",
      "\n",
      "Epoch 00027: val_loss improved from -0.48277 to -0.48614, saving model to ../processed/model3.h5\n",
      "Learning rate: 2.5e-04\n",
      "Speed per epoch: -0.0063\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5555 - val_loss: -0.4862\n",
      "\n",
      "Epoch 00028: val_loss improved from -0.48614 to -0.48616, saving model to ../processed/model3.h5\n",
      "Learning rate: 2.4e-04\n",
      "Speed per epoch: -0.0019\n",
      "Wait to stop: 2\n",
      "\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5719 - val_loss: -0.4672\n",
      "\n",
      "Epoch 00029: val_loss did not improve from -0.48616\n",
      "Learning rate: 2.4e-04\n",
      "Speed per epoch: -0.0051\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5579 - val_loss: -0.4878\n",
      "\n",
      "Epoch 00030: val_loss improved from -0.48616 to -0.48780, saving model to ../processed/model3.h5\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.0010\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5768 - val_loss: -0.5041\n",
      "\n",
      "Epoch 00031: val_loss improved from -0.48780 to -0.50412, saving model to ../processed/model3.h5\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.0054\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5778 - val_loss: -0.4925\n",
      "\n",
      "Epoch 00032: val_loss did not improve from -0.50412\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.0066\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5654 - val_loss: -0.4670\n",
      "\n",
      "Epoch 00033: val_loss did not improve from -0.50412\n",
      "Learning rate: 1.2e-04\n",
      "Speed per epoch: 0.0031\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5849 - val_loss: -0.4950\n",
      "\n",
      "Epoch 00034: val_loss did not improve from -0.50412\n",
      "Learning rate: 8.3e-05\n",
      "Speed per epoch: -0.0059\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5894 - val_loss: -0.4961\n",
      "\n",
      "Epoch 00035: val_loss did not improve from -0.50412\n",
      "Learning rate: 8.3e-05\n",
      "Speed per epoch: -0.0081\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5855 - val_loss: -0.5074\n",
      "\n",
      "Epoch 00036: val_loss improved from -0.50412 to -0.50742, saving model to ../processed/model3.h5\n",
      "Learning rate: 8.2e-05\n",
      "Speed per epoch: -0.0022\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5829 - val_loss: -0.4996\n",
      "\n",
      "Epoch 00037: val_loss did not improve from -0.50742\n",
      "Learning rate: 4.8e-05\n",
      "Speed per epoch: -0.0015\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5926 - val_loss: -0.4942\n",
      "\n",
      "Epoch 00038: val_loss did not improve from -0.50742\n",
      "Learning rate: 4.8e-05\n",
      "Speed per epoch: -0.0039\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5907 - val_loss: -0.4957\n",
      "\n",
      "Epoch 00039: val_loss did not improve from -0.50742\n",
      "Learning rate: 4.8e-05\n",
      "Speed per epoch: -0.0012\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5875 - val_loss: -0.5001\n",
      "\n",
      "Epoch 00040: val_loss did not improve from -0.50742\n",
      "Learning rate: 2.6e-05\n",
      "Speed per epoch: -0.0002\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.6072 - val_loss: -0.4994\n",
      "\n",
      "Epoch 00041: val_loss did not improve from -0.50742\n",
      "Learning rate: 2.1e-05\n",
      "Speed per epoch: -0.0081\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.6044 - val_loss: -0.5043\n",
      "\n",
      "Epoch 00042: val_loss did not improve from -0.50742\n",
      "Learning rate: 2.1e-05\n",
      "Speed per epoch: -0.0050\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.6119 - val_loss: -0.5033\n",
      "\n",
      "Epoch 00043: val_loss did not improve from -0.50742\n",
      "Learning rate: 2.1e-05\n",
      "Speed per epoch: -0.0076\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.6027 - val_loss: -0.5048\n",
      "\n",
      "Epoch 00044: val_loss did not improve from -0.50742\n",
      "Learning rate: 1.7e-05\n",
      "Speed per epoch: 0.0018\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5991 - val_loss: -0.5039\n",
      "\n",
      "Epoch 00045: val_loss did not improve from -0.50742\n",
      "Learning rate: 6.6e-06\n",
      "Speed per epoch: 0.0018\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.6015 - val_loss: -0.5036\n",
      "\n",
      "Epoch 00046: val_loss did not improve from -0.50742\n",
      "Learning rate: 2.6e-06\n",
      "Speed per epoch: 0.0005\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.6131 - val_loss: -0.5042\n",
      "\n",
      "Epoch 00047: val_loss did not improve from -0.50742\n",
      "Learning rate: 2.6e-06\n",
      "Speed per epoch: -0.0028\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 48/100\n",
      " 328/1000 [========>.....................] - ETA: 3s - loss: -0.6163"
     ]
    }
   ],
   "source": [
    "train_oof((votes, answers), votes_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
