{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "from keras import backend as K\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "COORDINATES = [\"Xmin\", \"Ymin\", \"Xmax\", \"Ymax\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WLMike/Documents/PycharmProjects/ML_venv/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/WLMike/Documents/PycharmProjects/ML_venv/lib/python3.6/site-packages/ipykernel_launcher.py:4: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "answers = pd.read_csv(\"../raw/train_answers.csv\").set_index(\"itemId\")\n",
    "answers.columns = COORDINATES\n",
    "scaler = preprocessing.StandardScaler().fit(np.vstack([answers.values, answers.values[:, [2, 3, 0, 1]]]))\n",
    "answers[COORDINATES] = scaler.transform(answers[COORDINATES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WLMike/Documents/PycharmProjects/ML_venv/lib/python3.6/site-packages/ipykernel_launcher.py:2: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "votes = pd.read_csv(\"../raw/train_data.csv\").set_index(\"itemId\")\n",
    "votes[COORDINATES] = scaler.transform(votes[COORDINATES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WLMike/Documents/PycharmProjects/ML_venv/lib/python3.6/site-packages/ipykernel_launcher.py:2: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "votes_test = pd.read_csv(\"../raw/test_data.csv\").set_index(\"itemId\")\n",
    "votes_test[COORDINATES] = scaler.transform(votes_test[COORDINATES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_batch(data):\n",
    "    votes, answers = data\n",
    "    while True:\n",
    "        item_id = np.random.choice(votes.index, 1)\n",
    "        forecasts = votes.loc[item_id].set_index(\"userId\")\n",
    "        x = np.zeros((1, len(forecasts), 4),)\n",
    "        y = np.zeros((1, 4))\n",
    "        \n",
    "        x[0] = forecasts.sample(len(forecasts))\n",
    "        y[0] = answers.loc[item_id]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_batch_val(data):\n",
    "    votes, answers = data\n",
    "    item_ids = set(votes.index)\n",
    "    while True:\n",
    "        for item_id in item_ids:\n",
    "            forecasts = votes.loc[item_id].set_index(\"userId\")\n",
    "            x = np.zeros((1, len(forecasts), 4),)\n",
    "            y = np.zeros((1, 4))\n",
    "\n",
    "            x[0] = forecasts\n",
    "            y[0] = answers.loc[item_id]\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_batch_test(data):\n",
    "    item_ids = data.index.unique()\n",
    "    for item_id in item_ids:\n",
    "        forecasts = data.loc[item_id].set_index(\"userId\")\n",
    "        x = np.zeros((1, len(forecasts), 4),)\n",
    "        y = np.zeros((1, 4))\n",
    "        x[0] = forecasts\n",
    "        yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(boxes_pred, boxes_true):\n",
    "\n",
    "    x_min = K.stack([boxes_pred[:, 0], boxes_true[:, 0]], axis=-1)\n",
    "    y_min = K.stack([boxes_pred[:, 1], boxes_true[:, 1]], axis=-1)\n",
    "    x_max = K.stack([boxes_pred[:, 2], boxes_true[:, 2]], axis=-1)\n",
    "    y_max = K.stack([boxes_pred[:, 3], boxes_true[:, 3]], axis=-1)\n",
    "\n",
    "    x_min = K.max(x_min, axis=-1)\n",
    "    y_min = K.max(y_min, axis=-1)\n",
    "    x_max = K.min(x_max, axis=-1)\n",
    "    y_max = K.min(y_max, axis=-1)\n",
    "\n",
    "    zeros = K.zeros_like(x_max)\n",
    "\n",
    "    x_inter = K.stack([zeros, x_max - x_min], axis=-1)\n",
    "    y_inter = K.stack([zeros, y_max - y_min], axis=-1)\n",
    "\n",
    "    x_inter = K.max(x_inter, axis=-1)\n",
    "    y_inter = K.max(y_inter, axis=-1)\n",
    "    inter_area = x_inter * y_inter\n",
    "    \n",
    "    area_pred = (K.max(K.stack([zeros, boxes_pred[:, 2] - boxes_pred[:, 0]], axis=-1), axis=-1) * \n",
    "                 K.max(K.stack([zeros, boxes_pred[:, 3] - boxes_pred[:, 1]], axis=-1), axis=-1))\n",
    "    area_true = (K.max(K.stack([zeros, boxes_true[:, 2] - boxes_true[:, 0]], axis=-1), axis=-1) * \n",
    "                 K.max(K.stack([zeros, boxes_true[:, 3] - boxes_true[:, 1]], axis=-1), axis=-1))\n",
    "\n",
    "    iou = inter_area / (area_pred + area_true - inter_area + K.epsilon())\n",
    "    \n",
    "    return -K.mean(iou, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(filters):\n",
    "    K.clear_session()\n",
    "    \n",
    "    y = x = layers.Input(shape=(None, 4))\n",
    "    \n",
    "    mul = 4\n",
    "    y = layers.Bidirectional(layers.LSTM(\n",
    "        units=filters * mul,\n",
    "        return_sequences=True\n",
    "    ))(y)\n",
    "    y = layers.Bidirectional(layers.LSTM(\n",
    "        units=filters * mul,\n",
    "        return_sequences=False\n",
    "    ))(y)\n",
    "    \n",
    "    y = layers.Dense(\n",
    "        units=filters * 4,\n",
    "        activation=\"relu\"\n",
    "    )(y)\n",
    "    y = layers.Dense(\n",
    "        units=filters * 2,\n",
    "        activation=\"relu\"\n",
    "    )(y)\n",
    "    y = layers.Dense(\n",
    "        units=filters,\n",
    "        activation=\"relu\"\n",
    "    )(y)\n",
    "    y = layers.Dense(\n",
    "        units=4,\n",
    "        activation=None\n",
    "    )(y)\n",
    "    \n",
    "    model = models.Model(inputs=x, outputs=y)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_train, data_val, units, epochs=100):\n",
    "    lr = 0.001\n",
    "    \n",
    "    rez = None \n",
    "    \n",
    "    while rez is None or min(rez.history[\"val_loss\"]) > -0.53:\n",
    "        model = make_model(units)\n",
    "        model.compile(optimizer=optimizers.Nadam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004),\n",
    "                      loss=intersection_over_union,\n",
    "                      metrics=None\n",
    "        )\n",
    "        cb = [\n",
    "            callbacks.ModelCheckpoint(\"../processed/model.h5\", monitor=\"val_loss\", verbose=1, save_best_only=True),\n",
    "            callbacks.EarlyStopping(monitor='val_loss', patience=epochs // 10, min_delta=0.0001, verbose=10),\n",
    "            callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
    "        ]\n",
    "        rez = model.fit_generator(\n",
    "            yield_batch(data_train),\n",
    "            steps_per_epoch=1000,\n",
    "            epochs=epochs,\n",
    "            callbacks=cb,\n",
    "            validation_data=yield_batch_val(data_val),\n",
    "            validation_steps=len(data_val[1].index)\n",
    "        )\n",
    "    \n",
    "    model = models.load_model(\"../processed/model.h5\", custom_objects={\"intersection_over_union\": intersection_over_union})\n",
    "    \n",
    "    return rez, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDS = 5\n",
    "\n",
    "def train_oof(train_set, test_set, units=16):\n",
    "    x_train, y_train = train_set\n",
    "    x_test = test_set\n",
    "\n",
    "    y_oof = pd.DataFrame(0, index=y_train.index, columns=COORDINATES)\n",
    "    y_pred = pd.DataFrame(0, index=x_test.index.unique(), columns=COORDINATES)\n",
    "    scores = []\n",
    "    folds = model_selection.KFold(n_splits=FOLDS, shuffle=True)\n",
    "    \n",
    "    for n, (index_train, index_valid) in enumerate(folds.split(y_train)):\n",
    "        print(f\"/nFold - {n}\")\n",
    "        \n",
    "        ids_train = y_train.index[index_train]\n",
    "        ids_valid = y_train.index[index_valid]\n",
    "        \n",
    "        data_train = (x_train.loc[ids_train], y_train.loc[ids_train])\n",
    "        data_val = (x_train.loc[ids_valid], y_train.loc[ids_valid])\n",
    "        \n",
    "        rez, model = train_model(data_train, data_val, units)\n",
    "        \n",
    "        pd.DataFrame(rez.history)[[\"loss\", \"val_loss\"]].plot(figsize=(16, 8))\n",
    "        scores.append(min(rez.history[\"val_loss\"]))\n",
    "        \n",
    "        feat = yield_batch_test(data_val[0])\n",
    "        df = model.predict_generator(feat, steps=len(data_val[0].index.unique()))\n",
    "        df = scaler.inverse_transform(df)\n",
    "        y_oof.loc[ids_valid] = df\n",
    "        \n",
    "        feat = yield_batch_test(x_test)\n",
    "        df = model.predict_generator(feat, steps=len(x_test.index.unique()))\n",
    "        df = scaler.inverse_transform(df)\n",
    "        y_pred += df / FOLDS\n",
    "\n",
    "    print(f\"IOU на кроссвалидации: \" + str(-np.round(sorted(scores), 5)))\n",
    "    print(f\"IOU среднее: {-np.mean(scores):0.5f} +/- {np.std(scores):0.5f}\")\n",
    "\n",
    "    subdir = time.strftime('%Y-%m-%d_%H-%M')\n",
    "    path = pathlib.Path(f\"../processed/{subdir}\")\n",
    "    path.mkdir(exist_ok=True)\n",
    "    y_oof.to_csv(path / f\"oof-{-np.mean(scores):0.5f}.csv\", header=False)\n",
    "    y_pred.to_csv(path / f\"sub-{-np.mean(scores):0.5f}.csv\", header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold - 0\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, 4)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 128)         35328     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 145,076\n",
      "Trainable params: 145,076\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -7.6293e-08 - val_loss: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00000, saving model to ../processed/model.h5\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00000\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00000\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00000\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00000\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00000\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00000\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00000\n",
      "Epoch 00011: early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, 4)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 128)         35328     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 145,076\n",
      "Trainable params: 145,076\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 16s 16ms/step - loss: -0.2482 - val_loss: -0.4188\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to -0.41878, saving model to ../processed/model.h5\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: -0.4137 - val_loss: -0.4297\n",
      "\n",
      "Epoch 00002: val_loss improved from -0.41878 to -0.42972, saving model to ../processed/model.h5\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: -0.4443 - val_loss: -0.4246\n",
      "\n",
      "Epoch 00003: val_loss did not improve from -0.42972\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: -0.4535 - val_loss: -0.4413\n",
      "\n",
      "Epoch 00004: val_loss improved from -0.42972 to -0.44126, saving model to ../processed/model.h5\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: -0.4642 - val_loss: -0.4701\n",
      "\n",
      "Epoch 00005: val_loss improved from -0.44126 to -0.47013, saving model to ../processed/model.h5\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: -0.4655 - val_loss: -0.4989\n",
      "\n",
      "Epoch 00006: val_loss improved from -0.47013 to -0.49890, saving model to ../processed/model.h5\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.4776 - val_loss: -0.4991\n",
      "\n",
      "Epoch 00007: val_loss improved from -0.49890 to -0.49909, saving model to ../processed/model.h5\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: -0.4569 - val_loss: -0.4849\n",
      "\n",
      "Epoch 00008: val_loss did not improve from -0.49909\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.4695 - val_loss: -0.4720\n",
      "\n",
      "Epoch 00009: val_loss did not improve from -0.49909\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.4812 - val_loss: -0.4853\n",
      "\n",
      "Epoch 00010: val_loss did not improve from -0.49909\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.5298 - val_loss: -0.4982\n",
      "\n",
      "Epoch 00011: val_loss did not improve from -0.49909\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.5197 - val_loss: -0.5042\n",
      "\n",
      "Epoch 00012: val_loss improved from -0.49909 to -0.50420, saving model to ../processed/model.h5\n",
      "Epoch 13/100\n",
      " 662/1000 [==================>...........] - ETA: 4s - loss: -0.5181"
     ]
    }
   ],
   "source": [
    "train_oof((votes, answers), votes_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
