{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "from keras import backend as K\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COORDINATES = [\"Xmin\", \"Ymin\", \"Xmax\", \"Ymax\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WLMike/Documents/PycharmProjects/ML_venv/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/WLMike/Documents/PycharmProjects/ML_venv/lib/python3.6/site-packages/ipykernel_launcher.py:4: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "answers = pd.read_csv(\"../raw/train_answers.csv\").set_index(\"itemId\")\n",
    "answers.columns = COORDINATES\n",
    "scaler = preprocessing.StandardScaler().fit(np.vstack([answers.values, answers.values[:, [2, 3, 0, 1]]]))\n",
    "answers[COORDINATES] = scaler.transform(answers[COORDINATES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WLMike/Documents/PycharmProjects/ML_venv/lib/python3.6/site-packages/ipykernel_launcher.py:2: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "votes = pd.read_csv(\"../raw/train_data.csv\").set_index(\"itemId\")\n",
    "votes[COORDINATES] = scaler.transform(votes[COORDINATES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WLMike/Documents/PycharmProjects/ML_venv/lib/python3.6/site-packages/ipykernel_launcher.py:2: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "votes_test = pd.read_csv(\"../raw/test_data.csv\").set_index(\"itemId\")\n",
    "votes_test[COORDINATES] = scaler.transform(votes_test[COORDINATES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_center(y):\n",
    "    center_x = (y[0, 2] + y[0, 0]) / 2\n",
    "    center_y = (y[0, 3] + y[0, 1]) / 2 \n",
    "    return np.array([center_x, center_y, center_x, center_y])\n",
    "\n",
    "\n",
    "def scale(x, y, err=0.1):\n",
    "    center = find_center(y)\n",
    "    x, y = x - np.reshape(center, (1, 1, 4)), y - np.reshape(center, (1, 4))\n",
    "    \n",
    "    err_x, err_y = np.random.random(2)  \n",
    "    \n",
    "    err_x = 1 + (2 * err_x - 1) * err\n",
    "    err_y = 1 + (2 * err_y - 1) * err\n",
    "    \n",
    "    err = np.array([err_x, err_y, err_x, err_y])\n",
    "    \n",
    "    return (x * np.reshape(err, (1, 1, 4)) + np.reshape(center, (1, 1, 4)), \n",
    "            y * np.reshape(err, (1, 4)) + np.reshape(center, (1, 4)))\n",
    "    \n",
    "\n",
    "def move(x, y, err=0.1):\n",
    "    center = find_center(y)\n",
    "    x, y = x - np.reshape(center, (1, 1, 4)), y - np.reshape(center, (1, 4))\n",
    "    \n",
    "    size_x = (y[0, 2] - y[0, 0]) / 2\n",
    "    size_y = (y[0, 3] - y[0, 1]) / 2\n",
    "    \n",
    "    err_cx, err_cy = np.random.random(2)  \n",
    "    \n",
    "    err_cx = (2 * size_x * err_cx - size_x) * err\n",
    "    err_cy = (2 * size_y * err_cy - size_y) * err\n",
    "    \n",
    "    err = np.array([err_cx, err_cy, err_cx, err_cy])\n",
    "    \n",
    "    return x + np.reshape(err + center, (1, 1, 4)), y + np.reshape(err + center, (1, 4))\n",
    "    \n",
    "def aug_batch(x, y, err=0.1):\n",
    "    x, y = scale(x, y, err)\n",
    "    x, y = move(x, y, err)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_batch(data):\n",
    "    votes, answers = data\n",
    "    while True:\n",
    "        item_id = np.random.choice(votes.index, 1)\n",
    "        forecasts = votes.loc[item_id].set_index(\"userId\")\n",
    "        x = np.zeros((1, len(forecasts), 4),)\n",
    "        y = np.zeros((1, 4))\n",
    "        \n",
    "        x[0] = forecasts.sample(len(forecasts))\n",
    "        y[0] = answers.loc[item_id]\n",
    "        yield aug_batch(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_batch_val(data):\n",
    "    votes, answers = data\n",
    "    item_ids = set(votes.index)\n",
    "    while True:\n",
    "        for item_id in item_ids:\n",
    "            forecasts = votes.loc[item_id].set_index(\"userId\")\n",
    "            x = np.zeros((1, len(forecasts), 4),)\n",
    "            y = np.zeros((1, 4))\n",
    "\n",
    "            x[0] = forecasts\n",
    "            y[0] = answers.loc[item_id]\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_batch_test(data):\n",
    "    item_ids = data.index.unique()\n",
    "    for item_id in item_ids:\n",
    "        forecasts = data.loc[item_id].set_index(\"userId\")\n",
    "        x = np.zeros((1, len(forecasts), 4),)\n",
    "        y = np.zeros((1, 4))\n",
    "        x[0] = forecasts\n",
    "        yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(boxes_pred, boxes_true):\n",
    "\n",
    "    x_min = K.stack([boxes_pred[:, 0], boxes_true[:, 0]], axis=-1)\n",
    "    y_min = K.stack([boxes_pred[:, 1], boxes_true[:, 1]], axis=-1)\n",
    "    x_max = K.stack([boxes_pred[:, 2], boxes_true[:, 2]], axis=-1)\n",
    "    y_max = K.stack([boxes_pred[:, 3], boxes_true[:, 3]], axis=-1)\n",
    "\n",
    "    x_min = K.max(x_min, axis=-1)\n",
    "    y_min = K.max(y_min, axis=-1)\n",
    "    x_max = K.min(x_max, axis=-1)\n",
    "    y_max = K.min(y_max, axis=-1)\n",
    "\n",
    "    zeros = K.zeros_like(x_max)\n",
    "\n",
    "    x_inter = K.stack([zeros, x_max - x_min], axis=-1)\n",
    "    y_inter = K.stack([zeros, y_max - y_min], axis=-1)\n",
    "\n",
    "    x_inter = K.max(x_inter, axis=-1)\n",
    "    y_inter = K.max(y_inter, axis=-1)\n",
    "    inter_area = x_inter * y_inter\n",
    "    \n",
    "    area_pred = (K.max(K.stack([zeros, boxes_pred[:, 2] - boxes_pred[:, 0]], axis=-1), axis=-1) * \n",
    "                 K.max(K.stack([zeros, boxes_pred[:, 3] - boxes_pred[:, 1]], axis=-1), axis=-1))\n",
    "    area_true = (K.max(K.stack([zeros, boxes_true[:, 2] - boxes_true[:, 0]], axis=-1), axis=-1) * \n",
    "                 K.max(K.stack([zeros, boxes_true[:, 3] - boxes_true[:, 1]], axis=-1), axis=-1))\n",
    "\n",
    "    iou = inter_area / (area_pred + area_true - inter_area + K.epsilon())\n",
    "    \n",
    "    return -K.mean(iou, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(filters):\n",
    "    K.clear_session()\n",
    "    \n",
    "    y = x = layers.Input(shape=(None, 4))\n",
    "    \n",
    "    mul = 4\n",
    "    y = layers.Bidirectional(layers.LSTM(\n",
    "        units=filters * mul,\n",
    "        return_sequences=True\n",
    "    ))(y)\n",
    "    y = layers.Bidirectional(layers.LSTM(\n",
    "        units=filters * mul,\n",
    "        return_sequences=False\n",
    "    ))(y)\n",
    "    \n",
    "    y = layers.Dense(\n",
    "        units=filters * 4,\n",
    "        activation=\"relu\"\n",
    "    )(y)\n",
    "    y = layers.Dense(\n",
    "        units=filters * 2,\n",
    "        activation=\"relu\"\n",
    "    )(y)\n",
    "    y = layers.Dense(\n",
    "        units=filters,\n",
    "        activation=\"relu\"\n",
    "    )(y)\n",
    "    y = layers.Dense(\n",
    "        units=4,\n",
    "        activation=None\n",
    "    )(y)\n",
    "    \n",
    "    model = models.Model(inputs=x, outputs=y)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicLR(callbacks.Callback):\n",
    "\n",
    "    def __init__(self, max_lr):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_lr = max_lr\n",
    "        \n",
    "        self.steps = None\n",
    "        self.epochs = None\n",
    "        \n",
    "        self.iterations = 0\n",
    "        self.history = {}\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.iterations = 1\n",
    "        self.history = {}\n",
    "     \n",
    "        self.steps = self.params[\"steps\"]\n",
    "        self.epochs = self.params[\"epochs\"]\n",
    "        \n",
    "        lr = 2 / self.steps / self.epochs\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = 2 * self.iterations / self.steps / self.epochs\n",
    "        \n",
    "        if cycle <= 1:\n",
    "            return self.max_lr * cycle\n",
    "        \n",
    "        return self.max_lr - self.max_lr * (cycle - 1)\n",
    "            \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.history.setdefault(\"iterations\", []).append(self.iterations)\n",
    "        self.history.setdefault(\"lr\", []).append(K.get_value(self.model.optimizer.lr))\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "            \n",
    "        self.iterations += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"\\nLearning rate: {K.get_value(self.model.optimizer.lr):.1e}\")\n",
    "        \n",
    "    def plot(self, smooth=None):\n",
    "        smooth = int(smooth or self.steps)\n",
    "        df = pd.DataFrame(self.history).set_index(\"iterations\")[[\"loss\", \"lr\"]].rolling(smooth).mean()\n",
    "        df.plot(figsize=(16, 8), secondary_y=\"lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecayingCyclicLR(callbacks.Callback):\n",
    "\n",
    "    def __init__(self, max_lr, half=4, decay=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_lr = max_lr\n",
    "        self.half = half\n",
    "        self.decay = decay\n",
    "        \n",
    "        self.steps = None\n",
    "        self.epochs = None\n",
    "        \n",
    "        self.iterations = 0\n",
    "        self.history = {}\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.iterations = 1\n",
    "        self.history = {}\n",
    "     \n",
    "        self.steps = self.params[\"steps\"]\n",
    "        self.epochs = self.params[\"epochs\"]\n",
    "        \n",
    "        lr = 1 / self.steps / self.half\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        \n",
    "    def clr(self):\n",
    "        num, cycle = divmod(self.iterations / (self.steps * self.half), 2)\n",
    "        \n",
    "        if cycle <= 1:\n",
    "            return self.max_lr * cycle * self.decay ** num\n",
    "        \n",
    "        return (self.max_lr - self.max_lr * (cycle - 1)) * self.decay ** num\n",
    "            \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.history.setdefault(\"iterations\", []).append(self.iterations)\n",
    "        self.history.setdefault(\"lr\", []).append(K.get_value(self.model.optimizer.lr))\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "            \n",
    "        self.iterations += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"\\nLearning rate: {K.get_value(self.model.optimizer.lr):.1e}\")\n",
    "        \n",
    "    def plot(self, smooth=None):\n",
    "        smooth = int(smooth or self.steps)\n",
    "        df = pd.DataFrame(self.history).set_index(\"iterations\")[[\"loss\", \"lr\"]].rolling(smooth).mean()\n",
    "        df.plot(figsize=(16, 8), secondary_y=\"lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxLRTest(callbacks.Callback):\n",
    "\n",
    "    def __init__(self, base_lr=1.0e-9, max_lr=10, steps=10000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.steps = steps\n",
    "        \n",
    "        self.iterations = None\n",
    "        self.history = None\n",
    "        \n",
    "    def clr(self):\n",
    "        return self.base_lr * (self.max_lr / self.base_lr) ** (self.iterations / self.steps) \n",
    "  \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.iterations = 0\n",
    "        self.history = {}\n",
    "        K.set_value(self.model.optimizer.lr, self.base_lr)     \n",
    "            \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if self.iterations > self.steps:\n",
    "            self.model.stop_training = True\n",
    "        \n",
    "        self.history.setdefault(\"iterations\", []).append(self.iterations)\n",
    "        self.history.setdefault(\"lr\", []).append(K.get_value(self.model.optimizer.lr))\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "            \n",
    "        self.iterations += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "        \n",
    "    def plot(self, smooth=None, t_score=2):\n",
    "        smooth = int(smooth or self.steps // 10)\n",
    "        df = pd.DataFrame(self.history).set_index(\"lr\").loss.rolling(smooth).agg([\"mean\", \"std\"])\n",
    "        df[\"std\"] = df[\"mean\"] + t_score * df[\"std\"] / smooth ** 0.5\n",
    "        print(f\"Max speed learning rate  - {df['std'].idxmin():.1e}\")\n",
    "        df.plot(logx=True, figsize=(16, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecayingLR(callbacks.Callback):\n",
    "\n",
    "    def __init__(self, max_lr=0.1, warm_up=1, decay_per_epoch=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_lr = max_lr\n",
    "        self.warm_up = warm_up\n",
    "        self.decay_per_epoch = decay_per_epoch\n",
    "        \n",
    "        self.iterations = None\n",
    "        self.history = None\n",
    "        \n",
    "        self.d_loss_iterations = None\n",
    "        self.d_loss = None\n",
    "        self.d_iterations = None\n",
    "        self.d_iterations2 = None\n",
    "        self.d_1 = None\n",
    "                \n",
    "        self.steps = None\n",
    "        self.decay = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.iterations = 1\n",
    "        self.history = {}\n",
    "        \n",
    "        self.d_loss_iterations = 0\n",
    "        self.d_loss = 0\n",
    "        self.d_iterations = 0\n",
    "        self.d_iterations2 = 0\n",
    "        self.d_1 = 0\n",
    "        \n",
    "        self.steps = self.params[\"steps\"]\n",
    "        self.decay = (1 - 1 / self.steps) ** (1 / self.decay_per_epoch)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.lr())\n",
    "    \n",
    "    def make_decay(self, old, new):\n",
    "        decay = self.decay\n",
    "        return old * decay + new * (1 - decay)\n",
    "    \n",
    "    def lr(self):\n",
    "        warm_up = self.iterations / self.steps * self.warm_up\n",
    "        if warm_up <= 1:\n",
    "            return self.max_lr * warm_up \n",
    "        \n",
    "        lr = self.history[\"lr\"][-1]\n",
    "        speed = self.history[\"speed\"][-1]\n",
    "        \n",
    "        if speed > 0:\n",
    "            return lr * self.decay\n",
    "        \n",
    "        return lr\n",
    "            \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.history.setdefault(\"iterations\", []).append(self.iterations)\n",
    "        self.history.setdefault(\"lr\", []).append(K.get_value(self.model.optimizer.lr))\n",
    "        \n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "            \n",
    "        self.d_loss_iterations = self.make_decay(self.d_loss_iterations, logs[\"loss\"] * self.iterations)\n",
    "        self.d_loss = self.make_decay(self.d_loss, logs[\"loss\"])\n",
    "        self.d_iterations = self.make_decay(self.d_iterations, self.iterations)\n",
    "        self.d_iterations2 = self.make_decay(self.d_iterations2, self.iterations ** 2)\n",
    "        self.d_1 = self.make_decay(self.d_1, 1)\n",
    "        \n",
    "        cov = self.d_loss_iterations - self.d_loss * self.d_iterations / self.d_1\n",
    "        var = self.d_iterations2 - self.d_iterations ** 2 / self.d_1\n",
    "        \n",
    "        speed = self.steps * cov / var\n",
    "        self.history.setdefault(\"speed\", []).append(speed)\n",
    "        \n",
    "        self.iterations += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.lr())     \n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = self.history[\"lr\"][-1]\n",
    "        speed = self.history[\"speed\"][-1]\n",
    "        print(f\"\\nLearning rate: {lr:.1e}\")\n",
    "        print(f\"Speed per epoch: {speed:.4f}\")\n",
    "        \n",
    "    def plot(self, smooth=None):\n",
    "        smooth = int(smooth or self.steps)\n",
    "        df = pd.DataFrame(self.history).set_index(\"iterations\")[[\"loss\", \"lr\"]].rolling(smooth).mean()\n",
    "        df.plot(figsize=(16, 8), secondary_y=\"lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_train, data_val, units, test=False):\n",
    "    \n",
    "    max_lr = 1.7e-03\n",
    "    \n",
    "    steps = 40000\n",
    "    steps_per_epoch = 1000\n",
    "    epochs = steps // steps_per_epoch\n",
    "    \n",
    "    model = make_model(units)  \n",
    "    \n",
    "    model.compile(optimizer=optimizers.Nadam(lr=max_lr / 100, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004),\n",
    "                      loss=\"mae\",\n",
    "                      metrics=[intersection_over_union]\n",
    "        )\n",
    "    rez = model.fit_generator(\n",
    "            yield_batch(data_train),\n",
    "            steps_per_epoch=1000,\n",
    "            epochs=1,\n",
    "            callbacks=None,\n",
    "            validation_data=yield_batch_val(data_val),\n",
    "            validation_steps=len(data_val[1].index)\n",
    "        )\n",
    "    \n",
    "    \n",
    "    model.compile(optimizer=optimizers.Nadam(lr=max_lr, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004),\n",
    "                  loss=intersection_over_union,\n",
    "                  metrics=None\n",
    "    )\n",
    "    cycle = MaxLRTest() if test else DecayingCyclicLR(max_lr=max_lr)\n",
    "    cb = [\n",
    "        callbacks.ModelCheckpoint(\"../processed/model.h5\", monitor=\"val_loss\", verbose=1, save_best_only=True),\n",
    "        cycle\n",
    "    ]\n",
    "    rez = model.fit_generator(\n",
    "        yield_batch(data_train),\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=epochs,\n",
    "        callbacks=cb,\n",
    "        validation_data=yield_batch_val(data_val),\n",
    "        validation_steps=len(data_val[1].index)\n",
    "        )\n",
    "    \n",
    "    model = models.load_model(\"../processed/model.h5\", custom_objects={\"intersection_over_union\": intersection_over_union})\n",
    "    \n",
    "    return rez, cycle, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDS = 10\n",
    "\n",
    "def train_oof(train_set, test_set, units=16):\n",
    "    x_train, y_train = train_set\n",
    "    x_test = test_set\n",
    "\n",
    "    y_oof = pd.DataFrame(0, index=y_train.index, columns=COORDINATES)\n",
    "    y_pred = pd.DataFrame(0, index=x_test.index.unique(), columns=COORDINATES)\n",
    "    scores = []\n",
    "    folds = model_selection.KFold(n_splits=FOLDS, shuffle=True)\n",
    "    \n",
    "    for n, (index_train, index_valid) in enumerate(folds.split(y_train), 1):\n",
    "        print(f\"\\nFold - {n} / {FOLDS}\")\n",
    "        \n",
    "        ids_train = y_train.index[index_train]\n",
    "        ids_valid = y_train.index[index_valid]\n",
    "        \n",
    "        data_train = (x_train.loc[ids_train], y_train.loc[ids_train])\n",
    "        data_val = (x_train.loc[ids_valid], y_train.loc[ids_valid])\n",
    "        \n",
    "        rez, cycle, model = train_model(data_train, data_val, units)\n",
    "        \n",
    "        cycle.plot()\n",
    "        pd.DataFrame(rez.history)[[\"loss\", \"val_loss\"]].plot(figsize=(16, 8))\n",
    "        scores.append(min(rez.history[\"val_loss\"]))\n",
    "        \n",
    "        feat = yield_batch_test(data_val[0])\n",
    "        df = model.predict_generator(feat, steps=len(data_val[0].index.unique()))\n",
    "        df = scaler.inverse_transform(df)\n",
    "        y_oof.loc[ids_valid] = df\n",
    "        \n",
    "        feat = yield_batch_test(x_test)\n",
    "        df = model.predict_generator(feat, steps=len(x_test.index.unique()))\n",
    "        df = scaler.inverse_transform(df)\n",
    "        y_pred += df / FOLDS\n",
    "\n",
    "    print(f\"IOU на кроссвалидации: \" + str(-np.round(sorted(scores), 5)))\n",
    "    print(f\"IOU среднее: {-np.mean(scores):0.5f} +/- {np.std(scores):0.5f}\")\n",
    "\n",
    "    subdir = time.strftime('%Y-%m-%d_%H-%M')\n",
    "    path = pathlib.Path(f\"../processed/{subdir}\")\n",
    "    path.mkdir(exist_ok=True)\n",
    "    y_oof.to_csv(path / f\"oof-{-np.mean(scores):0.5f}.csv\", header=False)\n",
    "    y_pred.to_csv(path / f\"sub-{-np.mean(scores):0.5f}.csv\", header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold - 1 / 10\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, 4)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 128)         35328     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 145,076\n",
      "Trainable params: 145,076\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 16s 16ms/step - loss: 0.6942 - intersection_over_union: -0.0521 - val_loss: 0.5927 - val_intersection_over_union: -0.1144\n",
      "Epoch 1/40\n",
      "1000/1000 [==============================] - 16s 16ms/step - loss: -0.2694 - val_loss: -0.3264\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to -0.32644, saving model to ../processed/model.h5\n",
      "\n",
      "Learning rate: 4.3e-04\n",
      "Epoch 2/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.4025 - val_loss: -0.4134\n",
      "\n",
      "Epoch 00002: val_loss improved from -0.32644 to -0.41335, saving model to ../processed/model.h5\n",
      "\n",
      "Learning rate: 8.5e-04\n",
      "Epoch 3/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.4149 - val_loss: -0.4370\n",
      "\n",
      "Epoch 00003: val_loss improved from -0.41335 to -0.43702, saving model to ../processed/model.h5\n",
      "\n",
      "Learning rate: 1.3e-03\n",
      "Epoch 4/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.4158 - val_loss: -0.3962\n",
      "\n",
      "Epoch 00004: val_loss did not improve from -0.43702\n",
      "\n",
      "Learning rate: 1.7e-03\n",
      "Epoch 5/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.4154 - val_loss: -0.4317\n",
      "\n",
      "Epoch 00005: val_loss did not improve from -0.43702\n",
      "\n",
      "Learning rate: 1.3e-03\n",
      "Epoch 6/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.4554 - val_loss: -0.4622\n",
      "\n",
      "Epoch 00006: val_loss improved from -0.43702 to -0.46220, saving model to ../processed/model.h5\n",
      "\n",
      "Learning rate: 8.5e-04\n",
      "Epoch 7/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.4801 - val_loss: -0.4993\n",
      "\n",
      "Epoch 00007: val_loss improved from -0.46220 to -0.49927, saving model to ../processed/model.h5\n",
      "\n",
      "Learning rate: 4.2e-04\n",
      "Epoch 8/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.5115 - val_loss: -0.5348\n",
      "\n",
      "Epoch 00008: val_loss improved from -0.49927 to -0.53475, saving model to ../processed/model.h5\n",
      "\n",
      "Learning rate: 2.1e-07\n",
      "Epoch 9/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.5240 - val_loss: -0.5186\n",
      "\n",
      "Epoch 00009: val_loss did not improve from -0.53475\n",
      "\n",
      "Learning rate: 2.1e-04\n",
      "Epoch 10/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.5175 - val_loss: -0.5126\n",
      "\n",
      "Epoch 00010: val_loss did not improve from -0.53475\n",
      "\n",
      "Learning rate: 4.3e-04\n",
      "Epoch 11/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.5069 - val_loss: -0.4741\n",
      "\n",
      "Epoch 00011: val_loss did not improve from -0.53475\n",
      "\n",
      "Learning rate: 6.4e-04\n",
      "Epoch 12/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.4908 - val_loss: -0.4585\n",
      "\n",
      "Epoch 00012: val_loss did not improve from -0.53475\n",
      "\n",
      "Learning rate: 8.5e-04\n",
      "Epoch 13/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.4917 - val_loss: -0.4939\n",
      "\n",
      "Epoch 00013: val_loss did not improve from -0.53475\n",
      "\n",
      "Learning rate: 6.4e-04\n",
      "Epoch 14/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.5012 - val_loss: -0.4835\n",
      "\n",
      "Epoch 00014: val_loss did not improve from -0.53475\n",
      "\n",
      "Learning rate: 4.2e-04\n",
      "Epoch 15/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.5175 - val_loss: -0.5232\n",
      "\n",
      "Epoch 00015: val_loss did not improve from -0.53475\n",
      "\n",
      "Learning rate: 2.1e-04\n",
      "Epoch 16/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5471 - val_loss: -0.5417\n",
      "\n",
      "Epoch 00016: val_loss improved from -0.53475 to -0.54171, saving model to ../processed/model.h5\n",
      "\n",
      "Learning rate: 1.1e-07\n",
      "Epoch 17/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.5612 - val_loss: -0.5359\n",
      "\n",
      "Epoch 00017: val_loss did not improve from -0.54171\n",
      "\n",
      "Learning rate: 1.1e-04\n",
      "Epoch 18/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5421 - val_loss: -0.5278\n",
      "\n",
      "Epoch 00018: val_loss did not improve from -0.54171\n",
      "\n",
      "Learning rate: 2.1e-04\n",
      "Epoch 19/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5424 - val_loss: -0.5276\n",
      "\n",
      "Epoch 00019: val_loss did not improve from -0.54171\n",
      "\n",
      "Learning rate: 3.2e-04\n",
      "Epoch 20/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.5337 - val_loss: -0.5001\n",
      "\n",
      "Epoch 00020: val_loss did not improve from -0.54171\n",
      "\n",
      "Learning rate: 4.2e-04\n",
      "Epoch 21/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.5271 - val_loss: -0.5173\n",
      "\n",
      "Epoch 00021: val_loss did not improve from -0.54171\n",
      "\n",
      "Learning rate: 3.2e-04\n",
      "Epoch 22/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.5396 - val_loss: -0.5058\n",
      "\n",
      "Epoch 00022: val_loss did not improve from -0.54171\n",
      "\n",
      "Learning rate: 2.1e-04\n",
      "Epoch 23/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5479 - val_loss: -0.5478\n",
      "\n",
      "Epoch 00023: val_loss improved from -0.54171 to -0.54777, saving model to ../processed/model.h5\n",
      "\n",
      "Learning rate: 1.1e-04\n",
      "Epoch 24/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5663 - val_loss: -0.5543\n",
      "\n",
      "Epoch 00024: val_loss improved from -0.54777 to -0.55434, saving model to ../processed/model.h5\n",
      "\n",
      "Learning rate: 5.3e-08\n",
      "Epoch 25/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5703 - val_loss: -0.5578\n",
      "\n",
      "Epoch 00025: val_loss improved from -0.55434 to -0.55780, saving model to ../processed/model.h5\n",
      "\n",
      "Learning rate: 5.3e-05\n",
      "Epoch 26/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5637 - val_loss: -0.5403\n",
      "\n",
      "Epoch 00026: val_loss did not improve from -0.55780\n",
      "\n",
      "Learning rate: 1.1e-04\n",
      "Epoch 27/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.5617 - val_loss: -0.5418\n",
      "\n",
      "Epoch 00027: val_loss did not improve from -0.55780\n",
      "\n",
      "Learning rate: 1.6e-04\n",
      "Epoch 28/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.5639 - val_loss: -0.5429\n",
      "\n",
      "Epoch 00028: val_loss did not improve from -0.55780\n",
      "\n",
      "Learning rate: 2.1e-04\n",
      "Epoch 29/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.5521 - val_loss: -0.5305\n",
      "\n",
      "Epoch 00029: val_loss did not improve from -0.55780\n",
      "\n",
      "Learning rate: 1.6e-04\n",
      "Epoch 30/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5603 - val_loss: -0.5445\n",
      "\n",
      "Epoch 00030: val_loss did not improve from -0.55780\n",
      "\n",
      "Learning rate: 1.1e-04\n",
      "Epoch 31/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.5688 - val_loss: -0.5498\n",
      "\n",
      "Epoch 00031: val_loss did not improve from -0.55780\n",
      "\n",
      "Learning rate: 5.3e-05\n",
      "Epoch 32/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.5770 - val_loss: -0.5545\n",
      "\n",
      "Epoch 00032: val_loss did not improve from -0.55780\n",
      "\n",
      "Learning rate: 2.7e-08\n",
      "Epoch 33/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5714 - val_loss: -0.5534\n",
      "\n",
      "Epoch 00033: val_loss did not improve from -0.55780\n",
      "\n",
      "Learning rate: 2.7e-05\n",
      "Epoch 34/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.5833 - val_loss: -0.5569\n",
      "\n",
      "Epoch 00034: val_loss did not improve from -0.55780\n",
      "\n",
      "Learning rate: 5.3e-05\n",
      "Epoch 35/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.5806 - val_loss: -0.5551\n",
      "\n",
      "Epoch 00035: val_loss did not improve from -0.55780\n",
      "\n",
      "Learning rate: 8.0e-05\n",
      "Epoch 36/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.5737 - val_loss: -0.5396\n",
      "\n",
      "Epoch 00036: val_loss did not improve from -0.55780\n",
      "\n",
      "Learning rate: 1.1e-04\n",
      "Epoch 37/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.5738 - val_loss: -0.5367\n",
      "\n",
      "Epoch 00037: val_loss did not improve from -0.55780\n",
      "\n",
      "Learning rate: 8.0e-05\n",
      "Epoch 38/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.5748 - val_loss: -0.5499\n",
      "\n",
      "Epoch 00038: val_loss did not improve from -0.55780\n",
      "\n",
      "Learning rate: 5.3e-05\n",
      "Epoch 39/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5814 - val_loss: -0.5558\n",
      "\n",
      "Epoch 00039: val_loss did not improve from -0.55780\n",
      "\n",
      "Learning rate: 2.7e-05\n",
      "Epoch 40/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.5759 - val_loss: -0.5553\n",
      "\n",
      "Epoch 00040: val_loss did not improve from -0.55780\n",
      "\n",
      "Learning rate: 1.3e-08\n",
      "\n",
      "Fold - 2 / 10\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, 4)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 128)         35328     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 145,076\n",
      "Trainable params: 145,076\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 17s 17ms/step - loss: 0.7638 - intersection_over_union: -0.0026 - val_loss: 0.6824 - val_intersection_over_union: -0.0133\n",
      "Epoch 1/40\n",
      "1000/1000 [==============================] - 18s 18ms/step - loss: -0.2186 - val_loss: -0.3913\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to -0.39126, saving model to ../processed/model.h5\n",
      "\n",
      "Learning rate: 4.3e-04\n",
      "Epoch 2/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.4129 - val_loss: -0.4317\n",
      "\n",
      "Epoch 00002: val_loss improved from -0.39126 to -0.43170, saving model to ../processed/model.h5\n",
      "\n",
      "Learning rate: 8.5e-04\n",
      "Epoch 3/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.4121 - val_loss: -0.3942\n",
      "\n",
      "Epoch 00003: val_loss did not improve from -0.43170\n",
      "\n",
      "Learning rate: 1.3e-03\n",
      "Epoch 4/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.4258 - val_loss: -0.4219\n",
      "\n",
      "Epoch 00004: val_loss did not improve from -0.43170\n",
      "\n",
      "Learning rate: 1.7e-03\n",
      "Epoch 5/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.4286 - val_loss: -0.4628\n",
      "\n",
      "Epoch 00005: val_loss improved from -0.43170 to -0.46283, saving model to ../processed/model.h5\n",
      "\n",
      "Learning rate: 1.3e-03\n",
      "Epoch 6/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.4610 - val_loss: -0.4527\n",
      "\n",
      "Epoch 00006: val_loss did not improve from -0.46283\n",
      "\n",
      "Learning rate: 8.5e-04\n",
      "Epoch 7/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.4852 - val_loss: -0.5065\n",
      "\n",
      "Epoch 00007: val_loss improved from -0.46283 to -0.50648, saving model to ../processed/model.h5\n",
      "\n",
      "Learning rate: 4.2e-04\n",
      "Epoch 8/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5157 - val_loss: -0.5282\n",
      "\n",
      "Epoch 00008: val_loss improved from -0.50648 to -0.52818, saving model to ../processed/model.h5\n",
      "\n",
      "Learning rate: 2.1e-07\n",
      "Epoch 9/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5279 - val_loss: -0.5205\n",
      "\n",
      "Epoch 00009: val_loss did not improve from -0.52818\n",
      "\n",
      "Learning rate: 2.1e-04\n",
      "Epoch 10/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5219 - val_loss: -0.4926\n",
      "\n",
      "Epoch 00010: val_loss did not improve from -0.52818\n",
      "\n",
      "Learning rate: 4.3e-04\n",
      "Epoch 11/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5038 - val_loss: -0.4608\n",
      "\n",
      "Epoch 00011: val_loss did not improve from -0.52818\n",
      "\n",
      "Learning rate: 6.4e-04\n",
      "Epoch 12/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.4871 - val_loss: -0.4672\n",
      "\n",
      "Epoch 00012: val_loss did not improve from -0.52818\n",
      "\n",
      "Learning rate: 8.5e-04\n",
      "Epoch 13/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5017 - val_loss: -0.4971\n",
      "\n",
      "Epoch 00013: val_loss did not improve from -0.52818\n",
      "\n",
      "Learning rate: 6.4e-04\n",
      "Epoch 14/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5132 - val_loss: -0.5137\n",
      "\n",
      "Epoch 00014: val_loss did not improve from -0.52818\n",
      "\n",
      "Learning rate: 4.2e-04\n",
      "Epoch 15/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5306 - val_loss: -0.5360\n",
      "\n",
      "Epoch 00015: val_loss improved from -0.52818 to -0.53603, saving model to ../processed/model.h5\n",
      "\n",
      "Learning rate: 2.1e-04\n",
      "Epoch 16/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5624 - val_loss: -0.5512\n",
      "\n",
      "Epoch 00016: val_loss improved from -0.53603 to -0.55117, saving model to ../processed/model.h5\n",
      "\n",
      "Learning rate: 1.1e-07\n",
      "Epoch 17/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5536 - val_loss: -0.5484\n",
      "\n",
      "Epoch 00017: val_loss did not improve from -0.55117\n",
      "\n",
      "Learning rate: 1.1e-04\n",
      "Epoch 18/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5487 - val_loss: -0.5433\n",
      "\n",
      "Epoch 00018: val_loss did not improve from -0.55117\n",
      "\n",
      "Learning rate: 2.1e-04\n",
      "Epoch 19/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5381 - val_loss: -0.5253\n",
      "\n",
      "Epoch 00019: val_loss did not improve from -0.55117\n",
      "\n",
      "Learning rate: 3.2e-04\n",
      "Epoch 20/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5384 - val_loss: -0.5136\n",
      "\n",
      "Epoch 00020: val_loss did not improve from -0.55117\n",
      "\n",
      "Learning rate: 4.2e-04\n",
      "Epoch 21/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5201 - val_loss: -0.5395\n",
      "\n",
      "Epoch 00021: val_loss did not improve from -0.55117\n",
      "\n",
      "Learning rate: 3.2e-04\n",
      "Epoch 22/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5625 - val_loss: -0.5314\n",
      "\n",
      "Epoch 00022: val_loss did not improve from -0.55117\n",
      "\n",
      "Learning rate: 2.1e-04\n",
      "Epoch 23/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5528 - val_loss: -0.5413\n",
      "\n",
      "Epoch 00023: val_loss did not improve from -0.55117\n",
      "\n",
      "Learning rate: 1.1e-04\n",
      "Epoch 24/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5787 - val_loss: -0.5475\n",
      "\n",
      "Epoch 00024: val_loss did not improve from -0.55117\n",
      "\n",
      "Learning rate: 5.3e-08\n",
      "Epoch 25/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5738 - val_loss: -0.5487\n",
      "\n",
      "Epoch 00025: val_loss did not improve from -0.55117\n",
      "\n",
      "Learning rate: 5.3e-05\n",
      "Epoch 26/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5859 - val_loss: -0.5329\n",
      "\n",
      "Epoch 00026: val_loss did not improve from -0.55117\n",
      "\n",
      "Learning rate: 1.1e-04\n",
      "Epoch 27/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5743 - val_loss: -0.5204\n",
      "\n",
      "Epoch 00027: val_loss did not improve from -0.55117\n",
      "\n",
      "Learning rate: 1.6e-04\n",
      "Epoch 28/40\n",
      "1000/1000 [==============================] - 16s 16ms/step - loss: -0.5575 - val_loss: -0.5436\n",
      "\n",
      "Epoch 00028: val_loss did not improve from -0.55117\n",
      "\n",
      "Learning rate: 2.1e-04\n",
      "Epoch 29/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5633 - val_loss: -0.5392\n",
      "\n",
      "Epoch 00029: val_loss did not improve from -0.55117\n",
      "\n",
      "Learning rate: 1.6e-04\n",
      "Epoch 30/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.5661 - val_loss: -0.5446\n",
      "\n",
      "Epoch 00030: val_loss did not improve from -0.55117\n",
      "\n",
      "Learning rate: 1.1e-04\n",
      "Epoch 31/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5724 - val_loss: -0.5488\n",
      "\n",
      "Epoch 00031: val_loss did not improve from -0.55117\n",
      "\n",
      "Learning rate: 5.3e-05\n",
      "Epoch 32/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5840 - val_loss: -0.5518\n",
      "\n",
      "Epoch 00032: val_loss improved from -0.55117 to -0.55184, saving model to ../processed/model.h5\n",
      "\n",
      "Learning rate: 2.7e-08\n",
      "Epoch 33/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5916 - val_loss: -0.5502\n",
      "\n",
      "Epoch 00033: val_loss did not improve from -0.55184\n",
      "\n",
      "Learning rate: 2.7e-05\n",
      "Epoch 34/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5812 - val_loss: -0.5507\n",
      "\n",
      "Epoch 00034: val_loss did not improve from -0.55184\n",
      "\n",
      "Learning rate: 5.3e-05\n",
      "Epoch 35/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5818 - val_loss: -0.5445\n",
      "\n",
      "Epoch 00035: val_loss did not improve from -0.55184\n",
      "\n",
      "Learning rate: 8.0e-05\n",
      "Epoch 36/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5753 - val_loss: -0.5399\n",
      "\n",
      "Epoch 00036: val_loss did not improve from -0.55184\n",
      "\n",
      "Learning rate: 1.1e-04\n",
      "Epoch 37/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5784 - val_loss: -0.5365\n",
      "\n",
      "Epoch 00037: val_loss did not improve from -0.55184\n",
      "\n",
      "Learning rate: 8.0e-05\n",
      "Epoch 38/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5859 - val_loss: -0.5468\n",
      "\n",
      "Epoch 00038: val_loss did not improve from -0.55184\n",
      "\n",
      "Learning rate: 5.3e-05\n",
      "Epoch 39/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5871 - val_loss: -0.5490\n",
      "\n",
      "Epoch 00039: val_loss did not improve from -0.55184\n",
      "\n",
      "Learning rate: 2.7e-05\n",
      "Epoch 40/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5855 - val_loss: -0.5501\n",
      "\n",
      "Epoch 00040: val_loss did not improve from -0.55184\n",
      "\n",
      "Learning rate: 1.3e-08\n",
      "\n",
      "Fold - 3 / 10\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, 4)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 128)         35328     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 145,076\n",
      "Trainable params: 145,076\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 16s 16ms/step - loss: 0.7194 - intersection_over_union: -0.0283 - val_loss: 0.6303 - val_intersection_over_union: -0.1067\n",
      "Epoch 1/40\n",
      "1000/1000 [==============================] - 17s 17ms/step - loss: -0.2728 - val_loss: -0.3676\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to -0.36758, saving model to ../processed/model.h5\n",
      "\n",
      "Learning rate: 4.3e-04\n",
      "Epoch 2/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.3893 - val_loss: -0.3781\n",
      "\n",
      "Epoch 00002: val_loss improved from -0.36758 to -0.37808, saving model to ../processed/model.h5\n",
      "\n",
      "Learning rate: 8.5e-04\n",
      "Epoch 3/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.4100 - val_loss: -0.4280\n",
      "\n",
      "Epoch 00003: val_loss improved from -0.37808 to -0.42801, saving model to ../processed/model.h5\n",
      "\n",
      "Learning rate: 1.3e-03\n",
      "Epoch 4/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.4033 - val_loss: -0.4377\n",
      "\n",
      "Epoch 00004: val_loss improved from -0.42801 to -0.43774, saving model to ../processed/model.h5\n",
      "\n",
      "Learning rate: 1.7e-03\n",
      "Epoch 5/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.4416 - val_loss: -0.4813\n",
      "\n",
      "Epoch 00005: val_loss improved from -0.43774 to -0.48133, saving model to ../processed/model.h5\n",
      "\n",
      "Learning rate: 1.3e-03\n",
      "Epoch 6/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.4641 - val_loss: -0.4608\n",
      "\n",
      "Epoch 00006: val_loss did not improve from -0.48133\n",
      "\n",
      "Learning rate: 8.5e-04\n",
      "Epoch 7/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.4822 - val_loss: -0.5036\n",
      "\n",
      "Epoch 00007: val_loss improved from -0.48133 to -0.50362, saving model to ../processed/model.h5\n",
      "\n",
      "Learning rate: 4.2e-04\n",
      "Epoch 8/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.5195 - val_loss: -0.5490\n",
      "\n",
      "Epoch 00008: val_loss improved from -0.50362 to -0.54901, saving model to ../processed/model.h5\n",
      "\n",
      "Learning rate: 2.1e-07\n",
      "Epoch 9/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.5422 - val_loss: -0.5538\n",
      "\n",
      "Epoch 00009: val_loss improved from -0.54901 to -0.55384, saving model to ../processed/model.h5\n",
      "\n",
      "Learning rate: 2.1e-04\n",
      "Epoch 10/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.5178 - val_loss: -0.5350\n",
      "\n",
      "Epoch 00010: val_loss did not improve from -0.55384\n",
      "\n",
      "Learning rate: 4.3e-04\n",
      "Epoch 11/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5076 - val_loss: -0.5390\n",
      "\n",
      "Epoch 00011: val_loss did not improve from -0.55384\n",
      "\n",
      "Learning rate: 6.4e-04\n",
      "Epoch 12/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.4897 - val_loss: -0.4014\n",
      "\n",
      "Epoch 00012: val_loss did not improve from -0.55384\n",
      "\n",
      "Learning rate: 8.5e-04\n",
      "Epoch 13/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.4676 - val_loss: -0.5217\n",
      "\n",
      "Epoch 00013: val_loss did not improve from -0.55384\n",
      "\n",
      "Learning rate: 6.4e-04\n",
      "Epoch 14/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5086 - val_loss: -0.5336\n",
      "\n",
      "Epoch 00014: val_loss did not improve from -0.55384\n",
      "\n",
      "Learning rate: 4.2e-04\n",
      "Epoch 15/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5205 - val_loss: -0.5563\n",
      "\n",
      "Epoch 00015: val_loss improved from -0.55384 to -0.55628, saving model to ../processed/model.h5\n",
      "\n",
      "Learning rate: 2.1e-04\n",
      "Epoch 16/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5492 - val_loss: -0.5562\n",
      "\n",
      "Epoch 00016: val_loss did not improve from -0.55628\n",
      "\n",
      "Learning rate: 1.1e-07\n",
      "Epoch 17/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5518 - val_loss: -0.5611\n",
      "\n",
      "Epoch 00017: val_loss improved from -0.55628 to -0.56105, saving model to ../processed/model.h5\n",
      "\n",
      "Learning rate: 1.1e-04\n",
      "Epoch 18/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5471 - val_loss: -0.5545\n",
      "\n",
      "Epoch 00018: val_loss did not improve from -0.56105\n",
      "\n",
      "Learning rate: 2.1e-04\n",
      "Epoch 19/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5374 - val_loss: -0.5520\n",
      "\n",
      "Epoch 00019: val_loss did not improve from -0.56105\n",
      "\n",
      "Learning rate: 3.2e-04\n",
      "Epoch 20/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.5245 - val_loss: -0.5382\n",
      "\n",
      "Epoch 00020: val_loss did not improve from -0.56105\n",
      "\n",
      "Learning rate: 4.2e-04\n",
      "Epoch 21/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5347 - val_loss: -0.5391\n",
      "\n",
      "Epoch 00021: val_loss did not improve from -0.56105\n",
      "\n",
      "Learning rate: 3.2e-04\n",
      "Epoch 22/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5481 - val_loss: -0.5621\n",
      "\n",
      "Epoch 00022: val_loss improved from -0.56105 to -0.56210, saving model to ../processed/model.h5\n",
      "\n",
      "Learning rate: 2.1e-04\n",
      "Epoch 23/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5598 - val_loss: -0.5700\n",
      "\n",
      "Epoch 00023: val_loss improved from -0.56210 to -0.56999, saving model to ../processed/model.h5\n",
      "\n",
      "Learning rate: 1.1e-04\n",
      "Epoch 24/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.5649 - val_loss: -0.5720\n",
      "\n",
      "Epoch 00024: val_loss improved from -0.56999 to -0.57200, saving model to ../processed/model.h5\n",
      "\n",
      "Learning rate: 5.3e-08\n",
      "Epoch 25/40\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: -0.5624 - val_loss: -0.5700\n",
      "\n",
      "Epoch 00025: val_loss did not improve from -0.57200\n",
      "\n",
      "Learning rate: 5.3e-05\n",
      "Epoch 26/40\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: -0.5707 - val_loss: -0.5617\n",
      "\n",
      "Epoch 00026: val_loss did not improve from -0.57200\n",
      "\n",
      "Learning rate: 1.1e-04\n",
      "Epoch 27/40\n",
      " 998/1000 [============================>.] - ETA: 0s - loss: -0.5565"
     ]
    }
   ],
   "source": [
    "train_oof((votes, answers), votes_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
