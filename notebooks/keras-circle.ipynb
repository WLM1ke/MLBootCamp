{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "from keras import backend as K\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "COORDINATES = [\"Xmin\", \"Ymin\", \"Xmax\", \"Ymax\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WLMike/Documents/PycharmProjects/ML_venv/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/WLMike/Documents/PycharmProjects/ML_venv/lib/python3.6/site-packages/ipykernel_launcher.py:4: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "answers = pd.read_csv(\"../raw/train_answers.csv\").set_index(\"itemId\")\n",
    "answers.columns = COORDINATES\n",
    "scaler = preprocessing.StandardScaler().fit(np.vstack([answers.values, answers.values[:, [2, 3, 0, 1]]]))\n",
    "answers[COORDINATES] = scaler.transform(answers[COORDINATES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WLMike/Documents/PycharmProjects/ML_venv/lib/python3.6/site-packages/ipykernel_launcher.py:2: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "votes = pd.read_csv(\"../raw/train_data.csv\").set_index(\"itemId\")\n",
    "votes[COORDINATES] = scaler.transform(votes[COORDINATES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WLMike/Documents/PycharmProjects/ML_venv/lib/python3.6/site-packages/ipykernel_launcher.py:2: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "votes_test = pd.read_csv(\"../raw/test_data.csv\").set_index(\"itemId\")\n",
    "votes_test[COORDINATES] = scaler.transform(votes_test[COORDINATES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def find_center(y):\n",
    "    center_x = (y[0, 2] + y[0, 0]) / 2\n",
    "    center_y = (y[0, 3] + y[0, 1]) / 2 \n",
    "    return np.array([center_x, center_y, center_x, center_y])\n",
    "\n",
    "\n",
    "def scale(x, y, err=0.1):\n",
    "    center = find_center(y)\n",
    "    x, y = x - np.reshape(center, (1, 1, 4)), y - np.reshape(center, (1, 4))\n",
    "    \n",
    "    err_x, err_y = np.random.random(2)  \n",
    "    \n",
    "    err_x = 1 + (2 * err_x - 1) * err\n",
    "    err_y = 1 + (2 * err_y - 1) * err\n",
    "    \n",
    "    err = np.array([err_x, err_y, err_x, err_y])\n",
    "    \n",
    "    return (x * np.reshape(err, (1, 1, 4)) + np.reshape(center, (1, 1, 4)), \n",
    "            y * np.reshape(err, (1, 4)) + np.reshape(center, (1, 4)))\n",
    "    \n",
    "\n",
    "def move(x, y, err=0.1):\n",
    "    center = find_center(y)\n",
    "    x, y = x - np.reshape(center, (1, 1, 4)), y - np.reshape(center, (1, 4))\n",
    "    \n",
    "    size_x = (y[0, 2] - y[0, 0]) / 2\n",
    "    size_y = (y[0, 3] - y[0, 1]) / 2\n",
    "    \n",
    "    err_cx, err_cy = np.random.random(2)  \n",
    "    \n",
    "    err_cx = (2 * size_x * err_cx - size_x) * err\n",
    "    err_cy = (2 * size_y * err_cy - size_y) * err\n",
    "    \n",
    "    err = np.array([err_cx, err_cy, err_cx, err_cy])\n",
    "    \n",
    "    return x + np.reshape(err + center, (1, 1, 4)), y + np.reshape(err + center, (1, 4))\n",
    "    \n",
    "def aug_batch(x, y, err=0.1):\n",
    "    x, y = scale(x, y, err)\n",
    "    x, y = move(x, y, err)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def yield_batch(data, batch):\n",
    "    votes, answers = data\n",
    "    counts = votes[[\"userId\"]].groupby(\"itemId\").count()\n",
    "    feat = votes.groupby(\"itemId\")[COORDINATES].agg([\"min\", \"max\", \"std\", \"median\", \"mean\"])\n",
    "    feat[\"count\"] = counts\n",
    "    item_ids = list(set(votes.index))\n",
    "    while True:\n",
    "        item_id = np.random.choice(item_ids, batch)\n",
    "        x = feat.loc[item_id]\n",
    "        y = answers.loc[item_id]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def yield_batch_val(data):\n",
    "    votes, answers = data\n",
    "    counts = votes[[\"userId\"]].groupby(\"itemId\").count()\n",
    "    feat = votes.groupby(\"itemId\")[COORDINATES].agg([\"min\", \"max\", \"std\", \"median\", \"mean\"])\n",
    "    feat[\"count\"] = counts\n",
    "    item_ids = list(set(votes.index))\n",
    "    while True:\n",
    "        x = feat.loc[item_ids]\n",
    "        y = answers.loc[item_ids]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def yield_batch_test(data):\n",
    "    counts = data[[\"userId\"]].groupby(\"itemId\").count()\n",
    "    feat = data.groupby(\"itemId\")[COORDINATES].agg([\"min\", \"max\", \"std\", \"median\", \"mean\"])\n",
    "    feat[\"count\"] = counts\n",
    "    item_ids = data.index.unique()\n",
    "    for item_id in item_ids:\n",
    "        x = feat.loc[[item_id]]\n",
    "        yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def intersection_over_union(boxes_pred, boxes_true):\n",
    "\n",
    "    x_min = K.stack([boxes_pred[:, 0], boxes_true[:, 0]], axis=-1)\n",
    "    y_min = K.stack([boxes_pred[:, 1], boxes_true[:, 1]], axis=-1)\n",
    "    x_max = K.stack([boxes_pred[:, 2], boxes_true[:, 2]], axis=-1)\n",
    "    y_max = K.stack([boxes_pred[:, 3], boxes_true[:, 3]], axis=-1)\n",
    "\n",
    "    x_min = K.max(x_min, axis=-1)\n",
    "    y_min = K.max(y_min, axis=-1)\n",
    "    x_max = K.min(x_max, axis=-1)\n",
    "    y_max = K.min(y_max, axis=-1)\n",
    "\n",
    "    zeros = K.zeros_like(x_max)\n",
    "\n",
    "    x_inter = K.stack([zeros, x_max - x_min], axis=-1)\n",
    "    y_inter = K.stack([zeros, y_max - y_min], axis=-1)\n",
    "\n",
    "    x_inter = K.max(x_inter, axis=-1)\n",
    "    y_inter = K.max(y_inter, axis=-1)\n",
    "    inter_area = x_inter * y_inter\n",
    "    \n",
    "    area_pred = (K.max(K.stack([zeros, boxes_pred[:, 2] - boxes_pred[:, 0]], axis=-1), axis=-1) * \n",
    "                 K.max(K.stack([zeros, boxes_pred[:, 3] - boxes_pred[:, 1]], axis=-1), axis=-1))\n",
    "    area_true = (K.max(K.stack([zeros, boxes_true[:, 2] - boxes_true[:, 0]], axis=-1), axis=-1) * \n",
    "                 K.max(K.stack([zeros, boxes_true[:, 3] - boxes_true[:, 1]], axis=-1), axis=-1))\n",
    "\n",
    "    iou = inter_area / (area_pred + area_true - inter_area + K.epsilon())\n",
    "    \n",
    "    return -K.mean(iou, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def make_model(units):\n",
    "    K.clear_session()\n",
    "    \n",
    "    y = x = layers.Input(shape=(21,))\n",
    "    \n",
    "    y = layers.Dense(\n",
    "        units=units * 8,\n",
    "        activation=\"relu\"\n",
    "    )(y)\n",
    "    y = layers.Dense(\n",
    "        units=units * 4,\n",
    "        activation=\"relu\"\n",
    "    )(y)\n",
    "    y = layers.Dense(\n",
    "        units=units * 2,\n",
    "        activation=\"relu\"\n",
    "    )(y)\n",
    "    y = layers.Dense(\n",
    "        units=units,\n",
    "        activation=\"relu\"\n",
    "    )(y)\n",
    "    y = layers.Dense(\n",
    "        units=4,\n",
    "        activation=None\n",
    "    )(y)\n",
    "    \n",
    "    model = models.Model(inputs=x, outputs=y)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class MaxLRTest(callbacks.Callback):\n",
    "\n",
    "    def __init__(self, base_lr=1.0e-9, max_lr=10, steps=10000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.steps = steps\n",
    "        \n",
    "        self.iterations = None\n",
    "        self.history = None\n",
    "        \n",
    "    def clr(self):\n",
    "        return self.base_lr * (self.max_lr / self.base_lr) ** (self.iterations / self.steps) \n",
    "  \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.iterations = 0\n",
    "        self.history = {}\n",
    "        K.set_value(self.model.optimizer.lr, self.base_lr)     \n",
    "            \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if self.iterations > self.steps:\n",
    "            self.model.stop_training = True\n",
    "        \n",
    "        self.history.setdefault(\"iterations\", []).append(self.iterations)\n",
    "        self.history.setdefault(\"lr\", []).append(K.get_value(self.model.optimizer.lr))\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "            \n",
    "        self.iterations += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "        \n",
    "    def plot(self, smooth=None, t_score=2):\n",
    "        smooth = int(smooth or self.steps // 10)\n",
    "        df = pd.DataFrame(self.history).set_index(\"lr\").loss.rolling(smooth).agg([\"mean\", \"std\"])\n",
    "        df[\"std\"] = df[\"mean\"] + t_score * df[\"std\"] / smooth ** 0.5\n",
    "        print(f\"Max speed learning rate  - {df['std'].idxmin():.1e}\")\n",
    "        df.plot(logx=True, figsize=(16, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class DecayingCyclicLR(callbacks.Callback):\n",
    "\n",
    "    def __init__(self, max_lr, half=4, decay=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_lr = max_lr\n",
    "        self.half = half\n",
    "        self.decay = decay\n",
    "        \n",
    "        self.steps = None\n",
    "        self.epochs = None\n",
    "        \n",
    "        self.iterations = 0\n",
    "        self.history = {}\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.iterations = 1\n",
    "        self.history = {}\n",
    "     \n",
    "        self.steps = self.params[\"steps\"]\n",
    "        self.epochs = self.params[\"epochs\"]\n",
    "        \n",
    "        lr = 1 / self.steps / self.half\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        \n",
    "    def clr(self):\n",
    "        num, cycle = divmod(self.iterations / (self.steps * self.half), 2)\n",
    "        \n",
    "        if cycle <= 1:\n",
    "            return self.max_lr * cycle * self.decay ** num\n",
    "        \n",
    "        return (self.max_lr - self.max_lr * (cycle - 1)) * self.decay ** num\n",
    "            \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.history.setdefault(\"iterations\", []).append(self.iterations)\n",
    "        self.history.setdefault(\"lr\", []).append(K.get_value(self.model.optimizer.lr))\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "            \n",
    "        self.iterations += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"\\nLearning rate: {K.get_value(self.model.optimizer.lr):.1e}\")\n",
    "        \n",
    "    def plot(self, smooth=None):\n",
    "        smooth = int(smooth or self.steps)\n",
    "        df = pd.DataFrame(self.history).set_index(\"iterations\")[[\"loss\", \"lr\"]].rolling(smooth).mean()\n",
    "        df.plot(figsize=(16, 8), secondary_y=\"lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def train_model(data_train, data_val, units, batch, test=False):\n",
    "    \n",
    "    max_lr = 3.9e-03\n",
    "    \n",
    "    steps_per_epoch = 1000 // batch\n",
    "    epochs = 40\n",
    "    \n",
    "    model = make_model(units)  \n",
    "    \n",
    "    model.compile(optimizer=optimizers.Nadam(lr=max_lr / 100, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004),\n",
    "                      loss=\"mae\",\n",
    "                      metrics=[intersection_over_union]\n",
    "        )\n",
    "    rez = model.fit_generator(\n",
    "            yield_batch(data_train, batch),\n",
    "            steps_per_epoch=1000,\n",
    "            epochs=1,\n",
    "            callbacks=None,\n",
    "            validation_data=yield_batch_val(data_val),\n",
    "            validation_steps=1,\n",
    "        )\n",
    "    \n",
    "    \n",
    "    model.compile(optimizer=optimizers.Nadam(lr=max_lr, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004),\n",
    "                  loss=intersection_over_union,\n",
    "                  metrics=None\n",
    "    )\n",
    "    cycle = MaxLRTest() if test else DecayingCyclicLR(max_lr=max_lr)\n",
    "    cb = [\n",
    "        callbacks.ModelCheckpoint(\"../processed/model1.h5\", monitor=\"val_loss\", verbose=1, save_best_only=True),\n",
    "        cycle\n",
    "    ]\n",
    "    rez = model.fit_generator(\n",
    "        yield_batch(data_train, batch),\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=epochs,\n",
    "        callbacks=cb,\n",
    "        validation_data=yield_batch_val(data_val),\n",
    "        validation_steps=1,\n",
    "        )\n",
    "    \n",
    "    model = models.load_model(\"../processed/model1.h5\", custom_objects={\"intersection_over_union\": intersection_over_union})\n",
    "    \n",
    "    return rez, cycle, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "FOLDS = 10\n",
    "\n",
    "def train_oof(train_set, test_set, units=16, batch=1):\n",
    "    x_train, y_train = train_set\n",
    "    x_test = test_set\n",
    "\n",
    "    y_oof = pd.DataFrame(0, index=y_train.index, columns=COORDINATES)\n",
    "    y_pred = pd.DataFrame(0, index=x_test.index.unique(), columns=COORDINATES)\n",
    "    scores = []\n",
    "    folds = model_selection.KFold(n_splits=FOLDS, shuffle=True)\n",
    "    \n",
    "    for n, (index_train, index_valid) in enumerate(folds.split(y_train), 1):\n",
    "        print(f\"\\nFold - {n} / {FOLDS}\")\n",
    "        \n",
    "        ids_train = y_train.index[index_train]\n",
    "        ids_valid = y_train.index[index_valid]\n",
    "        \n",
    "        data_train = (x_train.loc[ids_train], y_train.loc[ids_train])\n",
    "        data_val = (x_train.loc[ids_valid], y_train.loc[ids_valid])\n",
    "        \n",
    "        rez, cycle, model = train_model(data_train, data_val, units, batch)\n",
    "        \n",
    "        cycle.plot()\n",
    "        pd.DataFrame(rez.history)[[\"loss\", \"val_loss\"]].plot(figsize=(16, 8))\n",
    "        scores.append(min(rez.history[\"val_loss\"]))\n",
    "        \n",
    "        feat = yield_batch_test(data_val[0])\n",
    "        df = model.predict_generator(feat, steps=len(data_val[0].index.unique()))\n",
    "        df = scaler.inverse_transform(df)\n",
    "        y_oof.loc[ids_valid] = df\n",
    "        \n",
    "        feat = yield_batch_test(x_test)\n",
    "        df = model.predict_generator(feat, steps=len(x_test.index.unique()))\n",
    "        df = scaler.inverse_transform(df)\n",
    "        y_pred += df / FOLDS\n",
    "\n",
    "    print(f\"IOU на кроссвалидации: \" + str(-np.round(sorted(scores), 5)))\n",
    "    print(f\"IOU среднее: {-np.mean(scores):0.5f} +/- {np.std(scores):0.5f}\")\n",
    "\n",
    "    subdir = time.strftime('%Y-%m-%d_%H-%M')\n",
    "    path = pathlib.Path(f\"../processed/{subdir}\")\n",
    "    path.mkdir(exist_ok=True)\n",
    "    y_oof.to_csv(path / f\"oof-{-np.mean(scores):0.5f}.csv\", header=False)\n",
    "    y_pred.to_csv(path / f\"sub-{-np.mean(scores):0.5f}.csv\", header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold - 1 / 10\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 21)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               2816      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 13,748\n",
      "Trainable params: 13,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.5583 - intersection_over_union: -0.0993 - val_loss: 0.3535 - val_intersection_over_union: -0.2121\n",
      "Epoch 1/40\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: -0.3611 - val_loss: -0.4372\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to -0.43724, saving model to ../processed/model1.h5\n",
      "\n",
      "Learning rate: 9.8e-04\n",
      "Epoch 2/40\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: -0.4018 - val_loss: -0.4192\n",
      "\n",
      "Epoch 00002: val_loss did not improve from -0.43724\n",
      "\n",
      "Learning rate: 2.0e-03\n",
      "Epoch 3/40\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: -0.4179 - val_loss: -0.4067\n",
      "\n",
      "Epoch 00003: val_loss did not improve from -0.43724\n",
      "\n",
      "Learning rate: 2.9e-03\n",
      "Epoch 4/40\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: -0.4137 - val_loss: -0.4597\n",
      "\n",
      "Epoch 00004: val_loss improved from -0.43724 to -0.45975, saving model to ../processed/model1.h5\n",
      "\n",
      "Learning rate: 3.9e-03\n",
      "Epoch 5/40\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: -0.4261 - val_loss: -0.3989\n",
      "\n",
      "Epoch 00005: val_loss did not improve from -0.45975\n",
      "\n",
      "Learning rate: 2.9e-03\n",
      "Epoch 6/40\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: -0.4504 - val_loss: -0.4237\n",
      "\n",
      "Epoch 00006: val_loss did not improve from -0.45975\n",
      "\n",
      "Learning rate: 1.9e-03\n",
      "Epoch 7/40\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: -0.4789 - val_loss: -0.5245\n",
      "\n",
      "Epoch 00007: val_loss improved from -0.45975 to -0.52455, saving model to ../processed/model1.h5\n",
      "\n",
      "Learning rate: 9.7e-04\n",
      "Epoch 8/40\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: -0.5343 - val_loss: -0.5534\n",
      "\n",
      "Epoch 00008: val_loss improved from -0.52455 to -0.55343, saving model to ../processed/model1.h5\n",
      "\n",
      "Learning rate: 4.9e-07\n",
      "Epoch 9/40\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: -0.5558 - val_loss: -0.5511\n",
      "\n",
      "Epoch 00009: val_loss did not improve from -0.55343\n",
      "\n",
      "Learning rate: 4.9e-04\n",
      "Epoch 10/40\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: -0.5259 - val_loss: -0.5357\n",
      "\n",
      "Epoch 00010: val_loss did not improve from -0.55343\n",
      "\n",
      "Learning rate: 9.8e-04\n",
      "Epoch 11/40\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: -0.5120 - val_loss: -0.4675\n",
      "\n",
      "Epoch 00011: val_loss did not improve from -0.55343\n",
      "\n",
      "Learning rate: 1.5e-03\n",
      "Epoch 12/40\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: -0.4903 - val_loss: -0.4609\n",
      "\n",
      "Epoch 00012: val_loss did not improve from -0.55343\n",
      "\n",
      "Learning rate: 1.9e-03\n",
      "Epoch 13/40\n",
      " 735/1000 [=====================>........] - ETA: 0s - loss: -0.4818"
     ]
    }
   ],
   "source": [
    "train_oof((votes, answers), votes_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
