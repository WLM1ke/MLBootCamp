{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "from keras import backend as K\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "from sklearn import compose\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "COORDINATES = [\"Xmin\", \"Ymin\", \"Xmax\", \"Ymax\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WLMike/Documents/PycharmProjects/ML_venv/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/WLMike/Documents/PycharmProjects/ML_venv/lib/python3.6/site-packages/ipykernel_launcher.py:4: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "answers = pd.read_csv(\"../raw/train_answers.csv\").set_index(\"itemId\")\n",
    "answers.columns = COORDINATES\n",
    "scaler = preprocessing.StandardScaler().fit(np.vstack([answers.values, answers.values[:, [2, 3, 0, 1]]]))\n",
    "answers[COORDINATES] = scaler.transform(answers[COORDINATES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WLMike/Documents/PycharmProjects/ML_venv/lib/python3.6/site-packages/ipykernel_launcher.py:2: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "votes = pd.read_csv(\"../raw/train_data.csv\").set_index(\"itemId\")\n",
    "votes[COORDINATES] = scaler.transform(votes[COORDINATES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WLMike/Documents/PycharmProjects/ML_venv/lib/python3.6/site-packages/ipykernel_launcher.py:2: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "votes_test = pd.read_csv(\"../raw/test_data.csv\").set_index(\"itemId\")\n",
    "votes_test[COORDINATES] = scaler.transform(votes_test[COORDINATES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def yield_batch(data):\n",
    "    votes, answers = data\n",
    "    item_ids = list(set(votes.index))\n",
    "    while True:\n",
    "        item_id = np.random.choice(item_ids, 1)\n",
    "        forecasts = votes.loc[item_id]\n",
    "        x = np.zeros((1, *forecasts.shape))\n",
    "        y = np.zeros((1, 4))\n",
    "        x[0] = forecasts.sample(len(forecasts))\n",
    "        y[0] = answers.loc[item_id]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def yield_batch_val(data):\n",
    "    votes, answers = data\n",
    "    item_ids = set(votes.index)\n",
    "    while True:\n",
    "        for item_id in item_ids:\n",
    "            forecasts = votes.loc[item_id]\n",
    "            x = np.zeros((1, *forecasts.shape))\n",
    "            y = np.zeros((1, 4))\n",
    "\n",
    "            x[0] = forecasts\n",
    "            y[0] = answers.loc[item_id]\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def yield_batch_test(data):\n",
    "    item_ids = data.index.unique()\n",
    "    for item_id in item_ids:\n",
    "        forecasts = data.loc[item_id]\n",
    "        x = np.zeros((1, *forecasts.shape))\n",
    "        y = np.zeros((1, 4))\n",
    "        x[0] = forecasts\n",
    "        yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def intersection_over_union(boxes_pred, boxes_true):\n",
    "\n",
    "    x_min = K.stack([boxes_pred[:, 0], boxes_true[:, 0]], axis=-1)\n",
    "    y_min = K.stack([boxes_pred[:, 1], boxes_true[:, 1]], axis=-1)\n",
    "    x_max = K.stack([boxes_pred[:, 2], boxes_true[:, 2]], axis=-1)\n",
    "    y_max = K.stack([boxes_pred[:, 3], boxes_true[:, 3]], axis=-1)\n",
    "\n",
    "    x_min = K.max(x_min, axis=-1)\n",
    "    y_min = K.max(y_min, axis=-1)\n",
    "    x_max = K.min(x_max, axis=-1)\n",
    "    y_max = K.min(y_max, axis=-1)\n",
    "\n",
    "    zeros = K.zeros_like(x_max)\n",
    "\n",
    "    x_inter = K.stack([zeros, x_max - x_min], axis=-1)\n",
    "    y_inter = K.stack([zeros, y_max - y_min], axis=-1)\n",
    "\n",
    "    x_inter = K.max(x_inter, axis=-1)\n",
    "    y_inter = K.max(y_inter, axis=-1)\n",
    "    inter_area = x_inter * y_inter\n",
    "    \n",
    "    area_pred = (K.max(K.stack([zeros, boxes_pred[:, 2] - boxes_pred[:, 0]], axis=-1), axis=-1) * \n",
    "                 K.max(K.stack([zeros, boxes_pred[:, 3] - boxes_pred[:, 1]], axis=-1), axis=-1))\n",
    "    area_true = (K.max(K.stack([zeros, boxes_true[:, 2] - boxes_true[:, 0]], axis=-1), axis=-1) * \n",
    "                 K.max(K.stack([zeros, boxes_true[:, 3] - boxes_true[:, 1]], axis=-1), axis=-1))\n",
    "\n",
    "    iou = inter_area / (area_pred + area_true - inter_area + K.epsilon())\n",
    "    \n",
    "    return -K.mean(iou, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def make_model(filters, input_shape, conv_l=6):\n",
    "    K.clear_session()\n",
    "    \n",
    "    y = x = layers.Input(shape=(None, input_shape))\n",
    "    \n",
    "    y_hot = layers.Lambda(lambda z: z[:, :, :-4])(y)\n",
    "    y_coor = layers.Lambda(lambda z: z[:, :, -4:])(y)\n",
    "    \n",
    "    y_hot = layers.Dropout(0.2)(y_hot)  # 0.15 новых\n",
    "    y_hot = layers.Dense(units=1, activation=None)(y_hot)\n",
    "    \n",
    "    y = layers.concatenate([y_hot, y_coor])\n",
    "    \n",
    "    for i in range(conv_l):\n",
    "        y = layers.Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=3,\n",
    "            strides=1,\n",
    "            padding=\"same\"\n",
    "        )(y)\n",
    "    \n",
    "    y = layers.GlobalAveragePooling1D()(y)\n",
    "    \n",
    "    y = layers.Dense(\n",
    "        units=filters // 2,\n",
    "        activation=\"relu\"\n",
    "    )(y)\n",
    "    y = layers.Dense(\n",
    "        units=4,\n",
    "        activation=None\n",
    "    )(y)\n",
    "    \n",
    "    model = models.Model(inputs=x, outputs=y)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class MaxLRTest(callbacks.Callback):\n",
    "\n",
    "    def __init__(self, base_lr=1.0e-9, max_lr=10, steps=10000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.steps = steps\n",
    "        \n",
    "        self.iterations = None\n",
    "        self.history = None\n",
    "        \n",
    "    def clr(self):\n",
    "        return self.base_lr * (self.max_lr / self.base_lr) ** (self.iterations / self.steps) \n",
    "  \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.iterations = 0\n",
    "        self.history = {}\n",
    "        K.set_value(self.model.optimizer.lr, self.base_lr)     \n",
    "            \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if self.iterations > self.steps:\n",
    "            self.model.stop_training = True\n",
    "        \n",
    "        self.history.setdefault(\"iterations\", []).append(self.iterations)\n",
    "        self.history.setdefault(\"lr\", []).append(K.get_value(self.model.optimizer.lr))\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "            \n",
    "        self.iterations += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "        \n",
    "    def plot(self, smooth=None, t_score=2):\n",
    "        smooth = int(smooth or self.steps // 10)\n",
    "        df = pd.DataFrame(self.history).set_index(\"lr\").loss.rolling(smooth).agg([\"mean\", \"std\"])\n",
    "        df[\"std\"] = df[\"mean\"] + t_score * df[\"std\"] / smooth ** 0.5\n",
    "        print(f\"Max speed learning rate  - {df['std'].idxmin():.1e}\")\n",
    "        df.plot(logx=True, figsize=(16, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class DecayingLR(callbacks.Callback):\n",
    "\n",
    "    def __init__(self, max_lr=0.1, warm_up=1, decay_per_epoch=1, epsilon=0.0001, wait=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_lr = max_lr\n",
    "        self.warm_up = warm_up\n",
    "        self.decay_per_epoch = decay_per_epoch\n",
    "        self.epsilon=epsilon\n",
    "        self.wait=wait\n",
    "        \n",
    "        self.iterations = None\n",
    "        self.history = None\n",
    "        \n",
    "        self.d_loss_iterations = None\n",
    "        self.d_loss = None\n",
    "        self.d_iterations = None\n",
    "        self.d_iterations2 = None\n",
    "        self.d_1 = None\n",
    "                \n",
    "        self.steps = None\n",
    "        self.decay = None\n",
    "        \n",
    "        self.wait_count=None\n",
    "        self.prev_loss=None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.iterations = 1\n",
    "        self.history = {}\n",
    "        \n",
    "        self.d_loss_iterations = 0.0\n",
    "        self.d_loss = 0.0\n",
    "        self.d_iterations = 0.0\n",
    "        self.d_iterations2 = 0.0\n",
    "        self.d_1 = 0.0\n",
    "        \n",
    "        self.steps = self.params[\"steps\"]\n",
    "        self.decay = (1 - 1 / self.steps) ** (1 / self.decay_per_epoch)\n",
    "        \n",
    "        self.wait_count=0\n",
    "        self.prev_loss=None\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.lr())\n",
    "    \n",
    "    def make_decay(self, old, new):\n",
    "        decay = self.decay\n",
    "        return old * decay + new * (1 - decay)\n",
    "    \n",
    "    def lr(self):\n",
    "        warm_up = self.iterations / self.steps * self.warm_up\n",
    "        if warm_up <= 1:\n",
    "            return self.max_lr * warm_up \n",
    "        \n",
    "        lr = self.history[\"lr\"][-1]\n",
    "        speed = self.history[\"speed\"][-1]\n",
    "        \n",
    "        if speed > 0:\n",
    "            return lr * self.decay\n",
    "        \n",
    "        return lr\n",
    "            \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.history.setdefault(\"iterations\", []).append(self.iterations)\n",
    "        self.history.setdefault(\"lr\", []).append(K.get_value(self.model.optimizer.lr))\n",
    "        \n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "            \n",
    "        self.d_loss_iterations = self.make_decay(self.d_loss_iterations, logs[\"loss\"] * self.iterations)\n",
    "        self.d_loss = self.make_decay(self.d_loss, logs[\"loss\"])\n",
    "        self.d_iterations = self.make_decay(self.d_iterations, self.iterations)\n",
    "        self.d_iterations2 = self.make_decay(self.d_iterations2, self.iterations ** 2)\n",
    "        self.d_1 = self.make_decay(self.d_1, 1)\n",
    "        \n",
    "        cov = self.d_loss_iterations - self.d_loss * self.d_iterations / self.d_1\n",
    "        var = self.d_iterations2 - self.d_iterations ** 2 / self.d_1\n",
    "        \n",
    "        speed = self.steps * cov / var\n",
    "        self.history.setdefault(\"speed\", []).append(speed)\n",
    "        \n",
    "        self.iterations += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.lr())     \n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = self.history[\"lr\"][-1]\n",
    "        speed = self.history[\"speed\"][-1]\n",
    "        print(f\"Learning rate: {lr:.1e}\")\n",
    "        print(f\"Speed per epoch: {speed:.4f}\")\n",
    "        \n",
    "        if self.prev_loss is None:\n",
    "            self.prev_loss = logs[\"val_loss\"]\n",
    "        elif abs(self.prev_loss - logs[\"val_loss\"]) < self.epsilon:\n",
    "            self.wait_count += 1\n",
    "        else:\n",
    "            self.wait_count = 0\n",
    "        self.prev_loss = logs[\"val_loss\"]\n",
    "        print(f\"Wait to stop: {self.wait - self.wait_count}\\n\")\n",
    "        if self.wait == self.wait_count:\n",
    "            self.model.stop_training = True\n",
    "        \n",
    "    def plot(self, smooth=None):\n",
    "        smooth = int(smooth or self.steps)\n",
    "        df = pd.DataFrame(self.history).set_index(\"iterations\")[[\"loss\", \"lr\"]].rolling(smooth).mean()\n",
    "        df.plot(figsize=(16, 8), secondary_y=\"lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def train_model(data_train, data_val, units, test=False):\n",
    "    \n",
    "    max_lr = 1.4e-04\n",
    "    \n",
    "    steps_per_epoch = 1000\n",
    "    epochs = 100\n",
    "    \n",
    "    model = make_model(units, data_train[0].shape[1])  \n",
    "    \n",
    "    model.compile(optimizer=optimizers.Nadam(lr=max_lr / 100, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004),\n",
    "                      loss=\"mae\",\n",
    "                      metrics=[intersection_over_union]\n",
    "        )\n",
    "    rez = model.fit_generator(\n",
    "            yield_batch(data_train),\n",
    "            steps_per_epoch=1000,\n",
    "            epochs=1,\n",
    "            callbacks=None,\n",
    "            validation_data=yield_batch_val(data_val),\n",
    "            validation_steps=len(data_val[1].index),\n",
    "        )\n",
    "    \n",
    "    \n",
    "    model.compile(optimizer=optimizers.Nadam(lr=max_lr, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004),\n",
    "                  loss=intersection_over_union,\n",
    "                  metrics=None\n",
    "    )\n",
    "    cycle = MaxLRTest() if test else DecayingLR(max_lr=max_lr)\n",
    "    cb = [\n",
    "        callbacks.ModelCheckpoint(\"../processed/model.h5\", monitor=\"val_loss\", verbose=1, save_best_only=True),\n",
    "        cycle\n",
    "    ]\n",
    "    rez = model.fit_generator(\n",
    "        yield_batch(data_train),\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=epochs,\n",
    "        callbacks=cb,\n",
    "        validation_data=yield_batch_val(data_val),\n",
    "        validation_steps=len(data_val[1].index),\n",
    "        )\n",
    "    \n",
    "    model = models.load_model(\"../processed/model.h5\", custom_objects={\"intersection_over_union\": intersection_over_union})\n",
    "    \n",
    "    return rez, cycle, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0727 18:16:56.408217 4623693248 deprecation.py:506] From /Users/WLMike/Documents/PycharmProjects/ML_venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 1361)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 1357)   0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 1357)   0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 1)      1358        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None, 4)      0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 5)      0           dense_1[0][0]                    \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 64)     1024        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 64)     12352       conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, None, 64)     12352       conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, None, 64)     12352       conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, None, 64)     12352       conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, None, 64)     12352       conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 64)           0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           2080        global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4)            132         dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 66,354\n",
      "Trainable params: 66,354\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.4097 - intersection_over_union: -0.2339 - val_loss: 0.2790 - val_intersection_over_union: -0.3699\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: -0.3611 - val_loss: -0.3701\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to -0.37007, saving model to ../processed/model.h5\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: -0.3705 - val_loss: -0.3721\n",
      "\n",
      "Epoch 00002: val_loss improved from -0.37007 to -0.37205, saving model to ../processed/model.h5\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: -0.3735 - val_loss: -0.3841\n",
      "\n",
      "Epoch 00003: val_loss improved from -0.37205 to -0.38413, saving model to ../processed/model.h5\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: -0.3770 - val_loss: -0.4091\n",
      "\n",
      "Epoch 00004: val_loss improved from -0.38413 to -0.40906, saving model to ../processed/model.h5\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: -0.4319 - val_loss: -0.4102\n",
      "\n",
      "Epoch 00005: val_loss improved from -0.40906 to -0.41017, saving model to ../processed/model.h5\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: -0.3878 - val_loss: -0.3036\n",
      "\n",
      "Epoch 00006: val_loss did not improve from -0.41017\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: -0.2004 - val_loss: -5.5997e-04\n",
      "\n",
      "Epoch 00007: val_loss did not improve from -0.41017\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: -6.9503e-05 - val_loss: -5.7218e-06\n",
      "\n",
      "Epoch 00008: val_loss did not improve from -0.41017\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: -2.8049e-06 - val_loss: -5.0884e-06\n",
      "\n",
      "Epoch 00009: val_loss did not improve from -0.41017\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: -9.8852e-07 - val_loss: 0.0000e+00\n",
      "\n",
      "Epoch 00010: val_loss did not improve from -0.41017\n",
      "Epoch 11/100\n",
      "   1/1000 [..............................] - ETA: 2s - loss: 0.0000e+00\n",
      "Epoch 00011: val_loss did not improve from -0.41017\n",
      "Max speed learning rate  - 1.4e-04\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a, *_ = transform(votes, votes, votes)\n",
    "train_model((a, answers), (a, answers), 64, True)[1].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(train, val, test):\n",
    "    trans = compose.ColumnTransformer(\n",
    "        [(\"onehot_id\", preprocessing.OneHotEncoder(handle_unknown=\"ignore\"), [0])],\n",
    "        remainder=\"passthrough\",\n",
    "        sparse_threshold=0\n",
    "    )\n",
    "    train = pd.DataFrame(trans.fit_transform(train), index=train.index)\n",
    "    val = pd.DataFrame(trans.transform(val), index=val.index)\n",
    "    test = pd.DataFrame(trans.transform(test), index=test.index)\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "FOLDS = 10\n",
    "\n",
    "def train_oof(train_set, test_set, units=64):\n",
    "    x_train, y_train = train_set\n",
    "    x_test = test_set\n",
    "\n",
    "    y_oof = pd.DataFrame(0, index=y_train.index, columns=COORDINATES)\n",
    "    y_pred = pd.DataFrame(0, index=x_test.index.unique(), columns=COORDINATES)\n",
    "    scores = []\n",
    "    folds = model_selection.KFold(n_splits=FOLDS, shuffle=True)\n",
    "    \n",
    "    for n, (index_train, index_valid) in enumerate(folds.split(y_train), 1):\n",
    "        print(f\"\\nFold - {n} / {FOLDS}\")\n",
    "        \n",
    "        ids_train = y_train.index[index_train]\n",
    "        ids_valid = y_train.index[index_valid]\n",
    "        \n",
    "        x_train_t, x_val_t, x_test_t = transform(x_train.loc[ids_train], x_train.loc[ids_valid], x_test)\n",
    "        \n",
    "        data_train = (x_train_t, y_train.loc[ids_train])\n",
    "        data_val = (x_val_t, y_train.loc[ids_valid])\n",
    "        \n",
    "        rez, cycle, model = train_model(data_train, data_val, units)\n",
    "        \n",
    "        cycle.plot()\n",
    "        pd.DataFrame(rez.history)[[\"loss\", \"val_loss\"]].plot(figsize=(16, 8))\n",
    "        scores.append(min(rez.history[\"val_loss\"]))\n",
    "        \n",
    "        feat = yield_batch_test(data_val[0])\n",
    "        df = model.predict_generator(feat, steps=len(data_val[0].index.unique()))\n",
    "        df = scaler.inverse_transform(df)\n",
    "        y_oof.loc[ids_valid] = df\n",
    "        \n",
    "        feat = yield_batch_test(x_test_t)\n",
    "        df = model.predict_generator(feat, steps=len(x_test.index.unique()))\n",
    "        df = scaler.inverse_transform(df)\n",
    "        y_pred += df / FOLDS\n",
    "\n",
    "    print(f\"IOU на кроссвалидации: \" + str(-np.round(sorted(scores), 5)))\n",
    "    print(f\"IOU среднее: {-np.mean(scores):0.5f} +/- {2 * np.std(scores) / len(scores) ** 2:0.5f}\")\n",
    "\n",
    "    subdir = time.strftime('%Y-%m-%d_%H-%M')\n",
    "    path = pathlib.Path(f\"../processed/{subdir}\")\n",
    "    path.mkdir(exist_ok=True)\n",
    "    y_oof.to_csv(path / f\"oof-{-np.mean(scores):0.5f}.csv\", header=False)\n",
    "    y_pred.to_csv(path / f\"sub-{-np.mean(scores):0.5f}.csv\", header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold - 1 / 10\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 1306)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 1302)   0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 1302)   0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 1)      1303        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None, 4)      0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 5)      0           dense_1[0][0]                    \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 64)     1024        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 64)     12352       conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, None, 64)     12352       conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, None, 64)     12352       conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, None, 64)     12352       conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, None, 64)     12352       conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 64)           0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           2080        global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4)            132         dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 66,299\n",
      "Trainable params: 66,299\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.8509 - intersection_over_union: -0.0079 - val_loss: 0.7283 - val_intersection_over_union: -0.0335\n",
      "Epoch 1/100\n",
      "  29/1000 [..............................] - ETA: 25s - loss: -0.0191"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WLMike/Documents/PycharmProjects/ML_venv/lib/python3.6/site-packages/ipykernel_launcher.py:78: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 7s 7ms/step - loss: -0.2886 - val_loss: -0.3631\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to -0.36311, saving model to ../processed/model.h5\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.3646\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: -0.4309 - val_loss: -0.4575\n",
      "\n",
      "Epoch 00002: val_loss improved from -0.36311 to -0.45746, saving model to ../processed/model.h5\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.1163\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: -0.4505 - val_loss: -0.4724\n",
      "\n",
      "Epoch 00003: val_loss improved from -0.45746 to -0.47240, saving model to ../processed/model.h5\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.0394\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: -0.4764 - val_loss: -0.4803\n",
      "\n",
      "Epoch 00004: val_loss improved from -0.47240 to -0.48033, saving model to ../processed/model.h5\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.0327\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4692 - val_loss: -0.4889\n",
      "\n",
      "Epoch 00005: val_loss improved from -0.48033 to -0.48893, saving model to ../processed/model.h5\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.0110\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4784 - val_loss: -0.4345\n",
      "\n",
      "Epoch 00006: val_loss did not improve from -0.48893\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.0088\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4765 - val_loss: -0.4992\n",
      "\n",
      "Epoch 00007: val_loss improved from -0.48893 to -0.49923, saving model to ../processed/model.h5\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.0015\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4905 - val_loss: -0.4450\n",
      "\n",
      "Epoch 00008: val_loss did not improve from -0.49923\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.0051\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4958 - val_loss: -0.4994\n",
      "\n",
      "Epoch 00009: val_loss improved from -0.49923 to -0.49944, saving model to ../processed/model.h5\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.0064\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4939 - val_loss: -0.4960\n",
      "\n",
      "Epoch 00010: val_loss did not improve from -0.49944\n",
      "Learning rate: 1.2e-04\n",
      "Speed per epoch: -0.0086\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5139 - val_loss: -0.4744\n",
      "\n",
      "Epoch 00011: val_loss did not improve from -0.49944\n",
      "Learning rate: 1.2e-04\n",
      "Speed per epoch: -0.0066\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: -0.5013 - val_loss: -0.5021\n",
      "\n",
      "Epoch 00012: val_loss improved from -0.49944 to -0.50213, saving model to ../processed/model.h5\n",
      "Learning rate: 9.9e-05\n",
      "Speed per epoch: -0.0007\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5220 - val_loss: -0.4856\n",
      "\n",
      "Epoch 00013: val_loss did not improve from -0.50213\n",
      "Learning rate: 9.9e-05\n",
      "Speed per epoch: -0.0083\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5245 - val_loss: -0.5149\n",
      "\n",
      "Epoch 00014: val_loss improved from -0.50213 to -0.51495, saving model to ../processed/model.h5\n",
      "Learning rate: 9.9e-05\n",
      "Speed per epoch: -0.0066\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5164 - val_loss: -0.5220\n",
      "\n",
      "Epoch 00015: val_loss improved from -0.51495 to -0.52199, saving model to ../processed/model.h5\n",
      "Learning rate: 7.7e-05\n",
      "Speed per epoch: -0.0007\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5282 - val_loss: -0.5384\n",
      "\n",
      "Epoch 00016: val_loss improved from -0.52199 to -0.53836, saving model to ../processed/model.h5\n",
      "Learning rate: 6.4e-05\n",
      "Speed per epoch: -0.0063\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5301 - val_loss: -0.5300\n",
      "\n",
      "Epoch 00017: val_loss did not improve from -0.53836\n",
      "Learning rate: 6.4e-05\n",
      "Speed per epoch: -0.0054\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5341 - val_loss: -0.5384\n",
      "\n",
      "Epoch 00018: val_loss improved from -0.53836 to -0.53838, saving model to ../processed/model.h5\n",
      "Learning rate: 6.4e-05\n",
      "Speed per epoch: -0.0029\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: -0.5347 - val_loss: -0.5165\n",
      "\n",
      "Epoch 00019: val_loss did not improve from -0.53838\n",
      "Learning rate: 4.5e-05\n",
      "Speed per epoch: -0.0058\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5536 - val_loss: -0.5386\n",
      "\n",
      "Epoch 00020: val_loss improved from -0.53838 to -0.53862, saving model to ../processed/model.h5\n",
      "Learning rate: 4.5e-05\n",
      "Speed per epoch: -0.0100\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5588 - val_loss: -0.5192\n",
      "\n",
      "Epoch 00021: val_loss did not improve from -0.53862\n",
      "Learning rate: 4.5e-05\n",
      "Speed per epoch: -0.0071\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5529 - val_loss: -0.5318\n",
      "\n",
      "Epoch 00022: val_loss did not improve from -0.53862\n",
      "Learning rate: 3.8e-05\n",
      "Speed per epoch: -0.0032\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5460 - val_loss: -0.5529\n",
      "\n",
      "Epoch 00023: val_loss improved from -0.53862 to -0.55291, saving model to ../processed/model.h5\n",
      "Learning rate: 2.5e-05\n",
      "Speed per epoch: 0.0024\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5605 - val_loss: -0.5647\n",
      "\n",
      "Epoch 00024: val_loss improved from -0.55291 to -0.56467, saving model to ../processed/model.h5\n",
      "Learning rate: 1.4e-05\n",
      "Speed per epoch: -0.0067\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5768 - val_loss: -0.5479\n",
      "\n",
      "Epoch 00025: val_loss did not improve from -0.56467\n",
      "Learning rate: 1.4e-05\n",
      "Speed per epoch: -0.0099\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5698 - val_loss: -0.5698\n",
      "\n",
      "Epoch 00026: val_loss improved from -0.56467 to -0.56982, saving model to ../processed/model.h5\n",
      "Learning rate: 1.4e-05\n",
      "Speed per epoch: -0.0055\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5716 - val_loss: -0.5636\n",
      "\n",
      "Epoch 00027: val_loss did not improve from -0.56982\n",
      "Learning rate: 1.4e-05\n",
      "Speed per epoch: -0.0011\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5743 - val_loss: -0.5627\n",
      "\n",
      "Epoch 00028: val_loss did not improve from -0.56982\n",
      "Learning rate: 1.1e-05\n",
      "Speed per epoch: -0.0010\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5676 - val_loss: -0.5710\n",
      "\n",
      "Epoch 00029: val_loss improved from -0.56982 to -0.57098, saving model to ../processed/model.h5\n",
      "Learning rate: 5.2e-06\n",
      "Speed per epoch: 0.0004\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5808 - val_loss: -0.5626\n",
      "\n",
      "Epoch 00030: val_loss did not improve from -0.57098\n",
      "Learning rate: 4.6e-06\n",
      "Speed per epoch: -0.0029\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: -0.5859 - val_loss: -0.5662\n",
      "\n",
      "Epoch 00031: val_loss did not improve from -0.57098\n",
      "Learning rate: 4.6e-06\n",
      "Speed per epoch: -0.0051\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5760 - val_loss: -0.5704\n",
      "\n",
      "Epoch 00032: val_loss did not improve from -0.57098\n",
      "Learning rate: 3.5e-06\n",
      "Speed per epoch: 0.0017\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5749 - val_loss: -0.5669\n",
      "\n",
      "Epoch 00033: val_loss did not improve from -0.57098\n",
      "Learning rate: 1.5e-06\n",
      "Speed per epoch: 0.0038\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: -0.5811 - val_loss: -0.5710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00034: val_loss improved from -0.57098 to -0.57099, saving model to ../processed/model.h5\n",
      "Learning rate: 9.5e-07\n",
      "Speed per epoch: 0.0008\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5651 - val_loss: -0.5705\n",
      "\n",
      "Epoch 00035: val_loss did not improve from -0.57099\n",
      "Learning rate: 3.5e-07\n",
      "Speed per epoch: 0.0041\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5803 - val_loss: -0.5707\n",
      "\n",
      "Epoch 00036: val_loss did not improve from -0.57099\n",
      "Learning rate: 2.2e-07\n",
      "Speed per epoch: -0.0022\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5777 - val_loss: -0.5702\n",
      "\n",
      "Epoch 00037: val_loss did not improve from -0.57099\n",
      "Learning rate: 1.9e-07\n",
      "Speed per epoch: -0.0017\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5800 - val_loss: -0.5705\n",
      "\n",
      "Epoch 00038: val_loss did not improve from -0.57099\n",
      "Learning rate: 1.8e-07\n",
      "Speed per epoch: 0.0013\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5787 - val_loss: -0.5708\n",
      "\n",
      "Epoch 00039: val_loss did not improve from -0.57099\n",
      "Learning rate: 7.4e-08\n",
      "Speed per epoch: -0.0019\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5759 - val_loss: -0.5707\n",
      "\n",
      "Epoch 00040: val_loss did not improve from -0.57099\n",
      "Learning rate: 4.8e-08\n",
      "Speed per epoch: 0.0008\n",
      "Wait to stop: 2\n",
      "\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5826 - val_loss: -0.5706\n",
      "\n",
      "Epoch 00041: val_loss did not improve from -0.57099\n",
      "Learning rate: 3.1e-08\n",
      "Speed per epoch: -0.0029\n",
      "Wait to stop: 1\n",
      "\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: -0.5899 - val_loss: -0.5706\n",
      "\n",
      "Epoch 00042: val_loss did not improve from -0.57099\n",
      "Learning rate: 3.1e-08\n",
      "Speed per epoch: -0.0046\n",
      "Wait to stop: 0\n",
      "\n",
      "\n",
      "Fold - 2 / 10\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 1317)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 1313)   0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 1313)   0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 1)      1314        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None, 4)      0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 5)      0           dense_1[0][0]                    \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 64)     1024        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 64)     12352       conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, None, 64)     12352       conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, None, 64)     12352       conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, None, 64)     12352       conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, None, 64)     12352       conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 64)           0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           2080        global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4)            132         dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 66,310\n",
      "Trainable params: 66,310\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.7844 - intersection_over_union: -2.4915e-04 - val_loss: 0.7870 - val_intersection_over_union: -0.0011\n",
      "Epoch 1/100\n",
      "  26/1000 [..............................] - ETA: 26s - loss: -0.0014  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WLMike/Documents/PycharmProjects/ML_venv/lib/python3.6/site-packages/ipykernel_launcher.py:78: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 7s 7ms/step - loss: -0.3003 - val_loss: -0.3401\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to -0.34005, saving model to ../processed/model.h5\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.3844\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: -0.4364 - val_loss: -0.4329\n",
      "\n",
      "Epoch 00002: val_loss improved from -0.34005 to -0.43291, saving model to ../processed/model.h5\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.1134\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4392 - val_loss: -0.4624\n",
      "\n",
      "Epoch 00003: val_loss improved from -0.43291 to -0.46245, saving model to ../processed/model.h5\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.0354\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: -0.4771 - val_loss: -0.4688\n",
      "\n",
      "Epoch 00004: val_loss improved from -0.46245 to -0.46881, saving model to ../processed/model.h5\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.0337\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4826 - val_loss: -0.4784\n",
      "\n",
      "Epoch 00005: val_loss improved from -0.46881 to -0.47844, saving model to ../processed/model.h5\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.0190\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4820 - val_loss: -0.4973\n",
      "\n",
      "Epoch 00006: val_loss improved from -0.47844 to -0.49729, saving model to ../processed/model.h5\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.0087\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4917 - val_loss: -0.4745\n",
      "\n",
      "Epoch 00007: val_loss did not improve from -0.49729\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.0048\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5000 - val_loss: -0.4898\n",
      "\n",
      "Epoch 00008: val_loss did not improve from -0.49729\n",
      "Learning rate: 1.3e-04\n",
      "Speed per epoch: -0.0109\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: -0.5005 - val_loss: -0.4867\n",
      "\n",
      "Epoch 00009: val_loss did not improve from -0.49729\n",
      "Learning rate: 1.3e-04\n",
      "Speed per epoch: -0.0038\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5023 - val_loss: -0.4503\n",
      "\n",
      "Epoch 00010: val_loss did not improve from -0.49729\n",
      "Learning rate: 1.2e-04\n",
      "Speed per epoch: -0.0064\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5192 - val_loss: -0.5041\n",
      "\n",
      "Epoch 00011: val_loss improved from -0.49729 to -0.50410, saving model to ../processed/model.h5\n",
      "Learning rate: 1.2e-04\n",
      "Speed per epoch: -0.0093\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5114 - val_loss: -0.4914\n",
      "\n",
      "Epoch 00012: val_loss did not improve from -0.50410\n",
      "Learning rate: 1.1e-04\n",
      "Speed per epoch: -0.0022\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5122 - val_loss: -0.5040\n",
      "\n",
      "Epoch 00013: val_loss did not improve from -0.50410\n",
      "Learning rate: 1.1e-04\n",
      "Speed per epoch: -0.0008\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5226 - val_loss: -0.5057\n",
      "\n",
      "Epoch 00014: val_loss improved from -0.50410 to -0.50566, saving model to ../processed/model.h5\n",
      "Learning rate: 9.5e-05\n",
      "Speed per epoch: -0.0044\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5373 - val_loss: -0.5077\n",
      "\n",
      "Epoch 00015: val_loss improved from -0.50566 to -0.50772, saving model to ../processed/model.h5\n",
      "Learning rate: 9.5e-05\n",
      "Speed per epoch: -0.0066\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5242 - val_loss: -0.4899\n",
      "\n",
      "Epoch 00016: val_loss did not improve from -0.50772\n",
      "Learning rate: 7.9e-05\n",
      "Speed per epoch: 0.0028\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5334 - val_loss: -0.5176\n",
      "\n",
      "Epoch 00017: val_loss improved from -0.50772 to -0.51755, saving model to ../processed/model.h5\n",
      "Learning rate: 5.9e-05\n",
      "Speed per epoch: -0.0004\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5436 - val_loss: -0.5337\n",
      "\n",
      "Epoch 00018: val_loss improved from -0.51755 to -0.53366, saving model to ../processed/model.h5\n",
      "Learning rate: 5.9e-05\n",
      "Speed per epoch: -0.0069\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5341 - val_loss: -0.5226\n",
      "\n",
      "Epoch 00019: val_loss did not improve from -0.53366\n",
      "Learning rate: 4.9e-05\n",
      "Speed per epoch: -0.0031\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5492 - val_loss: -0.5245\n",
      "\n",
      "Epoch 00020: val_loss did not improve from -0.53366\n",
      "Learning rate: 4.9e-05\n",
      "Speed per epoch: -0.0066\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5504 - val_loss: -0.5217\n",
      "\n",
      "Epoch 00021: val_loss did not improve from -0.53366\n",
      "Learning rate: 4.9e-05\n",
      "Speed per epoch: -0.0052\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5557 - val_loss: -0.5130\n",
      "\n",
      "Epoch 00022: val_loss did not improve from -0.53366\n",
      "Learning rate: 4.9e-05\n",
      "Speed per epoch: -0.0055\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5550 - val_loss: -0.5340\n",
      "\n",
      "Epoch 00023: val_loss improved from -0.53366 to -0.53405, saving model to ../processed/model.h5\n",
      "Learning rate: 4.9e-05\n",
      "Speed per epoch: -0.0028\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5512 - val_loss: -0.5313\n",
      "\n",
      "Epoch 00024: val_loss did not improve from -0.53405\n",
      "Learning rate: 3.4e-05\n",
      "Speed per epoch: 0.0001\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5556 - val_loss: -0.5345\n",
      "\n",
      "Epoch 00025: val_loss improved from -0.53405 to -0.53453, saving model to ../processed/model.h5\n",
      "Learning rate: 3.0e-05\n",
      "Speed per epoch: 0.0004\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5678 - val_loss: -0.5409\n",
      "\n",
      "Epoch 00026: val_loss improved from -0.53453 to -0.54094, saving model to ../processed/model.h5\n",
      "Learning rate: 3.0e-05\n",
      "Speed per epoch: -0.0039\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5630 - val_loss: -0.5339\n",
      "\n",
      "Epoch 00027: val_loss did not improve from -0.54094\n",
      "Learning rate: 3.0e-05\n",
      "Speed per epoch: -0.0023\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5624 - val_loss: -0.5377\n",
      "\n",
      "Epoch 00028: val_loss did not improve from -0.54094\n",
      "Learning rate: 2.3e-05\n",
      "Speed per epoch: 0.0002\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5695 - val_loss: -0.5391\n",
      "\n",
      "Epoch 00029: val_loss did not improve from -0.54094\n",
      "Learning rate: 1.6e-05\n",
      "Speed per epoch: -0.0047\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5728 - val_loss: -0.5380\n",
      "\n",
      "Epoch 00030: val_loss did not improve from -0.54094\n",
      "Learning rate: 1.6e-05\n",
      "Speed per epoch: -0.0032\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5666 - val_loss: -0.5374\n",
      "\n",
      "Epoch 00031: val_loss did not improve from -0.54094\n",
      "Learning rate: 1.3e-05\n",
      "Speed per epoch: 0.0010\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: -0.5657 - val_loss: -0.5398\n",
      "\n",
      "Epoch 00032: val_loss did not improve from -0.54094\n",
      "Learning rate: 5.6e-06\n",
      "Speed per epoch: 0.0008\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5787 - val_loss: -0.5408\n",
      "\n",
      "Epoch 00033: val_loss did not improve from -0.54094\n",
      "Learning rate: 5.4e-06\n",
      "Speed per epoch: -0.0014\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5727 - val_loss: -0.5434\n",
      "\n",
      "Epoch 00034: val_loss improved from -0.54094 to -0.54343, saving model to ../processed/model.h5\n",
      "Learning rate: 3.6e-06\n",
      "Speed per epoch: -0.0023\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5810 - val_loss: -0.5460\n",
      "\n",
      "Epoch 00035: val_loss improved from -0.54343 to -0.54603, saving model to ../processed/model.h5\n",
      "Learning rate: 3.6e-06\n",
      "Speed per epoch: -0.0024\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5796 - val_loss: -0.5434\n",
      "\n",
      "Epoch 00036: val_loss did not improve from -0.54603\n",
      "Learning rate: 2.9e-06\n",
      "Speed per epoch: -0.0040\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5776 - val_loss: -0.5434\n",
      "\n",
      "Epoch 00037: val_loss did not improve from -0.54603\n",
      "Learning rate: 2.6e-06\n",
      "Speed per epoch: 0.0002\n",
      "Wait to stop: 2\n",
      "\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5788 - val_loss: -0.5445\n",
      "\n",
      "Epoch 00038: val_loss did not improve from -0.54603\n",
      "Learning rate: 1.4e-06\n",
      "Speed per epoch: -0.0004\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5819 - val_loss: -0.5454\n",
      "\n",
      "Epoch 00039: val_loss did not improve from -0.54603\n",
      "Learning rate: 9.9e-07\n",
      "Speed per epoch: -0.0030\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5751 - val_loss: -0.5429\n",
      "\n",
      "Epoch 00040: val_loss did not improve from -0.54603\n",
      "Learning rate: 5.2e-07\n",
      "Speed per epoch: -0.0004\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5685 - val_loss: -0.5432\n",
      "\n",
      "Epoch 00041: val_loss did not improve from -0.54603\n",
      "Learning rate: 2.0e-07\n",
      "Speed per epoch: 0.0018\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5812 - val_loss: -0.5433\n",
      "\n",
      "Epoch 00042: val_loss did not improve from -0.54603\n",
      "Learning rate: 1.1e-07\n",
      "Speed per epoch: -0.0056\n",
      "Wait to stop: 2\n",
      "\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5716 - val_loss: -0.5432\n",
      "\n",
      "Epoch 00043: val_loss did not improve from -0.54603\n",
      "Learning rate: 7.5e-08\n",
      "Speed per epoch: 0.0003\n",
      "Wait to stop: 1\n",
      "\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5815 - val_loss: -0.5431\n",
      "\n",
      "Epoch 00044: val_loss did not improve from -0.54603\n",
      "Learning rate: 7.0e-08\n",
      "Speed per epoch: -0.0002\n",
      "Wait to stop: 0\n",
      "\n",
      "\n",
      "Fold - 3 / 10\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 1309)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 1305)   0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 1305)   0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 1)      1306        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None, 4)      0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 5)      0           dense_1[0][0]                    \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 64)     1024        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 64)     12352       conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, None, 64)     12352       conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, None, 64)     12352       conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, None, 64)     12352       conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, None, 64)     12352       conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 64)           0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           2080        global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4)            132         dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 66,302\n",
      "Trainable params: 66,302\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.7642 - intersection_over_union: -0.0051 - val_loss: 0.6572 - val_intersection_over_union: -0.0134\n",
      "Epoch 1/100\n",
      "  43/1000 [>.............................] - ETA: 15s - loss: -0.0124"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WLMike/Documents/PycharmProjects/ML_venv/lib/python3.6/site-packages/ipykernel_launcher.py:78: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.3042 - val_loss: -0.4117\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to -0.41171, saving model to ../processed/model.h5\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.3491\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4358 - val_loss: -0.4801\n",
      "\n",
      "Epoch 00002: val_loss improved from -0.41171 to -0.48012, saving model to ../processed/model.h5\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.1163\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4692 - val_loss: -0.4554\n",
      "\n",
      "Epoch 00003: val_loss did not improve from -0.48012\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.0528\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4678 - val_loss: -0.4230\n",
      "\n",
      "Epoch 00004: val_loss did not improve from -0.48012\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.0214\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4896 - val_loss: -0.4786\n",
      "\n",
      "Epoch 00005: val_loss did not improve from -0.48012\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.0183\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4788 - val_loss: -0.5177\n",
      "\n",
      "Epoch 00006: val_loss improved from -0.48012 to -0.51770, saving model to ../processed/model.h5\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.0031\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.4788 - val_loss: -0.4923\n",
      "\n",
      "Epoch 00007: val_loss did not improve from -0.51770\n",
      "Learning rate: 1.0e-04\n",
      "Speed per epoch: -0.0038\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5048 - val_loss: -0.5138\n",
      "\n",
      "Epoch 00008: val_loss did not improve from -0.51770\n",
      "Learning rate: 1.0e-04\n",
      "Speed per epoch: -0.0105\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5182 - val_loss: -0.4915\n",
      "\n",
      "Epoch 00009: val_loss did not improve from -0.51770\n",
      "Learning rate: 1.0e-04\n",
      "Speed per epoch: -0.0130\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5201 - val_loss: -0.5245\n",
      "\n",
      "Epoch 00010: val_loss improved from -0.51770 to -0.52451, saving model to ../processed/model.h5\n",
      "Learning rate: 1.0e-04\n",
      "Speed per epoch: -0.0085\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5184 - val_loss: -0.5341\n",
      "\n",
      "Epoch 00011: val_loss improved from -0.52451 to -0.53411, saving model to ../processed/model.h5\n",
      "Learning rate: 8.4e-05\n",
      "Speed per epoch: -0.0099\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5146 - val_loss: -0.5030\n",
      "\n",
      "Epoch 00012: val_loss did not improve from -0.53411\n",
      "Learning rate: 6.1e-05\n",
      "Speed per epoch: -0.0011\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5277 - val_loss: -0.5400\n",
      "\n",
      "Epoch 00013: val_loss improved from -0.53411 to -0.54000, saving model to ../processed/model.h5\n",
      "Learning rate: 6.1e-05\n",
      "Speed per epoch: -0.0034\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5294 - val_loss: -0.5295\n",
      "\n",
      "Epoch 00014: val_loss did not improve from -0.54000\n",
      "Learning rate: 6.1e-05\n",
      "Speed per epoch: -0.0041\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5374 - val_loss: -0.5394\n",
      "\n",
      "Epoch 00015: val_loss did not improve from -0.54000\n",
      "Learning rate: 6.1e-05\n",
      "Speed per epoch: -0.0068\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5331 - val_loss: -0.5329\n",
      "\n",
      "Epoch 00016: val_loss did not improve from -0.54000\n",
      "Learning rate: 6.0e-05\n",
      "Speed per epoch: -0.0021\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5376 - val_loss: -0.5479\n",
      "\n",
      "Epoch 00017: val_loss improved from -0.54000 to -0.54791, saving model to ../processed/model.h5\n",
      "Learning rate: 4.8e-05\n",
      "Speed per epoch: -0.0039\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: -0.5526 - val_loss: -0.5376\n",
      "\n",
      "Epoch 00018: val_loss did not improve from -0.54791\n",
      "Learning rate: 4.8e-05\n",
      "Speed per epoch: -0.0087\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5502 - val_loss: -0.5412\n",
      "\n",
      "Epoch 00019: val_loss did not improve from -0.54791\n",
      "Learning rate: 4.8e-05\n",
      "Speed per epoch: -0.0020\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5481 - val_loss: -0.5473\n",
      "\n",
      "Epoch 00020: val_loss did not improve from -0.54791\n",
      "Learning rate: 4.0e-05\n",
      "Speed per epoch: -0.0001\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5495 - val_loss: -0.5473\n",
      "\n",
      "Epoch 00021: val_loss did not improve from -0.54791\n",
      "Learning rate: 3.6e-05\n",
      "Speed per epoch: 0.0032\n",
      "Wait to stop: 2\n",
      "\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5552 - val_loss: -0.5598\n",
      "\n",
      "Epoch 00022: val_loss improved from -0.54791 to -0.55981, saving model to ../processed/model.h5\n",
      "Learning rate: 2.1e-05\n",
      "Speed per epoch: -0.0053\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5538 - val_loss: -0.5452\n",
      "\n",
      "Epoch 00023: val_loss did not improve from -0.55981\n",
      "Learning rate: 2.1e-05\n",
      "Speed per epoch: -0.0024\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5567 - val_loss: -0.5509\n",
      "\n",
      "Epoch 00024: val_loss did not improve from -0.55981\n",
      "Learning rate: 1.9e-05\n",
      "Speed per epoch: -0.0010\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5594 - val_loss: -0.5502\n",
      "\n",
      "Epoch 00025: val_loss did not improve from -0.55981\n",
      "Learning rate: 1.7e-05\n",
      "Speed per epoch: 0.0007\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5663 - val_loss: -0.5584\n",
      "\n",
      "Epoch 00026: val_loss did not improve from -0.55981\n",
      "Learning rate: 1.1e-05\n",
      "Speed per epoch: -0.0063\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5570 - val_loss: -0.5582\n",
      "\n",
      "Epoch 00027: val_loss did not improve from -0.55981\n",
      "Learning rate: 1.0e-05\n",
      "Speed per epoch: -0.0009\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5670 - val_loss: -0.5659\n",
      "\n",
      "Epoch 00028: val_loss improved from -0.55981 to -0.56589, saving model to ../processed/model.h5\n",
      "Learning rate: 9.9e-06\n",
      "Speed per epoch: -0.0024\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5685 - val_loss: -0.5607\n",
      "\n",
      "Epoch 00029: val_loss did not improve from -0.56589\n",
      "Learning rate: 6.6e-06\n",
      "Speed per epoch: -0.0063\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5748 - val_loss: -0.5641\n",
      "\n",
      "Epoch 00030: val_loss did not improve from -0.56589\n",
      "Learning rate: 6.6e-06\n",
      "Speed per epoch: -0.0037\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5651 - val_loss: -0.5604\n",
      "\n",
      "Epoch 00031: val_loss did not improve from -0.56589\n",
      "Learning rate: 3.8e-06\n",
      "Speed per epoch: 0.0012\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5831 - val_loss: -0.5568\n",
      "\n",
      "Epoch 00032: val_loss did not improve from -0.56589\n",
      "Learning rate: 3.4e-06\n",
      "Speed per epoch: -0.0053\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5763 - val_loss: -0.5581\n",
      "\n",
      "Epoch 00033: val_loss did not improve from -0.56589\n",
      "Learning rate: 3.1e-06\n",
      "Speed per epoch: -0.0034\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5704 - val_loss: -0.5560\n",
      "\n",
      "Epoch 00034: val_loss did not improve from -0.56589\n",
      "Learning rate: 1.7e-06\n",
      "Speed per epoch: 0.0003\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5703 - val_loss: -0.5580\n",
      "\n",
      "Epoch 00035: val_loss did not improve from -0.56589\n",
      "Learning rate: 9.2e-07\n",
      "Speed per epoch: 0.0027\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5704 - val_loss: -0.5578\n",
      "\n",
      "Epoch 00036: val_loss did not improve from -0.56589\n",
      "Learning rate: 4.1e-07\n",
      "Speed per epoch: -0.0006\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5657 - val_loss: -0.5583\n",
      "\n",
      "Epoch 00037: val_loss did not improve from -0.56589\n",
      "Learning rate: 1.7e-07\n",
      "Speed per epoch: 0.0008\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5706 - val_loss: -0.5584\n",
      "\n",
      "Epoch 00038: val_loss did not improve from -0.56589\n",
      "Learning rate: 9.8e-08\n",
      "Speed per epoch: -0.0011\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5694 - val_loss: -0.5584\n",
      "\n",
      "Epoch 00039: val_loss did not improve from -0.56589\n",
      "Learning rate: 5.8e-08\n",
      "Speed per epoch: -0.0011\n",
      "Wait to stop: 2\n",
      "\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5706 - val_loss: -0.5584\n",
      "\n",
      "Epoch 00040: val_loss did not improve from -0.56589\n",
      "Learning rate: 4.8e-08\n",
      "Speed per epoch: 0.0022\n",
      "Wait to stop: 1\n",
      "\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -0.5688 - val_loss: -0.5583\n",
      "\n",
      "Epoch 00041: val_loss did not improve from -0.56589\n",
      "Learning rate: 2.1e-08\n",
      "Speed per epoch: 0.0007\n",
      "Wait to stop: 0\n",
      "\n",
      "\n",
      "Fold - 4 / 10\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 1328)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 1324)   0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 1324)   0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 1)      1325        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None, 4)      0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 5)      0           dense_1[0][0]                    \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 64)     1024        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 64)     12352       conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, None, 64)     12352       conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, None, 64)     12352       conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, None, 64)     12352       conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, None, 64)     12352       conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 64)           0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           2080        global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4)            132         dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 66,321\n",
      "Trainable params: 66,321\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.7050 - intersection_over_union: -0.0172 - val_loss: 0.6663 - val_intersection_over_union: -0.0419\n",
      "Epoch 1/100\n",
      "  33/1000 [..............................] - ETA: 20s - loss: -0.0341"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WLMike/Documents/PycharmProjects/ML_venv/lib/python3.6/site-packages/ipykernel_launcher.py:78: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 7s 7ms/step - loss: -0.3203 - val_loss: -0.4100\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to -0.40997, saving model to ../processed/model.h5\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.3219\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: -0.4410 - val_loss: -0.4452\n",
      "\n",
      "Epoch 00002: val_loss improved from -0.40997 to -0.44519, saving model to ../processed/model.h5\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.1017\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4618 - val_loss: -0.4376\n",
      "\n",
      "Epoch 00003: val_loss did not improve from -0.44519\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.0401\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4610 - val_loss: -0.4859\n",
      "\n",
      "Epoch 00004: val_loss improved from -0.44519 to -0.48586, saving model to ../processed/model.h5\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.0223\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4888 - val_loss: -0.4976\n",
      "\n",
      "Epoch 00005: val_loss improved from -0.48586 to -0.49765, saving model to ../processed/model.h5\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.0190\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4797 - val_loss: -0.4750\n",
      "\n",
      "Epoch 00006: val_loss did not improve from -0.49765\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.0091\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5033 - val_loss: -0.4824\n",
      "\n",
      "Epoch 00007: val_loss did not improve from -0.49765\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.0150\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4965 - val_loss: -0.5084\n",
      "\n",
      "Epoch 00008: val_loss improved from -0.49765 to -0.50836, saving model to ../processed/model.h5\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: -0.0082\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.4903 - val_loss: -0.5036\n",
      "\n",
      "Epoch 00009: val_loss did not improve from -0.50836\n",
      "Learning rate: 1.4e-04\n",
      "Speed per epoch: 0.0018\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5047 - val_loss: -0.4892\n",
      "\n",
      "Epoch 00010: val_loss did not improve from -0.50836\n",
      "Learning rate: 1.1e-04\n",
      "Speed per epoch: -0.0052\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5076 - val_loss: -0.5114\n",
      "\n",
      "Epoch 00011: val_loss improved from -0.50836 to -0.51137, saving model to ../processed/model.h5\n",
      "Learning rate: 1.1e-04\n",
      "Speed per epoch: -0.0055\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: -0.5166 - val_loss: -0.5017\n",
      "\n",
      "Epoch 00012: val_loss did not improve from -0.51137\n",
      "Learning rate: 1.1e-04\n",
      "Speed per epoch: -0.0040\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5164 - val_loss: -0.5427\n",
      "\n",
      "Epoch 00013: val_loss improved from -0.51137 to -0.54271, saving model to ../processed/model.h5\n",
      "Learning rate: 1.1e-04\n",
      "Speed per epoch: -0.0054\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: -0.5155 - val_loss: -0.5388\n",
      "\n",
      "Epoch 00014: val_loss did not improve from -0.54271\n",
      "Learning rate: 7.3e-05\n",
      "Speed per epoch: -0.0049\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5262 - val_loss: -0.5208\n",
      "\n",
      "Epoch 00015: val_loss did not improve from -0.54271\n",
      "Learning rate: 7.3e-05\n",
      "Speed per epoch: -0.0037\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5398 - val_loss: -0.5185\n",
      "\n",
      "Epoch 00016: val_loss did not improve from -0.54271\n",
      "Learning rate: 7.3e-05\n",
      "Speed per epoch: -0.0046\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5436 - val_loss: -0.5274\n",
      "\n",
      "Epoch 00017: val_loss did not improve from -0.54271\n",
      "Learning rate: 7.3e-05\n",
      "Speed per epoch: -0.0051\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5492 - val_loss: -0.5209\n",
      "\n",
      "Epoch 00018: val_loss did not improve from -0.54271\n",
      "Learning rate: 7.3e-05\n",
      "Speed per epoch: -0.0070\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: -0.5411 - val_loss: -0.5433\n",
      "\n",
      "Epoch 00019: val_loss improved from -0.54271 to -0.54333, saving model to ../processed/model.h5\n",
      "Learning rate: 7.3e-05\n",
      "Speed per epoch: -0.0003\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5403 - val_loss: -0.5354\n",
      "\n",
      "Epoch 00020: val_loss did not improve from -0.54333\n",
      "Learning rate: 4.0e-05\n",
      "Speed per epoch: -0.0001\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5564 - val_loss: -0.5414\n",
      "\n",
      "Epoch 00021: val_loss did not improve from -0.54333\n",
      "Learning rate: 3.8e-05\n",
      "Speed per epoch: -0.0050\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5538 - val_loss: -0.5379\n",
      "\n",
      "Epoch 00022: val_loss did not improve from -0.54333\n",
      "Learning rate: 3.4e-05\n",
      "Speed per epoch: -0.0036\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5623 - val_loss: -0.5333\n",
      "\n",
      "Epoch 00023: val_loss did not improve from -0.54333\n",
      "Learning rate: 3.4e-05\n",
      "Speed per epoch: -0.0055\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5552 - val_loss: -0.5397\n",
      "\n",
      "Epoch 00024: val_loss did not improve from -0.54333\n",
      "Learning rate: 2.6e-05\n",
      "Speed per epoch: -0.0022\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: -0.5656 - val_loss: -0.5386\n",
      "\n",
      "Epoch 00025: val_loss did not improve from -0.54333\n",
      "Learning rate: 2.6e-05\n",
      "Speed per epoch: -0.0023\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5704 - val_loss: -0.5413\n",
      "\n",
      "Epoch 00026: val_loss did not improve from -0.54333\n",
      "Learning rate: 2.6e-05\n",
      "Speed per epoch: -0.0054\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5659 - val_loss: -0.5471\n",
      "\n",
      "Epoch 00027: val_loss improved from -0.54333 to -0.54712, saving model to ../processed/model.h5\n",
      "Learning rate: 2.5e-05\n",
      "Speed per epoch: -0.0006\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5682 - val_loss: -0.5418\n",
      "\n",
      "Epoch 00028: val_loss did not improve from -0.54712\n",
      "Learning rate: 2.0e-05\n",
      "Speed per epoch: -0.0007\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5741 - val_loss: -0.5469\n",
      "\n",
      "Epoch 00029: val_loss did not improve from -0.54712\n",
      "Learning rate: 1.6e-05\n",
      "Speed per epoch: -0.0028\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5724 - val_loss: -0.5409\n",
      "\n",
      "Epoch 00030: val_loss did not improve from -0.54712\n",
      "Learning rate: 1.6e-05\n",
      "Speed per epoch: 0.0010\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5702 - val_loss: -0.5482\n",
      "\n",
      "Epoch 00031: val_loss improved from -0.54712 to -0.54818, saving model to ../processed/model.h5\n",
      "Learning rate: 7.9e-06\n",
      "Speed per epoch: 0.0010\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5782 - val_loss: -0.5426\n",
      "\n",
      "Epoch 00032: val_loss did not improve from -0.54818\n",
      "Learning rate: 4.9e-06\n",
      "Speed per epoch: -0.0048\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5853 - val_loss: -0.5469\n",
      "\n",
      "Epoch 00033: val_loss did not improve from -0.54818\n",
      "Learning rate: 4.9e-06\n",
      "Speed per epoch: -0.0040\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5793 - val_loss: -0.5500\n",
      "\n",
      "Epoch 00034: val_loss improved from -0.54818 to -0.55002, saving model to ../processed/model.h5\n",
      "Learning rate: 4.7e-06\n",
      "Speed per epoch: 0.0011\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: -0.5843 - val_loss: -0.5478\n",
      "\n",
      "Epoch 00035: val_loss did not improve from -0.55002\n",
      "Learning rate: 3.2e-06\n",
      "Speed per epoch: -0.0011\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 7s 7ms/step - loss: -0.5823 - val_loss: -0.5482\n",
      "\n",
      "Epoch 00036: val_loss did not improve from -0.55002\n",
      "Learning rate: 2.1e-06\n",
      "Speed per epoch: -0.0018\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: -0.5817 - val_loss: -0.5486\n",
      "\n",
      "Epoch 00037: val_loss did not improve from -0.55002\n",
      "Learning rate: 1.4e-06\n",
      "Speed per epoch: -0.0002\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5887 - val_loss: -0.5488\n",
      "\n",
      "Epoch 00038: val_loss did not improve from -0.55002\n",
      "Learning rate: 1.1e-06\n",
      "Speed per epoch: -0.0055\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: -0.5938 - val_loss: -0.5497\n",
      "\n",
      "Epoch 00039: val_loss did not improve from -0.55002\n",
      "Learning rate: 1.1e-06\n",
      "Speed per epoch: -0.0045\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: -0.5860 - val_loss: -0.5489\n",
      "\n",
      "Epoch 00040: val_loss did not improve from -0.55002\n",
      "Learning rate: 8.5e-07\n",
      "Speed per epoch: 0.0018\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5902 - val_loss: -0.5496\n",
      "\n",
      "Epoch 00041: val_loss did not improve from -0.55002\n",
      "Learning rate: 5.4e-07\n",
      "Speed per epoch: -0.0002\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: -0.5825 - val_loss: -0.5495\n",
      "\n",
      "Epoch 00042: val_loss did not improve from -0.55002\n",
      "Learning rate: 2.6e-07\n",
      "Speed per epoch: 0.0035\n",
      "Wait to stop: 2\n",
      "\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: -0.5874 - val_loss: -0.5494\n",
      "\n",
      "Epoch 00043: val_loss did not improve from -0.55002\n",
      "Learning rate: 1.4e-07\n",
      "Speed per epoch: -0.0007\n",
      "Wait to stop: 3\n",
      "\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: -0.5823 - val_loss: -0.5495\n",
      "\n",
      "Epoch 00044: val_loss did not improve from -0.55002\n",
      "Learning rate: 9.4e-08\n",
      "Speed per epoch: 0.0031\n",
      "Wait to stop: 2\n",
      "\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: -0.5849 - val_loss: -0.5494\n",
      "\n",
      "Epoch 00045: val_loss did not improve from -0.55002\n",
      "Learning rate: 4.4e-08\n",
      "Speed per epoch: 0.0003\n",
      "Wait to stop: 1\n",
      "\n",
      "Epoch 46/100\n",
      " 713/1000 [====================>.........] - ETA: 1s - loss: -0.5900"
     ]
    }
   ],
   "source": [
    "train_oof((votes, answers), votes_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
